{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import TaggedCorpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, PooledFlairEmbeddings\n",
    "from typing import List\n",
    "from flair.visual.training_curves import Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-16 15:15:39,482 this function is deprecated, use smart_open.open instead\n"
     ]
    }
   ],
   "source": [
    "# initialize embeddings. This takes time to load.\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "    # GloVe embeddings for arabic\n",
    "    WordEmbeddings('ar'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-16 15:14:55,864 Reading data from /home/jupyter/data_ner_ANER\n",
      "2019-04-16 15:14:55,865 Train: /home/jupyter/data_ner_ANER/train.txt\n",
      "2019-04-16 15:14:55,867 Dev: /home/jupyter/data_ner_ANER/dev.txt\n",
      "2019-04-16 15:14:55,867 Test: None\n",
      "load 10 % of the train data as test dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{b'<unk>': 0,\n",
       " b'O': 1,\n",
       " b'B-LOC': 2,\n",
       " b'B-ORG': 3,\n",
       " b'I-ORG': 4,\n",
       " b'B-PER': 5,\n",
       " b'I-PER': 6,\n",
       " b'I-LOC': 7,\n",
       " b'B-MISC': 8,\n",
       " b'I-MISC': 9,\n",
       " b'': 10,\n",
       " b'\\xe2\\x80\\x8f': 11,\n",
       " b'<START>': 12,\n",
       " b'<STOP>': 13}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = {0: 'text', 1: 'ner'}\n",
    "data_folder = '/home/jupyter/data_ner_ANER/'\n",
    "corpus: TaggedCorpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns, \n",
    "  train_file='train.txt', \n",
    "  dev_file='dev.txt')\n",
    "dict_tmp = corpus.make_tag_dictionary('ner')\n",
    "dict_tmp.item2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-16 15:15:52,203 Reading data from /home/jupyter/data_ner_ANER\n",
      "2019-04-16 15:15:52,204 Train: /home/jupyter/data_ner_ANER/train.txt\n",
      "2019-04-16 15:15:52,205 Dev: /home/jupyter/data_ner_ANER/dev.txt\n",
      "2019-04-16 15:15:52,207 Test: None\n",
      "load 10 % of the train data as test dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. get the corpus\n",
    "columns = {0: 'text', 1: 'ner'}\n",
    "data_folder = '/home/jupyter/data_ner_ANER/'\n",
    "corpus: TaggedCorpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns, \n",
    "  train_file='train.txt', \n",
    "  dev_file='dev.txt')\n",
    "\n",
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'ner'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "\n",
    "\n",
    "# initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# have a relatively small hidden_size\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=64,\n",
    "                                        dropout = 0.2,\n",
    "                                        rnn_layers = 2,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)# initialize trainer\n",
    "# 6. Initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.training_utils import EvaluationMetric\n",
    "\n",
    "from flair.optim import AdamW\n",
    "# optimizer = optimizer(momentum=0.9, dampening=0.9)\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus, optimizer=AdamW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-16 15:16:05,516 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:16:05,517 Evaluation method: MICRO_F1_SCORE\n",
      "2019-04-16 15:16:05,519 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:16:06,205 epoch 1 - iter 0/121 - loss 74.46890259\n",
      "2019-04-16 15:16:17,729 epoch 1 - iter 12/121 - loss 27.51570760\n",
      "2019-04-16 15:16:24,560 epoch 1 - iter 24/121 - loss 22.52636974\n",
      "2019-04-16 15:16:31,475 epoch 1 - iter 36/121 - loss 20.34656063\n",
      "2019-04-16 15:16:37,446 epoch 1 - iter 48/121 - loss 18.31041048\n",
      "2019-04-16 15:16:42,967 epoch 1 - iter 60/121 - loss 17.49510449\n",
      "2019-04-16 15:16:51,552 epoch 1 - iter 72/121 - loss 16.83685119\n",
      "2019-04-16 15:16:56,907 epoch 1 - iter 84/121 - loss 16.17926962\n",
      "2019-04-16 15:17:02,785 epoch 1 - iter 96/121 - loss 15.66285282\n",
      "2019-04-16 15:17:09,724 epoch 1 - iter 108/121 - loss 15.28954564\n",
      "2019-04-16 15:17:14,509 epoch 1 - iter 120/121 - loss 14.94753212\n",
      "2019-04-16 15:17:14,539 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:17:14,540 EPOCH 1 done: loss 14.9475 - lr 0.1000 - bad epochs 0\n",
      "2019-04-16 15:17:23,202 DEV  : loss 10.02254105 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-16 15:17:31,359 TEST : loss 10.56986427 - f-score 0.0000 - acc 0.0000\n",
      "2019-04-16 15:17:36,896 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:17:37,621 epoch 2 - iter 0/121 - loss 14.07528496\n",
      "2019-04-16 15:17:42,747 epoch 2 - iter 12/121 - loss 11.64054287\n",
      "2019-04-16 15:17:48,852 epoch 2 - iter 24/121 - loss 11.80883736\n",
      "2019-04-16 15:17:53,929 epoch 2 - iter 36/121 - loss 12.12641873\n",
      "2019-04-16 15:18:05,152 epoch 2 - iter 48/121 - loss 12.49033309\n",
      "2019-04-16 15:18:10,710 epoch 2 - iter 60/121 - loss 12.24534182\n",
      "2019-04-16 15:18:15,084 epoch 2 - iter 72/121 - loss 12.20098057\n",
      "2019-04-16 15:18:18,884 epoch 2 - iter 84/121 - loss 12.17352338\n",
      "2019-04-16 15:18:23,583 epoch 2 - iter 96/121 - loss 12.11421447\n",
      "2019-04-16 15:18:29,564 epoch 2 - iter 108/121 - loss 12.03021360\n",
      "2019-04-16 15:18:42,488 epoch 2 - iter 120/121 - loss 12.22562513\n",
      "2019-04-16 15:18:42,510 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:18:42,511 EPOCH 2 done: loss 12.2256 - lr 0.1000 - bad epochs 0\n",
      "2019-04-16 15:18:50,867 DEV  : loss 10.07703590 - f-score 0.0016 - acc 0.0008\n",
      "2019-04-16 15:18:59,029 TEST : loss 10.60312462 - f-score 0.0038 - acc 0.0019\n",
      "2019-04-16 15:19:08,803 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:19:09,097 epoch 3 - iter 0/121 - loss 8.39690971\n",
      "2019-04-16 15:19:13,154 epoch 3 - iter 12/121 - loss 11.26278650\n",
      "2019-04-16 15:19:20,802 epoch 3 - iter 24/121 - loss 11.98613979\n",
      "2019-04-16 15:19:28,248 epoch 3 - iter 36/121 - loss 12.88257506\n",
      "2019-04-16 15:19:32,666 epoch 3 - iter 48/121 - loss 12.53476733\n",
      "2019-04-16 15:19:37,580 epoch 3 - iter 60/121 - loss 12.31328792\n",
      "2019-04-16 15:19:49,650 epoch 3 - iter 72/121 - loss 12.53041846\n",
      "2019-04-16 15:19:59,341 epoch 3 - iter 84/121 - loss 12.81209290\n",
      "2019-04-16 15:20:04,614 epoch 3 - iter 96/121 - loss 12.67088768\n",
      "2019-04-16 15:20:10,563 epoch 3 - iter 108/121 - loss 12.43644522\n",
      "2019-04-16 15:20:16,951 epoch 3 - iter 120/121 - loss 12.44576631\n",
      "2019-04-16 15:20:16,972 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:20:16,973 EPOCH 3 done: loss 12.4458 - lr 0.1000 - bad epochs 0\n",
      "2019-04-16 15:20:23,592 DEV  : loss 8.41322422 - f-score 0.3090 - acc 0.1827\n",
      "2019-04-16 15:20:30,339 TEST : loss 9.08106422 - f-score 0.3167 - acc 0.1881\n",
      "2019-04-16 15:20:30,342 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:20:30,838 epoch 4 - iter 0/121 - loss 9.98247051\n",
      "2019-04-16 15:20:42,804 epoch 4 - iter 12/121 - loss 13.51189276\n",
      "2019-04-16 15:20:49,041 epoch 4 - iter 24/121 - loss 13.36590542\n",
      "2019-04-16 15:20:55,395 epoch 4 - iter 36/121 - loss 13.35067855\n",
      "2019-04-16 15:21:00,912 epoch 4 - iter 48/121 - loss 12.91098484\n",
      "2019-04-16 15:21:06,757 epoch 4 - iter 60/121 - loss 12.81719698\n",
      "2019-04-16 15:21:11,208 epoch 4 - iter 72/121 - loss 12.61116008\n",
      "2019-04-16 15:21:15,973 epoch 4 - iter 84/121 - loss 12.52700332\n",
      "2019-04-16 15:21:27,531 epoch 4 - iter 96/121 - loss 12.75574286\n",
      "2019-04-16 15:21:34,003 epoch 4 - iter 108/121 - loss 12.67495422\n",
      "2019-04-16 15:21:37,968 epoch 4 - iter 120/121 - loss 12.52135723\n",
      "2019-04-16 15:21:37,990 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:21:37,991 EPOCH 4 done: loss 12.5214 - lr 0.1000 - bad epochs 1\n",
      "2019-04-16 15:21:47,061 DEV  : loss 9.80767536 - f-score 0.2569 - acc 0.1474\n",
      "2019-04-16 15:21:54,700 TEST : loss 9.90020752 - f-score 0.3288 - acc 0.1967\n",
      "2019-04-16 15:21:54,702 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:21:55,167 epoch 5 - iter 0/121 - loss 12.55752373\n",
      "2019-04-16 15:22:00,666 epoch 5 - iter 12/121 - loss 11.36642163\n",
      "2019-04-16 15:22:06,115 epoch 5 - iter 24/121 - loss 11.76367985\n",
      "2019-04-16 15:22:11,495 epoch 5 - iter 36/121 - loss 11.72930170\n",
      "2019-04-16 15:22:16,617 epoch 5 - iter 48/121 - loss 11.77629528\n",
      "2019-04-16 15:22:25,743 epoch 5 - iter 60/121 - loss 12.11074395\n",
      "2019-04-16 15:22:32,066 epoch 5 - iter 72/121 - loss 12.06494179\n",
      "2019-04-16 15:22:40,003 epoch 5 - iter 84/121 - loss 12.18931080\n",
      "2019-04-16 15:22:48,864 epoch 5 - iter 96/121 - loss 12.41383786\n",
      "2019-04-16 15:22:52,572 epoch 5 - iter 108/121 - loss 12.30833939\n",
      "2019-04-16 15:22:56,358 epoch 5 - iter 120/121 - loss 12.25034159\n",
      "2019-04-16 15:22:57,518 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:22:57,519 EPOCH 5 done: loss 12.2503 - lr 0.1000 - bad epochs 2\n",
      "2019-04-16 15:23:06,503 DEV  : loss 9.10846806 - f-score 0.1564 - acc 0.0849\n",
      "2019-04-16 15:23:13,881 TEST : loss 9.51885605 - f-score 0.1454 - acc 0.0784\n",
      "2019-04-16 15:23:13,883 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:23:14,229 epoch 6 - iter 0/121 - loss 11.39653969\n",
      "2019-04-16 15:23:18,218 epoch 6 - iter 12/121 - loss 12.74083944\n",
      "2019-04-16 15:23:28,325 epoch 6 - iter 24/121 - loss 13.23508720\n",
      "2019-04-16 15:23:35,868 epoch 6 - iter 36/121 - loss 12.54009564\n",
      "2019-04-16 15:23:41,105 epoch 6 - iter 48/121 - loss 12.12322103\n",
      "2019-04-16 15:23:45,065 epoch 6 - iter 60/121 - loss 11.95619638\n",
      "2019-04-16 15:23:48,820 epoch 6 - iter 72/121 - loss 11.94666507\n",
      "2019-04-16 15:23:52,726 epoch 6 - iter 84/121 - loss 12.03899996\n",
      "2019-04-16 15:23:59,228 epoch 6 - iter 96/121 - loss 11.98048014\n",
      "2019-04-16 15:24:09,704 epoch 6 - iter 108/121 - loss 12.03885508\n",
      "2019-04-16 15:24:16,080 epoch 6 - iter 120/121 - loss 12.09268786\n",
      "2019-04-16 15:24:16,101 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:24:16,102 EPOCH 6 done: loss 12.0927 - lr 0.1000 - bad epochs 3\n",
      "2019-04-16 15:24:24,985 DEV  : loss 10.46691608 - f-score 0.0560 - acc 0.0288\n",
      "2019-04-16 15:24:32,572 TEST : loss 11.41106796 - f-score 0.0447 - acc 0.0228\n",
      "2019-04-16 15:24:47,891 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:24:48,454 epoch 7 - iter 0/121 - loss 11.77583694\n",
      "2019-04-16 15:24:55,571 epoch 7 - iter 12/121 - loss 11.45053563\n",
      "2019-04-16 15:25:06,655 epoch 7 - iter 24/121 - loss 13.09675457\n",
      "2019-04-16 15:25:14,200 epoch 7 - iter 36/121 - loss 12.60081897\n",
      "2019-04-16 15:25:19,941 epoch 7 - iter 48/121 - loss 12.23389497\n",
      "2019-04-16 15:25:25,243 epoch 7 - iter 60/121 - loss 12.06531103\n",
      "2019-04-16 15:25:35,712 epoch 7 - iter 72/121 - loss 12.44288869\n",
      "2019-04-16 15:25:41,364 epoch 7 - iter 84/121 - loss 12.30366663\n",
      "2019-04-16 15:25:46,169 epoch 7 - iter 96/121 - loss 12.09797783\n",
      "2019-04-16 15:25:50,580 epoch 7 - iter 108/121 - loss 12.09806825\n",
      "2019-04-16 15:25:55,996 epoch 7 - iter 120/121 - loss 12.11533211\n",
      "2019-04-16 15:25:56,017 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:25:56,018 EPOCH 7 done: loss 12.1153 - lr 0.1000 - bad epochs 0\n",
      "2019-04-16 15:26:04,720 DEV  : loss 9.46758175 - f-score 0.1928 - acc 0.1067\n",
      "2019-04-16 15:26:12,728 TEST : loss 10.24881172 - f-score 0.1985 - acc 0.1102\n",
      "2019-04-16 15:26:12,731 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:26:13,524 epoch 8 - iter 0/121 - loss 12.61021137\n",
      "2019-04-16 15:26:19,198 epoch 8 - iter 12/121 - loss 10.56050480\n",
      "2019-04-16 15:26:24,550 epoch 8 - iter 24/121 - loss 11.29062931\n",
      "2019-04-16 15:26:29,209 epoch 8 - iter 36/121 - loss 11.30803612\n",
      "2019-04-16 15:26:32,836 epoch 8 - iter 48/121 - loss 11.47655833\n",
      "2019-04-16 15:26:39,603 epoch 8 - iter 60/121 - loss 11.78332280\n",
      "2019-04-16 15:26:51,629 epoch 8 - iter 72/121 - loss 12.07198762\n",
      "2019-04-16 15:27:02,652 epoch 8 - iter 84/121 - loss 12.48092435\n",
      "2019-04-16 15:27:06,336 epoch 8 - iter 96/121 - loss 12.48758725\n",
      "2019-04-16 15:27:09,475 epoch 8 - iter 108/121 - loss 12.33433097\n",
      "2019-04-16 15:27:16,298 epoch 8 - iter 120/121 - loss 12.23175721\n",
      "2019-04-16 15:27:16,321 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:27:16,321 EPOCH 8 done: loss 12.2318 - lr 0.1000 - bad epochs 1\n",
      "2019-04-16 15:27:24,929 DEV  : loss 8.33326626 - f-score 0.3901 - acc 0.2423\n",
      "2019-04-16 15:27:33,720 TEST : loss 9.16433811 - f-score 0.3847 - acc 0.2382\n",
      "2019-04-16 15:27:33,723 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:27:34,177 epoch 9 - iter 0/121 - loss 13.53282738\n",
      "2019-04-16 15:27:40,852 epoch 9 - iter 12/121 - loss 11.49729655\n",
      "2019-04-16 15:27:45,569 epoch 9 - iter 24/121 - loss 11.63181435\n",
      "2019-04-16 15:27:51,612 epoch 9 - iter 36/121 - loss 12.26375415\n",
      "2019-04-16 15:27:56,808 epoch 9 - iter 48/121 - loss 11.91548339\n",
      "2019-04-16 15:28:02,873 epoch 9 - iter 60/121 - loss 11.91114931\n",
      "2019-04-16 15:28:15,827 epoch 9 - iter 72/121 - loss 12.54998737\n",
      "2019-04-16 15:28:20,589 epoch 9 - iter 84/121 - loss 12.37434453\n",
      "2019-04-16 15:28:26,222 epoch 9 - iter 96/121 - loss 12.16325583\n",
      "2019-04-16 15:28:31,924 epoch 9 - iter 108/121 - loss 12.09309534\n",
      "2019-04-16 15:28:38,220 epoch 9 - iter 120/121 - loss 12.12931529\n",
      "2019-04-16 15:28:38,248 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:28:38,249 EPOCH 9 done: loss 12.1293 - lr 0.1000 - bad epochs 2\n",
      "2019-04-16 15:28:46,798 DEV  : loss 9.70935822 - f-score 0.0135 - acc 0.0068\n",
      "2019-04-16 15:28:54,961 TEST : loss 10.24125290 - f-score 0.0222 - acc 0.0113\n",
      "2019-04-16 15:28:54,964 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:28:55,181 epoch 10 - iter 0/121 - loss 7.74225235\n",
      "2019-04-16 15:28:59,354 epoch 10 - iter 12/121 - loss 11.45494806\n",
      "2019-04-16 15:29:03,620 epoch 10 - iter 24/121 - loss 11.64433659\n",
      "2019-04-16 15:29:09,936 epoch 10 - iter 36/121 - loss 12.11181651\n",
      "2019-04-16 15:29:15,630 epoch 10 - iter 48/121 - loss 11.93156781\n",
      "2019-04-16 15:29:27,350 epoch 10 - iter 60/121 - loss 12.30887795\n",
      "2019-04-16 15:29:32,203 epoch 10 - iter 72/121 - loss 12.26326116\n",
      "2019-04-16 15:29:36,752 epoch 10 - iter 84/121 - loss 12.29877959\n",
      "2019-04-16 15:29:40,012 epoch 10 - iter 96/121 - loss 12.22319182\n",
      "2019-04-16 15:29:46,021 epoch 10 - iter 108/121 - loss 12.03591975\n",
      "2019-04-16 15:29:56,727 epoch 10 - iter 120/121 - loss 12.19129277\n",
      "2019-04-16 15:29:56,749 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:29:56,749 EPOCH 10 done: loss 12.1913 - lr 0.1000 - bad epochs 3\n",
      "2019-04-16 15:30:04,747 DEV  : loss 11.53777218 - f-score 0.1787 - acc 0.0981\n",
      "2019-04-16 15:30:13,474 TEST : loss 11.25296783 - f-score 0.2338 - acc 0.1324\n",
      "Epoch     9: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2019-04-16 15:30:13,477 Epoch 9: reducing weight decay factor of group 0 to 5.0000e-02.\n",
      "2019-04-16 15:30:13,478 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:30:18,638 epoch 11 - iter 0/121 - loss 33.33537292\n",
      "2019-04-16 15:30:24,296 epoch 11 - iter 12/121 - loss 13.12340854\n",
      "2019-04-16 15:30:30,821 epoch 11 - iter 24/121 - loss 12.01364162\n",
      "2019-04-16 15:30:38,769 epoch 11 - iter 36/121 - loss 11.88543336\n",
      "2019-04-16 15:30:46,100 epoch 11 - iter 48/121 - loss 11.63696724\n",
      "2019-04-16 15:30:57,476 epoch 11 - iter 60/121 - loss 11.69613167\n",
      "2019-04-16 15:31:01,832 epoch 11 - iter 72/121 - loss 11.40046161\n",
      "2019-04-16 15:31:06,675 epoch 11 - iter 84/121 - loss 11.17946413\n",
      "2019-04-16 15:31:12,210 epoch 11 - iter 96/121 - loss 11.04759638\n",
      "2019-04-16 15:31:18,848 epoch 11 - iter 108/121 - loss 11.00406886\n",
      "2019-04-16 15:31:22,423 epoch 11 - iter 120/121 - loss 10.79661496\n",
      "2019-04-16 15:31:22,444 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:31:22,445 EPOCH 11 done: loss 10.7966 - lr 0.0500 - bad epochs 0\n",
      "2019-04-16 15:31:28,775 DEV  : loss 8.35209179 - f-score 0.3739 - acc 0.2300\n",
      "2019-04-16 15:31:35,272 TEST : loss 8.82570744 - f-score 0.3896 - acc 0.2419\n",
      "2019-04-16 15:31:45,369 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:31:45,756 epoch 12 - iter 0/121 - loss 6.79594612\n",
      "2019-04-16 15:31:50,559 epoch 12 - iter 12/121 - loss 9.81183323\n",
      "2019-04-16 15:31:54,509 epoch 12 - iter 24/121 - loss 10.27373590\n",
      "2019-04-16 15:32:00,242 epoch 12 - iter 36/121 - loss 10.60227379\n",
      "2019-04-16 15:32:04,842 epoch 12 - iter 48/121 - loss 10.71889795\n",
      "2019-04-16 15:32:16,520 epoch 12 - iter 60/121 - loss 10.94144853\n",
      "2019-04-16 15:32:22,525 epoch 12 - iter 72/121 - loss 10.73876082\n",
      "2019-04-16 15:32:30,643 epoch 12 - iter 84/121 - loss 10.85273014\n",
      "2019-04-16 15:32:35,832 epoch 12 - iter 96/121 - loss 10.86679628\n",
      "2019-04-16 15:32:42,668 epoch 12 - iter 108/121 - loss 10.72741124\n",
      "2019-04-16 15:32:48,672 epoch 12 - iter 120/121 - loss 10.75533288\n",
      "2019-04-16 15:32:48,694 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:32:48,695 EPOCH 12 done: loss 10.7553 - lr 0.0500 - bad epochs 0\n",
      "2019-04-16 15:32:56,413 DEV  : loss 9.56964970 - f-score 0.1959 - acc 0.1086\n",
      "2019-04-16 15:33:05,091 TEST : loss 10.06887913 - f-score 0.1455 - acc 0.0785\n",
      "2019-04-16 15:33:14,449 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:33:14,806 epoch 13 - iter 0/121 - loss 13.50577927\n",
      "2019-04-16 15:33:22,984 epoch 13 - iter 12/121 - loss 11.93928212\n",
      "2019-04-16 15:33:28,573 epoch 13 - iter 24/121 - loss 11.27770794\n",
      "2019-04-16 15:33:34,556 epoch 13 - iter 36/121 - loss 11.26976255\n",
      "2019-04-16 15:33:38,964 epoch 13 - iter 48/121 - loss 10.86542489\n",
      "2019-04-16 15:33:43,969 epoch 13 - iter 60/121 - loss 10.88333910\n",
      "2019-04-16 15:33:54,922 epoch 13 - iter 72/121 - loss 11.03489495\n",
      "2019-04-16 15:34:02,235 epoch 13 - iter 84/121 - loss 11.06807581\n",
      "2019-04-16 15:34:07,395 epoch 13 - iter 96/121 - loss 10.90919666\n",
      "2019-04-16 15:34:13,151 epoch 13 - iter 108/121 - loss 10.87121540\n",
      "2019-04-16 15:34:18,720 epoch 13 - iter 120/121 - loss 10.81264674\n",
      "2019-04-16 15:34:18,742 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:34:18,743 EPOCH 13 done: loss 10.8126 - lr 0.0500 - bad epochs 0\n",
      "2019-04-16 15:34:27,672 DEV  : loss 8.06019974 - f-score 0.3951 - acc 0.2462\n",
      "2019-04-16 15:34:35,521 TEST : loss 8.75255966 - f-score 0.4129 - acc 0.2602\n",
      "2019-04-16 15:34:35,523 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:34:36,081 epoch 14 - iter 0/121 - loss 11.60083485\n",
      "2019-04-16 15:34:42,700 epoch 14 - iter 12/121 - loss 10.99151509\n",
      "2019-04-16 15:34:47,955 epoch 14 - iter 24/121 - loss 10.80404404\n",
      "2019-04-16 15:34:52,085 epoch 14 - iter 36/121 - loss 10.41481950\n",
      "2019-04-16 15:34:59,194 epoch 14 - iter 48/121 - loss 10.28174057\n",
      "2019-04-16 15:35:04,937 epoch 14 - iter 60/121 - loss 10.18067054\n",
      "2019-04-16 15:35:14,407 epoch 14 - iter 72/121 - loss 10.44656549\n",
      "2019-04-16 15:35:17,886 epoch 14 - iter 84/121 - loss 10.48101848\n",
      "2019-04-16 15:35:23,356 epoch 14 - iter 96/121 - loss 10.57057462\n",
      "2019-04-16 15:35:36,350 epoch 14 - iter 108/121 - loss 10.79969565\n",
      "2019-04-16 15:35:41,811 epoch 14 - iter 120/121 - loss 10.77135173\n",
      "2019-04-16 15:35:41,836 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:35:41,837 EPOCH 14 done: loss 10.7714 - lr 0.0500 - bad epochs 1\n",
      "2019-04-16 15:35:49,907 DEV  : loss 7.73153114 - f-score 0.4069 - acc 0.2555\n",
      "2019-04-16 15:35:58,518 TEST : loss 8.31916046 - f-score 0.3829 - acc 0.2368\n",
      "2019-04-16 15:35:58,520 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:35:58,868 epoch 15 - iter 0/121 - loss 10.19354630\n",
      "2019-04-16 15:36:03,047 epoch 15 - iter 12/121 - loss 8.96625291\n",
      "2019-04-16 15:36:08,582 epoch 15 - iter 24/121 - loss 9.60547035\n",
      "2019-04-16 15:36:13,999 epoch 15 - iter 36/121 - loss 9.75918328\n",
      "2019-04-16 15:36:19,986 epoch 15 - iter 48/121 - loss 9.95706638\n",
      "2019-04-16 15:36:35,956 epoch 15 - iter 60/121 - loss 10.66043316\n",
      "2019-04-16 15:36:43,188 epoch 15 - iter 72/121 - loss 10.78136338\n",
      "2019-04-16 15:36:47,818 epoch 15 - iter 84/121 - loss 10.75018341\n",
      "2019-04-16 15:36:53,087 epoch 15 - iter 96/121 - loss 10.63447981\n",
      "2019-04-16 15:37:00,958 epoch 15 - iter 108/121 - loss 10.73240869\n",
      "2019-04-16 15:37:07,043 epoch 15 - iter 120/121 - loss 10.74019954\n",
      "2019-04-16 15:37:07,063 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:37:07,064 EPOCH 15 done: loss 10.7402 - lr 0.0500 - bad epochs 2\n",
      "2019-04-16 15:37:15,644 DEV  : loss 10.28491974 - f-score 0.0555 - acc 0.0285\n",
      "2019-04-16 15:37:23,691 TEST : loss 10.18225574 - f-score 0.0819 - acc 0.0427\n",
      "2019-04-16 15:37:35,971 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:37:36,216 epoch 16 - iter 0/121 - loss 9.25360394\n",
      "2019-04-16 15:37:41,927 epoch 16 - iter 12/121 - loss 11.40265245\n",
      "2019-04-16 15:37:48,203 epoch 16 - iter 24/121 - loss 10.52935303\n",
      "2019-04-16 15:37:54,050 epoch 16 - iter 36/121 - loss 10.11432445\n",
      "2019-04-16 15:38:01,179 epoch 16 - iter 48/121 - loss 10.46267302\n",
      "2019-04-16 15:38:05,445 epoch 16 - iter 60/121 - loss 10.63964741\n",
      "2019-04-16 15:38:09,470 epoch 16 - iter 72/121 - loss 10.70419510\n",
      "2019-04-16 15:38:18,644 epoch 16 - iter 84/121 - loss 10.71003148\n",
      "2019-04-16 15:38:30,152 epoch 16 - iter 96/121 - loss 10.83930454\n",
      "2019-04-16 15:38:34,659 epoch 16 - iter 108/121 - loss 10.72939277\n",
      "2019-04-16 15:38:38,440 epoch 16 - iter 120/121 - loss 10.59034551\n",
      "2019-04-16 15:38:38,460 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:38:38,461 EPOCH 16 done: loss 10.5903 - lr 0.0500 - bad epochs 0\n",
      "2019-04-16 15:38:44,318 DEV  : loss 7.37823248 - f-score 0.4547 - acc 0.2942\n",
      "2019-04-16 15:38:52,174 TEST : loss 7.74794197 - f-score 0.4272 - acc 0.2716\n",
      "2019-04-16 15:39:01,464 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:39:01,855 epoch 17 - iter 0/121 - loss 11.15615654\n",
      "2019-04-16 15:39:08,643 epoch 17 - iter 12/121 - loss 11.72902426\n",
      "2019-04-16 15:39:13,838 epoch 17 - iter 24/121 - loss 10.59694773\n",
      "2019-04-16 15:39:19,606 epoch 17 - iter 36/121 - loss 10.51481367\n",
      "2019-04-16 15:39:25,461 epoch 17 - iter 48/121 - loss 10.58494262\n",
      "2019-04-16 15:39:37,296 epoch 17 - iter 60/121 - loss 10.96965407\n",
      "2019-04-16 15:39:42,950 epoch 17 - iter 72/121 - loss 10.78115132\n",
      "2019-04-16 15:39:54,332 epoch 17 - iter 84/121 - loss 10.92905975\n",
      "2019-04-16 15:40:00,096 epoch 17 - iter 96/121 - loss 10.91410888\n",
      "2019-04-16 15:40:03,785 epoch 17 - iter 108/121 - loss 10.73000036\n",
      "2019-04-16 15:40:09,034 epoch 17 - iter 120/121 - loss 10.61324456\n",
      "2019-04-16 15:40:09,052 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:40:09,053 EPOCH 17 done: loss 10.6132 - lr 0.0500 - bad epochs 0\n",
      "2019-04-16 15:40:17,999 DEV  : loss 8.51150322 - f-score 0.3783 - acc 0.2333\n",
      "2019-04-16 15:40:25,683 TEST : loss 8.54100609 - f-score 0.4114 - acc 0.2590\n",
      "2019-04-16 15:40:25,686 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:40:26,292 epoch 18 - iter 0/121 - loss 10.09247208\n",
      "2019-04-16 15:40:33,720 epoch 18 - iter 12/121 - loss 11.14927967\n",
      "2019-04-16 15:40:38,608 epoch 18 - iter 24/121 - loss 10.41014124\n",
      "2019-04-16 15:40:43,621 epoch 18 - iter 36/121 - loss 10.77581660\n",
      "2019-04-16 15:40:49,204 epoch 18 - iter 48/121 - loss 10.59355792\n",
      "2019-04-16 15:40:56,155 epoch 18 - iter 60/121 - loss 10.55081700\n",
      "2019-04-16 15:41:06,035 epoch 18 - iter 72/121 - loss 10.63682727\n",
      "2019-04-16 15:41:12,071 epoch 18 - iter 84/121 - loss 10.71118451\n",
      "2019-04-16 15:41:23,433 epoch 18 - iter 96/121 - loss 10.86477506\n",
      "2019-04-16 15:41:29,002 epoch 18 - iter 108/121 - loss 10.80936768\n",
      "2019-04-16 15:41:32,908 epoch 18 - iter 120/121 - loss 10.74455299\n",
      "2019-04-16 15:41:32,929 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:41:32,930 EPOCH 18 done: loss 10.7446 - lr 0.0500 - bad epochs 1\n",
      "2019-04-16 15:41:38,922 DEV  : loss 9.34893703 - f-score 0.3035 - acc 0.1789\n",
      "2019-04-16 15:41:47,042 TEST : loss 9.50185585 - f-score 0.3413 - acc 0.2057\n",
      "2019-04-16 15:41:47,045 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:41:47,466 epoch 19 - iter 0/121 - loss 10.78669357\n",
      "2019-04-16 15:41:53,488 epoch 19 - iter 12/121 - loss 10.80870027\n",
      "2019-04-16 15:41:57,958 epoch 19 - iter 24/121 - loss 10.27198114\n",
      "2019-04-16 15:42:03,804 epoch 19 - iter 36/121 - loss 10.74119871\n",
      "2019-04-16 15:42:20,564 epoch 19 - iter 48/121 - loss 11.29615964\n",
      "2019-04-16 15:42:26,757 epoch 19 - iter 60/121 - loss 11.21530251\n",
      "2019-04-16 15:42:32,704 epoch 19 - iter 72/121 - loss 11.01537744\n",
      "2019-04-16 15:42:38,196 epoch 19 - iter 84/121 - loss 10.84970272\n",
      "2019-04-16 15:42:43,798 epoch 19 - iter 96/121 - loss 10.83622420\n",
      "2019-04-16 15:42:48,705 epoch 19 - iter 108/121 - loss 10.79117037\n",
      "2019-04-16 15:42:52,270 epoch 19 - iter 120/121 - loss 10.82076607\n",
      "2019-04-16 15:42:52,290 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:42:52,291 EPOCH 19 done: loss 10.8208 - lr 0.0500 - bad epochs 2\n",
      "2019-04-16 15:42:59,440 DEV  : loss 7.89668941 - f-score 0.4436 - acc 0.2851\n",
      "2019-04-16 15:43:08,184 TEST : loss 8.41008854 - f-score 0.4333 - acc 0.2765\n",
      "2019-04-16 15:43:08,186 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:43:08,707 epoch 20 - iter 0/121 - loss 12.06677151\n",
      "2019-04-16 15:43:12,769 epoch 20 - iter 12/121 - loss 9.68280345\n",
      "2019-04-16 15:43:26,587 epoch 20 - iter 24/121 - loss 11.24600071\n",
      "2019-04-16 15:43:31,712 epoch 20 - iter 36/121 - loss 10.84205718\n",
      "2019-04-16 15:43:41,271 epoch 20 - iter 48/121 - loss 11.22443145\n",
      "2019-04-16 15:43:47,241 epoch 20 - iter 60/121 - loss 10.93213139\n",
      "2019-04-16 15:43:53,499 epoch 20 - iter 72/121 - loss 10.78838815\n",
      "2019-04-16 15:44:00,130 epoch 20 - iter 84/121 - loss 11.03634990\n",
      "2019-04-16 15:44:06,340 epoch 20 - iter 96/121 - loss 10.97908879\n",
      "2019-04-16 15:44:11,620 epoch 20 - iter 108/121 - loss 10.87172972\n",
      "2019-04-16 15:44:17,130 epoch 20 - iter 120/121 - loss 10.73851783\n",
      "2019-04-16 15:44:17,157 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:44:17,158 EPOCH 20 done: loss 10.7385 - lr 0.0500 - bad epochs 3\n",
      "2019-04-16 15:44:25,323 DEV  : loss 7.91804361 - f-score 0.4236 - acc 0.2687\n",
      "2019-04-16 15:44:33,729 TEST : loss 8.22050381 - f-score 0.4225 - acc 0.2678\n",
      "Epoch    19: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2019-04-16 15:44:33,732 Epoch 19: reducing weight decay factor of group 0 to 2.5000e-02.\n",
      "2019-04-16 15:44:33,733 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:44:34,251 epoch 21 - iter 0/121 - loss 13.95664501\n",
      "2019-04-16 15:44:44,995 epoch 21 - iter 12/121 - loss 11.33737073\n",
      "2019-04-16 15:44:50,274 epoch 21 - iter 24/121 - loss 9.99159155\n",
      "2019-04-16 15:44:55,163 epoch 21 - iter 36/121 - loss 9.55938130\n",
      "2019-04-16 15:44:59,692 epoch 21 - iter 48/121 - loss 9.65149173\n",
      "2019-04-16 15:45:06,190 epoch 21 - iter 60/121 - loss 9.62960014\n",
      "2019-04-16 15:45:11,453 epoch 21 - iter 72/121 - loss 9.48281078\n",
      "2019-04-16 15:45:17,940 epoch 21 - iter 84/121 - loss 9.48423864\n",
      "2019-04-16 15:45:27,861 epoch 21 - iter 96/121 - loss 9.53740540\n",
      "2019-04-16 15:45:35,551 epoch 21 - iter 108/121 - loss 9.56267852\n",
      "2019-04-16 15:45:41,742 epoch 21 - iter 120/121 - loss 9.66909053\n",
      "2019-04-16 15:45:41,762 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:45:41,763 EPOCH 21 done: loss 9.6691 - lr 0.0250 - bad epochs 0\n",
      "2019-04-16 15:45:47,820 DEV  : loss 7.66826391 - f-score 0.4180 - acc 0.2642\n",
      "2019-04-16 15:45:53,859 TEST : loss 7.92867565 - f-score 0.4194 - acc 0.2654\n",
      "2019-04-16 15:46:03,919 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:46:04,468 epoch 22 - iter 0/121 - loss 8.66272259\n",
      "2019-04-16 15:46:08,997 epoch 22 - iter 12/121 - loss 8.92725189\n",
      "2019-04-16 15:46:26,704 epoch 22 - iter 24/121 - loss 10.60128393\n",
      "2019-04-16 15:46:32,443 epoch 22 - iter 36/121 - loss 10.45939038\n",
      "2019-04-16 15:46:37,099 epoch 22 - iter 48/121 - loss 10.20192789\n",
      "2019-04-16 15:46:43,419 epoch 22 - iter 60/121 - loss 10.11294506\n",
      "2019-04-16 15:46:49,045 epoch 22 - iter 72/121 - loss 10.00926753\n",
      "2019-04-16 15:46:53,327 epoch 22 - iter 84/121 - loss 9.88115657\n",
      "2019-04-16 15:46:59,890 epoch 22 - iter 96/121 - loss 9.83987522\n",
      "2019-04-16 15:47:06,985 epoch 22 - iter 108/121 - loss 9.65598776\n",
      "2019-04-16 15:47:12,835 epoch 22 - iter 120/121 - loss 9.56533156\n",
      "2019-04-16 15:47:12,856 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:47:12,857 EPOCH 22 done: loss 9.5653 - lr 0.0250 - bad epochs 0\n",
      "2019-04-16 15:47:20,812 DEV  : loss 7.11230326 - f-score 0.4296 - acc 0.2736\n",
      "2019-04-16 15:47:29,327 TEST : loss 7.32254744 - f-score 0.4496 - acc 0.2900\n",
      "2019-04-16 15:47:38,695 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:47:39,016 epoch 23 - iter 0/121 - loss 7.86463642\n",
      "2019-04-16 15:47:44,090 epoch 23 - iter 12/121 - loss 9.27653060\n",
      "2019-04-16 15:47:49,221 epoch 23 - iter 24/121 - loss 8.93869404\n",
      "2019-04-16 15:48:01,265 epoch 23 - iter 36/121 - loss 9.58691637\n",
      "2019-04-16 15:48:07,132 epoch 23 - iter 48/121 - loss 9.57830961\n",
      "2019-04-16 15:48:12,177 epoch 23 - iter 60/121 - loss 9.53047910\n",
      "2019-04-16 15:48:26,496 epoch 23 - iter 72/121 - loss 9.90824984\n",
      "2019-04-16 15:48:31,565 epoch 23 - iter 84/121 - loss 9.91359643\n",
      "2019-04-16 15:48:38,047 epoch 23 - iter 96/121 - loss 9.79119059\n",
      "2019-04-16 15:48:43,224 epoch 23 - iter 108/121 - loss 9.66241344\n",
      "2019-04-16 15:48:48,058 epoch 23 - iter 120/121 - loss 9.53461421\n",
      "2019-04-16 15:48:48,079 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:48:48,081 EPOCH 23 done: loss 9.5346 - lr 0.0250 - bad epochs 0\n",
      "2019-04-16 15:48:57,110 DEV  : loss 7.05698490 - f-score 0.5072 - acc 0.3397\n",
      "2019-04-16 15:49:05,318 TEST : loss 7.21825695 - f-score 0.5075 - acc 0.3400\n",
      "2019-04-16 15:49:15,125 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:49:15,413 epoch 24 - iter 0/121 - loss 7.03182411\n",
      "2019-04-16 15:49:24,210 epoch 24 - iter 12/121 - loss 9.79883605\n",
      "2019-04-16 15:49:28,956 epoch 24 - iter 24/121 - loss 8.79184414\n",
      "2019-04-16 15:49:34,599 epoch 24 - iter 36/121 - loss 8.96846860\n",
      "2019-04-16 15:49:39,464 epoch 24 - iter 48/121 - loss 9.01432417\n",
      "2019-04-16 15:49:43,901 epoch 24 - iter 60/121 - loss 8.93160621\n",
      "2019-04-16 15:49:50,436 epoch 24 - iter 72/121 - loss 9.02562811\n",
      "2019-04-16 15:50:02,805 epoch 24 - iter 84/121 - loss 9.46640850\n",
      "2019-04-16 15:50:07,523 epoch 24 - iter 96/121 - loss 9.53284717\n",
      "2019-04-16 15:50:19,049 epoch 24 - iter 108/121 - loss 9.61543561\n",
      "2019-04-16 15:50:24,907 epoch 24 - iter 120/121 - loss 9.61153619\n",
      "2019-04-16 15:50:24,929 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:50:24,930 EPOCH 24 done: loss 9.6115 - lr 0.0250 - bad epochs 0\n",
      "2019-04-16 15:50:32,925 DEV  : loss 6.51106739 - f-score 0.4740 - acc 0.3106\n",
      "2019-04-16 15:50:41,492 TEST : loss 7.13219595 - f-score 0.4680 - acc 0.3055\n",
      "2019-04-16 15:50:41,494 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:50:41,760 epoch 25 - iter 0/121 - loss 9.52975941\n",
      "2019-04-16 15:50:47,068 epoch 25 - iter 12/121 - loss 9.27268050\n",
      "2019-04-16 15:50:54,326 epoch 25 - iter 24/121 - loss 9.73553448\n",
      "2019-04-16 15:50:59,645 epoch 25 - iter 36/121 - loss 9.33375232\n",
      "2019-04-16 15:51:05,164 epoch 25 - iter 48/121 - loss 9.24233222\n",
      "2019-04-16 15:51:11,725 epoch 25 - iter 60/121 - loss 9.38059680\n",
      "2019-04-16 15:51:17,454 epoch 25 - iter 72/121 - loss 9.28325021\n",
      "2019-04-16 15:51:23,486 epoch 25 - iter 84/121 - loss 9.28213630\n",
      "2019-04-16 15:51:28,405 epoch 25 - iter 96/121 - loss 9.29041358\n",
      "2019-04-16 15:51:38,659 epoch 25 - iter 108/121 - loss 9.39746534\n",
      "2019-04-16 15:51:50,253 epoch 25 - iter 120/121 - loss 9.52945593\n",
      "2019-04-16 15:51:50,281 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:51:50,282 EPOCH 25 done: loss 9.5295 - lr 0.0250 - bad epochs 1\n",
      "2019-04-16 15:51:56,677 DEV  : loss 6.81356573 - f-score 0.4491 - acc 0.2896\n",
      "2019-04-16 15:52:03,433 TEST : loss 7.17638683 - f-score 0.4222 - acc 0.2676\n",
      "2019-04-16 15:52:12,963 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:52:13,301 epoch 26 - iter 0/121 - loss 11.62338638\n",
      "2019-04-16 15:52:21,015 epoch 26 - iter 12/121 - loss 9.38384474\n",
      "2019-04-16 15:52:28,113 epoch 26 - iter 24/121 - loss 9.97003235\n",
      "2019-04-16 15:52:33,999 epoch 26 - iter 36/121 - loss 9.85147178\n",
      "2019-04-16 15:52:38,489 epoch 26 - iter 48/121 - loss 9.48933044\n",
      "2019-04-16 15:52:43,287 epoch 26 - iter 60/121 - loss 9.34525047\n",
      "2019-04-16 15:52:51,844 epoch 26 - iter 72/121 - loss 9.46628673\n",
      "2019-04-16 15:52:57,411 epoch 26 - iter 84/121 - loss 9.40175228\n",
      "2019-04-16 15:53:01,635 epoch 26 - iter 96/121 - loss 9.26143195\n",
      "2019-04-16 15:53:08,660 epoch 26 - iter 108/121 - loss 9.33772724\n",
      "2019-04-16 15:53:18,749 epoch 26 - iter 120/121 - loss 9.47891992\n",
      "2019-04-16 15:53:18,771 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:53:18,771 EPOCH 26 done: loss 9.4789 - lr 0.0250 - bad epochs 0\n",
      "2019-04-16 15:53:27,197 DEV  : loss 7.15108395 - f-score 0.4734 - acc 0.3101\n",
      "2019-04-16 15:53:35,193 TEST : loss 7.34685612 - f-score 0.4969 - acc 0.3306\n",
      "2019-04-16 15:53:44,348 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:53:44,686 epoch 27 - iter 0/121 - loss 8.10571861\n",
      "2019-04-16 15:53:50,952 epoch 27 - iter 12/121 - loss 9.74425760\n",
      "2019-04-16 15:53:56,864 epoch 27 - iter 24/121 - loss 9.38686958\n",
      "2019-04-16 15:54:07,401 epoch 27 - iter 36/121 - loss 9.63740328\n",
      "2019-04-16 15:54:13,463 epoch 27 - iter 48/121 - loss 9.43308006\n",
      "2019-04-16 15:54:19,359 epoch 27 - iter 60/121 - loss 9.50027949\n",
      "2019-04-16 15:54:25,562 epoch 27 - iter 72/121 - loss 9.48838568\n",
      "2019-04-16 15:54:30,489 epoch 27 - iter 84/121 - loss 9.47118636\n",
      "2019-04-16 15:54:35,332 epoch 27 - iter 96/121 - loss 9.40564848\n",
      "2019-04-16 15:54:42,455 epoch 27 - iter 108/121 - loss 9.37983019\n",
      "2019-04-16 15:54:52,819 epoch 27 - iter 120/121 - loss 9.53686993\n",
      "2019-04-16 15:54:52,838 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:54:52,839 EPOCH 27 done: loss 9.5369 - lr 0.0250 - bad epochs 0\n",
      "2019-04-16 15:55:01,192 DEV  : loss 6.84139013 - f-score 0.4813 - acc 0.3169\n",
      "2019-04-16 15:55:09,180 TEST : loss 6.98643303 - f-score 0.4847 - acc 0.3199\n",
      "2019-04-16 15:55:09,183 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:55:09,594 epoch 28 - iter 0/121 - loss 10.88255119\n",
      "2019-04-16 15:55:16,930 epoch 28 - iter 12/121 - loss 10.03147056\n",
      "2019-04-16 15:55:27,443 epoch 28 - iter 24/121 - loss 9.93244009\n",
      "2019-04-16 15:55:32,893 epoch 28 - iter 36/121 - loss 9.75842372\n",
      "2019-04-16 15:55:38,679 epoch 28 - iter 48/121 - loss 9.80765671\n",
      "2019-04-16 15:55:44,148 epoch 28 - iter 60/121 - loss 9.58058988\n",
      "2019-04-16 15:55:49,625 epoch 28 - iter 72/121 - loss 9.31098971\n",
      "2019-04-16 15:55:59,683 epoch 28 - iter 84/121 - loss 9.46767129\n",
      "2019-04-16 15:56:07,022 epoch 28 - iter 96/121 - loss 9.46222575\n",
      "2019-04-16 15:56:13,376 epoch 28 - iter 108/121 - loss 9.43669041\n",
      "2019-04-16 15:56:17,537 epoch 28 - iter 120/121 - loss 9.40913940\n",
      "2019-04-16 15:56:17,558 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:56:17,559 EPOCH 28 done: loss 9.4091 - lr 0.0250 - bad epochs 1\n",
      "2019-04-16 15:56:26,145 DEV  : loss 7.40833044 - f-score 0.4305 - acc 0.2743\n",
      "2019-04-16 15:56:33,942 TEST : loss 7.62002325 - f-score 0.4302 - acc 0.2741\n",
      "2019-04-16 15:56:45,848 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:56:46,154 epoch 29 - iter 0/121 - loss 7.15182829\n",
      "2019-04-16 15:56:50,564 epoch 29 - iter 12/121 - loss 9.42895053\n",
      "2019-04-16 15:56:57,705 epoch 29 - iter 24/121 - loss 9.28356501\n",
      "2019-04-16 15:57:09,111 epoch 29 - iter 36/121 - loss 9.82772828\n",
      "2019-04-16 15:57:14,911 epoch 29 - iter 48/121 - loss 9.76996252\n",
      "2019-04-16 15:57:20,982 epoch 29 - iter 60/121 - loss 9.59522872\n",
      "2019-04-16 15:57:27,816 epoch 29 - iter 72/121 - loss 9.61940475\n",
      "2019-04-16 15:57:32,097 epoch 29 - iter 84/121 - loss 9.50902289\n",
      "2019-04-16 15:57:36,150 epoch 29 - iter 96/121 - loss 9.45082989\n",
      "2019-04-16 15:57:44,175 epoch 29 - iter 108/121 - loss 9.64798762\n",
      "2019-04-16 15:57:49,486 epoch 29 - iter 120/121 - loss 9.54302318\n",
      "2019-04-16 15:57:49,506 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:57:49,507 EPOCH 29 done: loss 9.5430 - lr 0.0250 - bad epochs 0\n",
      "2019-04-16 15:57:58,232 DEV  : loss 6.70074368 - f-score 0.4768 - acc 0.3131\n",
      "2019-04-16 15:58:06,378 TEST : loss 6.89927197 - f-score 0.4834 - acc 0.3188\n",
      "2019-04-16 15:58:06,381 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:58:07,466 epoch 30 - iter 0/121 - loss 12.09260178\n",
      "2019-04-16 15:58:14,089 epoch 30 - iter 12/121 - loss 10.34052735\n",
      "2019-04-16 15:58:19,163 epoch 30 - iter 24/121 - loss 9.58101248\n",
      "2019-04-16 15:58:23,855 epoch 30 - iter 36/121 - loss 9.50596708\n",
      "2019-04-16 15:58:34,400 epoch 30 - iter 48/121 - loss 9.60789899\n",
      "2019-04-16 15:58:41,634 epoch 30 - iter 60/121 - loss 9.60282441\n",
      "2019-04-16 15:58:47,004 epoch 30 - iter 72/121 - loss 9.62883143\n",
      "2019-04-16 15:58:52,546 epoch 30 - iter 84/121 - loss 9.44741764\n",
      "2019-04-16 15:58:58,197 epoch 30 - iter 96/121 - loss 9.39501238\n",
      "2019-04-16 15:59:03,409 epoch 30 - iter 108/121 - loss 9.28613155\n",
      "2019-04-16 15:59:15,066 epoch 30 - iter 120/121 - loss 9.48327180\n",
      "2019-04-16 15:59:15,087 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:59:15,087 EPOCH 30 done: loss 9.4833 - lr 0.0250 - bad epochs 1\n",
      "2019-04-16 15:59:23,443 DEV  : loss 6.31983709 - f-score 0.5201 - acc 0.3514\n",
      "2019-04-16 15:59:31,768 TEST : loss 6.40309668 - f-score 0.5210 - acc 0.3523\n",
      "2019-04-16 15:59:31,770 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 15:59:32,187 epoch 31 - iter 0/121 - loss 8.20059204\n",
      "2019-04-16 15:59:44,529 epoch 31 - iter 12/121 - loss 11.42713697\n",
      "2019-04-16 15:59:49,369 epoch 31 - iter 24/121 - loss 10.15737669\n",
      "2019-04-16 15:59:55,856 epoch 31 - iter 36/121 - loss 9.96916147\n",
      "2019-04-16 16:00:01,938 epoch 31 - iter 48/121 - loss 9.78190767\n",
      "2019-04-16 16:00:06,600 epoch 31 - iter 60/121 - loss 9.59405325\n",
      "2019-04-16 16:00:17,064 epoch 31 - iter 72/121 - loss 9.71900283\n",
      "2019-04-16 16:00:22,762 epoch 31 - iter 84/121 - loss 9.62153496\n",
      "2019-04-16 16:00:28,204 epoch 31 - iter 96/121 - loss 9.61982958\n",
      "2019-04-16 16:00:33,772 epoch 31 - iter 108/121 - loss 9.56614063\n",
      "2019-04-16 16:00:40,886 epoch 31 - iter 120/121 - loss 9.53635071\n",
      "2019-04-16 16:00:40,906 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:00:40,907 EPOCH 31 done: loss 9.5364 - lr 0.0250 - bad epochs 2\n",
      "2019-04-16 16:00:49,025 DEV  : loss 7.38164759 - f-score 0.4113 - acc 0.2588\n",
      "2019-04-16 16:00:57,756 TEST : loss 7.51480961 - f-score 0.4413 - acc 0.2831\n",
      "2019-04-16 16:00:57,759 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:00:58,179 epoch 32 - iter 0/121 - loss 11.82644081\n",
      "2019-04-16 16:01:04,513 epoch 32 - iter 12/121 - loss 9.39965076\n",
      "2019-04-16 16:01:08,585 epoch 32 - iter 24/121 - loss 9.26884087\n",
      "2019-04-16 16:01:14,244 epoch 32 - iter 36/121 - loss 9.40474978\n",
      "2019-04-16 16:01:26,358 epoch 32 - iter 48/121 - loss 9.85628618\n",
      "2019-04-16 16:01:32,201 epoch 32 - iter 60/121 - loss 9.77909288\n",
      "2019-04-16 16:01:39,703 epoch 32 - iter 72/121 - loss 9.76557131\n",
      "2019-04-16 16:01:45,847 epoch 32 - iter 84/121 - loss 9.55837394\n",
      "2019-04-16 16:01:54,256 epoch 32 - iter 96/121 - loss 9.63714650\n",
      "2019-04-16 16:01:57,604 epoch 32 - iter 108/121 - loss 9.50960994\n",
      "2019-04-16 16:02:02,227 epoch 32 - iter 120/121 - loss 9.41506276\n",
      "2019-04-16 16:02:03,151 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:02:03,152 EPOCH 32 done: loss 9.4151 - lr 0.0250 - bad epochs 3\n",
      "2019-04-16 16:02:09,610 DEV  : loss 8.27077770 - f-score 0.4573 - acc 0.2964\n",
      "2019-04-16 16:02:18,311 TEST : loss 8.49019241 - f-score 0.4425 - acc 0.2841\n",
      "Epoch    31: reducing learning rate of group 0 to 1.2500e-02.\n",
      "2019-04-16 16:02:18,313 Epoch 31: reducing weight decay factor of group 0 to 1.2500e-02.\n",
      "2019-04-16 16:02:18,314 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:02:18,770 epoch 33 - iter 0/121 - loss 9.05836678\n",
      "2019-04-16 16:02:24,160 epoch 33 - iter 12/121 - loss 8.36670857\n",
      "2019-04-16 16:02:29,596 epoch 33 - iter 24/121 - loss 8.46066233\n",
      "2019-04-16 16:02:32,770 epoch 33 - iter 36/121 - loss 8.05456864\n",
      "2019-04-16 16:02:36,294 epoch 33 - iter 48/121 - loss 8.05275798\n",
      "2019-04-16 16:02:40,645 epoch 33 - iter 60/121 - loss 8.26643578\n",
      "2019-04-16 16:02:53,261 epoch 33 - iter 72/121 - loss 8.57641790\n",
      "2019-04-16 16:02:58,495 epoch 33 - iter 84/121 - loss 8.47537355\n",
      "2019-04-16 16:03:02,773 epoch 33 - iter 96/121 - loss 8.47884957\n",
      "2019-04-16 16:03:11,951 epoch 33 - iter 108/121 - loss 8.66163926\n",
      "2019-04-16 16:03:16,844 epoch 33 - iter 120/121 - loss 8.59942515\n",
      "2019-04-16 16:03:16,866 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:03:16,867 EPOCH 33 done: loss 8.5994 - lr 0.0125 - bad epochs 0\n",
      "2019-04-16 16:03:25,751 DEV  : loss 6.19775629 - f-score 0.5311 - acc 0.3616\n",
      "2019-04-16 16:03:33,612 TEST : loss 6.42093992 - f-score 0.5068 - acc 0.3394\n",
      "2019-04-16 16:03:43,915 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:03:44,403 epoch 34 - iter 0/121 - loss 8.35474586\n",
      "2019-04-16 16:03:49,736 epoch 34 - iter 12/121 - loss 7.63696788\n",
      "2019-04-16 16:03:58,602 epoch 34 - iter 24/121 - loss 8.59900581\n",
      "2019-04-16 16:04:04,603 epoch 34 - iter 36/121 - loss 8.61803311\n",
      "2019-04-16 16:04:11,297 epoch 34 - iter 48/121 - loss 8.53299273\n",
      "2019-04-16 16:04:21,622 epoch 34 - iter 60/121 - loss 8.61484787\n",
      "2019-04-16 16:04:27,264 epoch 34 - iter 72/121 - loss 8.58165254\n",
      "2019-04-16 16:04:33,070 epoch 34 - iter 84/121 - loss 8.54805746\n",
      "2019-04-16 16:04:39,224 epoch 34 - iter 96/121 - loss 8.49382491\n",
      "2019-04-16 16:04:44,108 epoch 34 - iter 108/121 - loss 8.38521584\n",
      "2019-04-16 16:04:49,492 epoch 34 - iter 120/121 - loss 8.32881472\n",
      "2019-04-16 16:04:49,513 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:04:49,514 EPOCH 34 done: loss 8.3288 - lr 0.0125 - bad epochs 0\n",
      "2019-04-16 16:04:58,426 DEV  : loss 5.96082926 - f-score 0.5686 - acc 0.3972\n",
      "2019-04-16 16:05:06,159 TEST : loss 6.10815620 - f-score 0.5525 - acc 0.3817\n",
      "2019-04-16 16:05:15,738 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:05:16,422 epoch 35 - iter 0/121 - loss 8.02006626\n",
      "2019-04-16 16:05:20,777 epoch 35 - iter 12/121 - loss 7.48288881\n",
      "2019-04-16 16:05:31,108 epoch 35 - iter 24/121 - loss 8.20635717\n",
      "2019-04-16 16:05:36,041 epoch 35 - iter 36/121 - loss 7.93609494\n",
      "2019-04-16 16:05:41,872 epoch 35 - iter 48/121 - loss 7.96711876\n",
      "2019-04-16 16:05:47,059 epoch 35 - iter 60/121 - loss 8.18126139\n",
      "2019-04-16 16:05:52,050 epoch 35 - iter 72/121 - loss 8.28159194\n",
      "2019-04-16 16:05:57,069 epoch 35 - iter 84/121 - loss 8.15579610\n",
      "2019-04-16 16:06:06,265 epoch 35 - iter 96/121 - loss 8.23798866\n",
      "2019-04-16 16:06:17,800 epoch 35 - iter 108/121 - loss 8.29214915\n",
      "2019-04-16 16:06:22,494 epoch 35 - iter 120/121 - loss 8.25587364\n",
      "2019-04-16 16:06:22,521 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:06:22,522 EPOCH 35 done: loss 8.2559 - lr 0.0125 - bad epochs 0\n",
      "2019-04-16 16:06:31,466 DEV  : loss 6.26464605 - f-score 0.5404 - acc 0.3703\n",
      "2019-04-16 16:06:39,149 TEST : loss 6.50818396 - f-score 0.5694 - acc 0.3981\n",
      "2019-04-16 16:06:55,188 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:06:55,643 epoch 36 - iter 0/121 - loss 8.07438660\n",
      "2019-04-16 16:07:01,183 epoch 36 - iter 12/121 - loss 8.43123755\n",
      "2019-04-16 16:07:13,550 epoch 36 - iter 24/121 - loss 9.06365170\n",
      "2019-04-16 16:07:18,061 epoch 36 - iter 36/121 - loss 8.67670198\n",
      "2019-04-16 16:07:21,701 epoch 36 - iter 48/121 - loss 8.66446681\n",
      "2019-04-16 16:07:25,528 epoch 36 - iter 60/121 - loss 8.56890653\n",
      "2019-04-16 16:07:31,033 epoch 36 - iter 72/121 - loss 8.51821611\n",
      "2019-04-16 16:07:38,974 epoch 36 - iter 84/121 - loss 8.43897077\n",
      "2019-04-16 16:07:48,982 epoch 36 - iter 96/121 - loss 8.55072797\n",
      "2019-04-16 16:07:53,493 epoch 36 - iter 108/121 - loss 8.45223361\n",
      "2019-04-16 16:08:00,817 epoch 36 - iter 120/121 - loss 8.35367619\n",
      "2019-04-16 16:08:00,837 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:08:00,837 EPOCH 36 done: loss 8.3537 - lr 0.0125 - bad epochs 0\n",
      "2019-04-16 16:08:09,088 DEV  : loss 6.18359947 - f-score 0.5586 - acc 0.3875\n",
      "2019-04-16 16:08:17,840 TEST : loss 6.27070713 - f-score 0.5716 - acc 0.4002\n",
      "2019-04-16 16:08:17,843 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:08:18,259 epoch 37 - iter 0/121 - loss 7.12648964\n",
      "2019-04-16 16:08:24,333 epoch 37 - iter 12/121 - loss 7.95972604\n",
      "2019-04-16 16:08:29,010 epoch 37 - iter 24/121 - loss 8.21110277\n",
      "2019-04-16 16:08:32,902 epoch 37 - iter 36/121 - loss 7.97060885\n",
      "2019-04-16 16:08:37,169 epoch 37 - iter 48/121 - loss 7.91164543\n",
      "2019-04-16 16:08:41,644 epoch 37 - iter 60/121 - loss 7.87360592\n",
      "2019-04-16 16:08:47,961 epoch 37 - iter 72/121 - loss 7.83773648\n",
      "2019-04-16 16:09:03,307 epoch 37 - iter 84/121 - loss 8.17335354\n",
      "2019-04-16 16:09:09,983 epoch 37 - iter 96/121 - loss 8.19894881\n",
      "2019-04-16 16:09:15,736 epoch 37 - iter 108/121 - loss 8.12097074\n",
      "2019-04-16 16:09:23,564 epoch 37 - iter 120/121 - loss 8.16008690\n",
      "2019-04-16 16:09:23,586 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:09:23,587 EPOCH 37 done: loss 8.1601 - lr 0.0125 - bad epochs 1\n",
      "2019-04-16 16:09:32,309 DEV  : loss 6.41235828 - f-score 0.5154 - acc 0.3472\n",
      "2019-04-16 16:09:40,658 TEST : loss 6.27812719 - f-score 0.5419 - acc 0.3717\n",
      "2019-04-16 16:09:50,040 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:09:50,601 epoch 38 - iter 0/121 - loss 6.71778202\n",
      "2019-04-16 16:09:57,745 epoch 38 - iter 12/121 - loss 8.28712746\n",
      "2019-04-16 16:10:02,305 epoch 38 - iter 24/121 - loss 8.00014746\n",
      "2019-04-16 16:10:18,335 epoch 38 - iter 36/121 - loss 8.70575739\n",
      "2019-04-16 16:10:24,390 epoch 38 - iter 48/121 - loss 8.37538254\n",
      "2019-04-16 16:10:29,019 epoch 38 - iter 60/121 - loss 8.37036578\n",
      "2019-04-16 16:10:33,194 epoch 38 - iter 72/121 - loss 8.18822510\n",
      "2019-04-16 16:10:36,959 epoch 38 - iter 84/121 - loss 8.14474588\n",
      "2019-04-16 16:10:42,958 epoch 38 - iter 96/121 - loss 8.14275375\n",
      "2019-04-16 16:10:51,600 epoch 38 - iter 108/121 - loss 8.25722604\n",
      "2019-04-16 16:10:56,680 epoch 38 - iter 120/121 - loss 8.15795852\n",
      "2019-04-16 16:10:56,702 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:10:56,703 EPOCH 38 done: loss 8.1580 - lr 0.0125 - bad epochs 0\n",
      "2019-04-16 16:11:03,117 DEV  : loss 6.32789183 - f-score 0.5229 - acc 0.3540\n",
      "2019-04-16 16:11:09,325 TEST : loss 6.31643009 - f-score 0.5418 - acc 0.3716\n",
      "2019-04-16 16:11:19,007 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:11:19,496 epoch 39 - iter 0/121 - loss 8.92296982\n",
      "2019-04-16 16:11:25,365 epoch 39 - iter 12/121 - loss 7.99391468\n",
      "2019-04-16 16:11:30,728 epoch 39 - iter 24/121 - loss 7.95904367\n",
      "2019-04-16 16:11:34,456 epoch 39 - iter 36/121 - loss 7.86110454\n",
      "2019-04-16 16:11:44,057 epoch 39 - iter 48/121 - loss 8.22666997\n",
      "2019-04-16 16:11:49,988 epoch 39 - iter 60/121 - loss 8.20180866\n",
      "2019-04-16 16:11:56,945 epoch 39 - iter 72/121 - loss 8.08487409\n",
      "2019-04-16 16:12:02,397 epoch 39 - iter 84/121 - loss 8.10468924\n",
      "2019-04-16 16:12:07,999 epoch 39 - iter 96/121 - loss 8.07482058\n",
      "2019-04-16 16:12:13,907 epoch 39 - iter 108/121 - loss 7.96376885\n",
      "2019-04-16 16:12:24,720 epoch 39 - iter 120/121 - loss 8.01034035\n",
      "2019-04-16 16:12:24,746 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:12:24,747 EPOCH 39 done: loss 8.0103 - lr 0.0125 - bad epochs 0\n",
      "2019-04-16 16:12:30,527 DEV  : loss 6.10593605 - f-score 0.5301 - acc 0.3606\n",
      "2019-04-16 16:12:37,381 TEST : loss 5.87406111 - f-score 0.5583 - acc 0.3872\n",
      "2019-04-16 16:12:46,563 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:12:47,129 epoch 40 - iter 0/121 - loss 7.80651712\n",
      "2019-04-16 16:12:51,440 epoch 40 - iter 12/121 - loss 7.10318422\n",
      "2019-04-16 16:12:55,911 epoch 40 - iter 24/121 - loss 7.41132362\n",
      "2019-04-16 16:13:01,644 epoch 40 - iter 36/121 - loss 7.41829232\n",
      "2019-04-16 16:13:08,604 epoch 40 - iter 48/121 - loss 7.64510929\n",
      "2019-04-16 16:13:13,367 epoch 40 - iter 60/121 - loss 7.76698278\n",
      "2019-04-16 16:13:22,613 epoch 40 - iter 72/121 - loss 7.86390605\n",
      "2019-04-16 16:13:26,519 epoch 40 - iter 84/121 - loss 7.85367072\n",
      "2019-04-16 16:13:33,372 epoch 40 - iter 96/121 - loss 7.90508739\n",
      "2019-04-16 16:13:42,029 epoch 40 - iter 108/121 - loss 8.09008755\n",
      "2019-04-16 16:13:51,880 epoch 40 - iter 120/121 - loss 8.21004713\n",
      "2019-04-16 16:13:51,899 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:13:51,900 EPOCH 40 done: loss 8.2100 - lr 0.0125 - bad epochs 0\n",
      "2019-04-16 16:14:00,846 DEV  : loss 5.66760397 - f-score 0.5793 - acc 0.4078\n",
      "2019-04-16 16:14:08,707 TEST : loss 6.05298376 - f-score 0.5590 - acc 0.3880\n",
      "2019-04-16 16:14:08,710 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:14:09,066 epoch 41 - iter 0/121 - loss 9.24752617\n",
      "2019-04-16 16:14:15,424 epoch 41 - iter 12/121 - loss 7.89015062\n",
      "2019-04-16 16:14:21,124 epoch 41 - iter 24/121 - loss 7.66266838\n",
      "2019-04-16 16:14:26,769 epoch 41 - iter 36/121 - loss 7.64954348\n",
      "2019-04-16 16:14:32,358 epoch 41 - iter 48/121 - loss 7.88028361\n",
      "2019-04-16 16:14:37,953 epoch 41 - iter 60/121 - loss 7.74938612\n",
      "2019-04-16 16:14:43,401 epoch 41 - iter 72/121 - loss 7.73191612\n",
      "2019-04-16 16:14:49,649 epoch 41 - iter 84/121 - loss 7.79582320\n",
      "2019-04-16 16:14:53,862 epoch 41 - iter 96/121 - loss 7.73336636\n",
      "2019-04-16 16:15:01,022 epoch 41 - iter 108/121 - loss 7.83530213\n",
      "2019-04-16 16:15:16,826 epoch 41 - iter 120/121 - loss 8.10394363\n",
      "2019-04-16 16:15:16,853 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:15:16,854 EPOCH 41 done: loss 8.1039 - lr 0.0125 - bad epochs 1\n",
      "2019-04-16 16:15:22,877 DEV  : loss 5.70944500 - f-score 0.5564 - acc 0.3855\n",
      "2019-04-16 16:15:29,934 TEST : loss 5.89928818 - f-score 0.5651 - acc 0.3938\n",
      "2019-04-16 16:15:29,936 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:15:30,502 epoch 42 - iter 0/121 - loss 6.03728104\n",
      "2019-04-16 16:15:37,706 epoch 42 - iter 12/121 - loss 7.72725472\n",
      "2019-04-16 16:15:44,187 epoch 42 - iter 24/121 - loss 7.92509604\n",
      "2019-04-16 16:15:48,383 epoch 42 - iter 36/121 - loss 7.84214048\n",
      "2019-04-16 16:15:55,145 epoch 42 - iter 48/121 - loss 8.06612058\n",
      "2019-04-16 16:16:04,921 epoch 42 - iter 60/121 - loss 8.14050169\n",
      "2019-04-16 16:16:12,434 epoch 42 - iter 72/121 - loss 8.17310450\n",
      "2019-04-16 16:16:18,311 epoch 42 - iter 84/121 - loss 8.24808647\n",
      "2019-04-16 16:16:22,537 epoch 42 - iter 96/121 - loss 8.06863671\n",
      "2019-04-16 16:16:28,184 epoch 42 - iter 108/121 - loss 8.00602946\n",
      "2019-04-16 16:16:35,458 epoch 42 - iter 120/121 - loss 8.05868565\n",
      "2019-04-16 16:16:35,479 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:16:35,480 EPOCH 42 done: loss 8.0587 - lr 0.0125 - bad epochs 2\n",
      "2019-04-16 16:16:43,220 DEV  : loss 5.84120846 - f-score 0.5538 - acc 0.3829\n",
      "2019-04-16 16:16:52,055 TEST : loss 5.97916555 - f-score 0.5712 - acc 0.3998\n",
      "2019-04-16 16:16:52,058 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:16:54,062 epoch 43 - iter 0/121 - loss 11.93950748\n",
      "2019-04-16 16:17:02,199 epoch 43 - iter 12/121 - loss 8.74669203\n",
      "2019-04-16 16:17:06,230 epoch 43 - iter 24/121 - loss 8.35858253\n",
      "2019-04-16 16:17:11,019 epoch 43 - iter 36/121 - loss 8.19720991\n",
      "2019-04-16 16:17:16,920 epoch 43 - iter 48/121 - loss 8.16273969\n",
      "2019-04-16 16:17:22,410 epoch 43 - iter 60/121 - loss 8.05874105\n",
      "2019-04-16 16:17:27,904 epoch 43 - iter 72/121 - loss 7.94279203\n",
      "2019-04-16 16:17:31,902 epoch 43 - iter 84/121 - loss 7.82443525\n",
      "2019-04-16 16:17:41,566 epoch 43 - iter 96/121 - loss 8.06397787\n",
      "2019-04-16 16:17:47,245 epoch 43 - iter 108/121 - loss 7.98678811\n",
      "2019-04-16 16:17:54,210 epoch 43 - iter 120/121 - loss 8.00071257\n",
      "2019-04-16 16:17:54,231 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:17:54,232 EPOCH 43 done: loss 8.0007 - lr 0.0125 - bad epochs 3\n",
      "2019-04-16 16:18:02,073 DEV  : loss 5.80971384 - f-score 0.5685 - acc 0.3971\n",
      "2019-04-16 16:18:10,659 TEST : loss 5.78031349 - f-score 0.5765 - acc 0.4049\n",
      "2019-04-16 16:18:19,965 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:18:20,479 epoch 44 - iter 0/121 - loss 8.46678925\n",
      "2019-04-16 16:18:26,435 epoch 44 - iter 12/121 - loss 7.27906429\n",
      "2019-04-16 16:18:32,767 epoch 44 - iter 24/121 - loss 7.34966513\n",
      "2019-04-16 16:18:38,361 epoch 44 - iter 36/121 - loss 7.54525847\n",
      "2019-04-16 16:18:54,039 epoch 44 - iter 48/121 - loss 8.18775698\n",
      "2019-04-16 16:18:59,718 epoch 44 - iter 60/121 - loss 8.07813827\n",
      "2019-04-16 16:19:06,728 epoch 44 - iter 72/121 - loss 8.05647198\n",
      "2019-04-16 16:19:12,228 epoch 44 - iter 84/121 - loss 7.99972194\n",
      "2019-04-16 16:19:20,654 epoch 44 - iter 96/121 - loss 8.03854387\n",
      "2019-04-16 16:19:25,079 epoch 44 - iter 108/121 - loss 7.94256089\n",
      "2019-04-16 16:19:29,419 epoch 44 - iter 120/121 - loss 7.94461980\n",
      "2019-04-16 16:19:29,439 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:19:29,440 EPOCH 44 done: loss 7.9446 - lr 0.0125 - bad epochs 0\n",
      "2019-04-16 16:19:35,541 DEV  : loss 5.53776884 - f-score 0.5727 - acc 0.4013\n",
      "2019-04-16 16:19:43,921 TEST : loss 5.86140060 - f-score 0.5965 - acc 0.4250\n",
      "2019-04-16 16:19:53,275 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:19:53,499 epoch 45 - iter 0/121 - loss 9.37515831\n",
      "2019-04-16 16:19:57,769 epoch 45 - iter 12/121 - loss 7.82921560\n",
      "2019-04-16 16:20:01,784 epoch 45 - iter 24/121 - loss 8.08694586\n",
      "2019-04-16 16:20:09,108 epoch 45 - iter 36/121 - loss 7.92421583\n",
      "2019-04-16 16:20:14,950 epoch 45 - iter 48/121 - loss 7.61372895\n",
      "2019-04-16 16:20:20,376 epoch 45 - iter 60/121 - loss 7.54700077\n",
      "2019-04-16 16:20:24,912 epoch 45 - iter 72/121 - loss 7.72836849\n",
      "2019-04-16 16:20:28,436 epoch 45 - iter 84/121 - loss 7.72029902\n",
      "2019-04-16 16:20:32,709 epoch 45 - iter 96/121 - loss 7.68005069\n",
      "2019-04-16 16:20:43,928 epoch 45 - iter 108/121 - loss 7.82864912\n",
      "2019-04-16 16:20:55,629 epoch 45 - iter 120/121 - loss 8.01141519\n",
      "2019-04-16 16:20:55,650 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:20:55,651 EPOCH 45 done: loss 8.0114 - lr 0.0125 - bad epochs 0\n",
      "2019-04-16 16:21:01,963 DEV  : loss 5.89310265 - f-score 0.5592 - acc 0.3881\n",
      "2019-04-16 16:21:09,294 TEST : loss 5.95094919 - f-score 0.5643 - acc 0.3930\n",
      "2019-04-16 16:21:09,297 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:21:09,619 epoch 46 - iter 0/121 - loss 6.96066093\n",
      "2019-04-16 16:21:15,082 epoch 46 - iter 12/121 - loss 7.68735644\n",
      "2019-04-16 16:21:26,134 epoch 46 - iter 24/121 - loss 8.25787361\n",
      "2019-04-16 16:21:30,715 epoch 46 - iter 36/121 - loss 7.85919174\n",
      "2019-04-16 16:21:39,069 epoch 46 - iter 48/121 - loss 8.16190545\n",
      "2019-04-16 16:21:46,466 epoch 46 - iter 60/121 - loss 8.14126530\n",
      "2019-04-16 16:21:52,992 epoch 46 - iter 72/121 - loss 7.98942167\n",
      "2019-04-16 16:21:58,710 epoch 46 - iter 84/121 - loss 8.00321249\n",
      "2019-04-16 16:22:03,545 epoch 46 - iter 96/121 - loss 7.90176380\n",
      "2019-04-16 16:22:08,771 epoch 46 - iter 108/121 - loss 7.84129613\n",
      "2019-04-16 16:22:15,960 epoch 46 - iter 120/121 - loss 7.83238289\n",
      "2019-04-16 16:22:15,982 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:22:15,983 EPOCH 46 done: loss 7.8324 - lr 0.0125 - bad epochs 1\n",
      "2019-04-16 16:22:23,785 DEV  : loss 5.42363596 - f-score 0.5887 - acc 0.4171\n",
      "2019-04-16 16:22:32,466 TEST : loss 5.65008736 - f-score 0.5798 - acc 0.4083\n",
      "2019-04-16 16:22:41,994 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:22:42,548 epoch 47 - iter 0/121 - loss 6.47832251\n",
      "2019-04-16 16:22:55,047 epoch 47 - iter 12/121 - loss 8.83337637\n",
      "2019-04-16 16:23:00,300 epoch 47 - iter 24/121 - loss 8.06628143\n",
      "2019-04-16 16:23:06,442 epoch 47 - iter 36/121 - loss 7.96050270\n",
      "2019-04-16 16:23:14,215 epoch 47 - iter 48/121 - loss 7.98470790\n",
      "2019-04-16 16:23:20,459 epoch 47 - iter 60/121 - loss 8.09276217\n",
      "2019-04-16 16:23:24,453 epoch 47 - iter 72/121 - loss 7.99000142\n",
      "2019-04-16 16:23:31,162 epoch 47 - iter 84/121 - loss 8.09444090\n",
      "2019-04-16 16:23:36,887 epoch 47 - iter 96/121 - loss 8.01310781\n",
      "2019-04-16 16:23:46,834 epoch 47 - iter 108/121 - loss 8.00649446\n",
      "2019-04-16 16:23:51,248 epoch 47 - iter 120/121 - loss 7.98877030\n",
      "2019-04-16 16:23:51,269 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:23:51,270 EPOCH 47 done: loss 7.9888 - lr 0.0125 - bad epochs 0\n",
      "2019-04-16 16:23:57,026 DEV  : loss 6.31953621 - f-score 0.5506 - acc 0.3799\n",
      "2019-04-16 16:24:04,051 TEST : loss 6.48755884 - f-score 0.5473 - acc 0.3767\n",
      "2019-04-16 16:24:04,055 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:24:05,492 epoch 48 - iter 0/121 - loss 10.03525257\n",
      "2019-04-16 16:24:10,895 epoch 48 - iter 12/121 - loss 7.84861150\n",
      "2019-04-16 16:24:16,719 epoch 48 - iter 24/121 - loss 7.63511086\n",
      "2019-04-16 16:24:25,739 epoch 48 - iter 36/121 - loss 8.25645299\n",
      "2019-04-16 16:24:29,945 epoch 48 - iter 48/121 - loss 8.21816331\n",
      "2019-04-16 16:24:39,084 epoch 48 - iter 60/121 - loss 8.28407012\n",
      "2019-04-16 16:24:44,588 epoch 48 - iter 72/121 - loss 8.18324996\n",
      "2019-04-16 16:24:54,851 epoch 48 - iter 84/121 - loss 8.11434291\n",
      "2019-04-16 16:24:59,946 epoch 48 - iter 96/121 - loss 8.13929779\n",
      "2019-04-16 16:25:03,607 epoch 48 - iter 108/121 - loss 8.04957968\n",
      "2019-04-16 16:25:07,917 epoch 48 - iter 120/121 - loss 8.00922760\n",
      "2019-04-16 16:25:07,937 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:25:07,938 EPOCH 48 done: loss 8.0092 - lr 0.0125 - bad epochs 1\n",
      "2019-04-16 16:25:16,979 DEV  : loss 5.50409603 - f-score 0.5747 - acc 0.4032\n",
      "2019-04-16 16:25:24,298 TEST : loss 5.77814960 - f-score 0.6049 - acc 0.4336\n",
      "2019-04-16 16:25:24,300 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:25:24,495 epoch 49 - iter 0/121 - loss 6.24315214\n",
      "2019-04-16 16:25:28,202 epoch 49 - iter 12/121 - loss 7.13656150\n",
      "2019-04-16 16:25:31,793 epoch 49 - iter 24/121 - loss 7.26140112\n",
      "2019-04-16 16:25:37,113 epoch 49 - iter 36/121 - loss 7.41214567\n",
      "2019-04-16 16:25:42,871 epoch 49 - iter 48/121 - loss 7.56148669\n",
      "2019-04-16 16:25:49,813 epoch 49 - iter 60/121 - loss 7.79919800\n",
      "2019-04-16 16:25:54,636 epoch 49 - iter 72/121 - loss 7.73740522\n",
      "2019-04-16 16:25:59,396 epoch 49 - iter 84/121 - loss 7.82671372\n",
      "2019-04-16 16:26:05,484 epoch 49 - iter 96/121 - loss 7.79753782\n",
      "2019-04-16 16:26:11,208 epoch 49 - iter 108/121 - loss 7.79091440\n",
      "2019-04-16 16:26:26,510 epoch 49 - iter 120/121 - loss 7.96329483\n",
      "2019-04-16 16:26:26,531 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:26:26,532 EPOCH 49 done: loss 7.9633 - lr 0.0125 - bad epochs 2\n",
      "2019-04-16 16:26:32,424 DEV  : loss 5.89658594 - f-score 0.5829 - acc 0.4113\n",
      "2019-04-16 16:26:38,675 TEST : loss 5.86869049 - f-score 0.5916 - acc 0.4201\n",
      "2019-04-16 16:26:38,677 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:26:39,022 epoch 50 - iter 0/121 - loss 7.96983576\n",
      "2019-04-16 16:26:44,911 epoch 50 - iter 12/121 - loss 7.95810333\n",
      "2019-04-16 16:26:53,465 epoch 50 - iter 24/121 - loss 7.98065683\n",
      "2019-04-16 16:27:04,424 epoch 50 - iter 36/121 - loss 8.10984419\n",
      "2019-04-16 16:27:11,683 epoch 50 - iter 48/121 - loss 7.85735612\n",
      "2019-04-16 16:27:16,920 epoch 50 - iter 60/121 - loss 7.77972542\n",
      "2019-04-16 16:27:24,856 epoch 50 - iter 72/121 - loss 7.84627530\n",
      "2019-04-16 16:27:28,744 epoch 50 - iter 84/121 - loss 7.85523707\n",
      "2019-04-16 16:27:34,230 epoch 50 - iter 96/121 - loss 7.87895814\n",
      "2019-04-16 16:27:39,800 epoch 50 - iter 108/121 - loss 7.82714479\n",
      "2019-04-16 16:27:45,736 epoch 50 - iter 120/121 - loss 7.85970879\n",
      "2019-04-16 16:27:45,758 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:27:45,759 EPOCH 50 done: loss 7.8597 - lr 0.0125 - bad epochs 3\n",
      "2019-04-16 16:27:53,188 DEV  : loss 5.86241436 - f-score 0.5650 - acc 0.3938\n",
      "2019-04-16 16:27:59,017 TEST : loss 5.67275238 - f-score 0.5914 - acc 0.4199\n",
      "Epoch    49: reducing learning rate of group 0 to 6.2500e-03.\n",
      "2019-04-16 16:27:59,020 Epoch 49: reducing weight decay factor of group 0 to 6.2500e-03.\n",
      "2019-04-16 16:27:59,021 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:27:59,246 epoch 51 - iter 0/121 - loss 8.61184883\n",
      "2019-04-16 16:28:02,650 epoch 51 - iter 12/121 - loss 7.17418638\n",
      "2019-04-16 16:28:11,272 epoch 51 - iter 24/121 - loss 7.17874081\n",
      "2019-04-16 16:28:23,114 epoch 51 - iter 36/121 - loss 7.69728909\n",
      "2019-04-16 16:28:29,055 epoch 51 - iter 48/121 - loss 7.75350948\n",
      "2019-04-16 16:28:35,783 epoch 51 - iter 60/121 - loss 7.47142574\n",
      "2019-04-16 16:28:45,972 epoch 51 - iter 72/121 - loss 7.51822259\n",
      "2019-04-16 16:28:51,116 epoch 51 - iter 84/121 - loss 7.33250396\n",
      "2019-04-16 16:28:56,486 epoch 51 - iter 96/121 - loss 7.28906934\n",
      "2019-04-16 16:29:03,208 epoch 51 - iter 108/121 - loss 7.24582397\n",
      "2019-04-16 16:29:08,248 epoch 51 - iter 120/121 - loss 7.23869634\n",
      "2019-04-16 16:29:08,267 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:29:08,268 EPOCH 51 done: loss 7.2387 - lr 0.0063 - bad epochs 0\n",
      "2019-04-16 16:29:17,243 DEV  : loss 5.05006218 - f-score 0.5917 - acc 0.4201\n",
      "2019-04-16 16:29:25,509 TEST : loss 5.11685038 - f-score 0.6434 - acc 0.4742\n",
      "2019-04-16 16:29:34,747 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:29:35,325 epoch 52 - iter 0/121 - loss 6.05095768\n",
      "2019-04-16 16:29:41,671 epoch 52 - iter 12/121 - loss 6.22789086\n",
      "2019-04-16 16:29:45,605 epoch 52 - iter 24/121 - loss 6.44157259\n",
      "2019-04-16 16:29:50,199 epoch 52 - iter 36/121 - loss 6.38806990\n",
      "2019-04-16 16:29:56,569 epoch 52 - iter 48/121 - loss 6.33410066\n",
      "2019-04-16 16:30:03,953 epoch 52 - iter 60/121 - loss 6.57177355\n",
      "2019-04-16 16:30:10,536 epoch 52 - iter 72/121 - loss 6.69339671\n",
      "2019-04-16 16:30:17,713 epoch 52 - iter 84/121 - loss 6.84368341\n",
      "2019-04-16 16:30:21,378 epoch 52 - iter 96/121 - loss 6.82515994\n",
      "2019-04-16 16:30:33,645 epoch 52 - iter 108/121 - loss 6.95419669\n",
      "2019-04-16 16:30:39,477 epoch 52 - iter 120/121 - loss 6.93313914\n",
      "2019-04-16 16:30:39,497 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:30:39,497 EPOCH 52 done: loss 6.9331 - lr 0.0063 - bad epochs 0\n",
      "2019-04-16 16:30:46,069 DEV  : loss 5.14157915 - f-score 0.6023 - acc 0.4309\n",
      "2019-04-16 16:30:51,916 TEST : loss 4.99641514 - f-score 0.6294 - acc 0.4592\n",
      "2019-04-16 16:31:01,288 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:31:01,828 epoch 53 - iter 0/121 - loss 5.39326096\n",
      "2019-04-16 16:31:07,221 epoch 53 - iter 12/121 - loss 6.00395558\n",
      "2019-04-16 16:31:12,345 epoch 53 - iter 24/121 - loss 6.69781630\n",
      "2019-04-16 16:31:24,520 epoch 53 - iter 36/121 - loss 6.90099167\n",
      "2019-04-16 16:31:29,561 epoch 53 - iter 48/121 - loss 6.84843478\n",
      "2019-04-16 16:31:36,151 epoch 53 - iter 60/121 - loss 6.93779694\n",
      "2019-04-16 16:31:43,255 epoch 53 - iter 72/121 - loss 6.88597237\n",
      "2019-04-16 16:31:49,093 epoch 53 - iter 84/121 - loss 6.78047731\n",
      "2019-04-16 16:31:53,542 epoch 53 - iter 96/121 - loss 6.79273575\n",
      "2019-04-16 16:32:01,398 epoch 53 - iter 108/121 - loss 6.87982272\n",
      "2019-04-16 16:32:05,249 epoch 53 - iter 120/121 - loss 6.91782422\n",
      "2019-04-16 16:32:05,268 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:32:05,269 EPOCH 53 done: loss 6.9178 - lr 0.0063 - bad epochs 0\n",
      "2019-04-16 16:32:13,495 DEV  : loss 5.21240997 - f-score 0.5874 - acc 0.4158\n",
      "2019-04-16 16:32:21,700 TEST : loss 5.32244062 - f-score 0.6257 - acc 0.4553\n",
      "2019-04-16 16:32:30,960 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:32:31,404 epoch 54 - iter 0/121 - loss 8.89143372\n",
      "2019-04-16 16:32:37,624 epoch 54 - iter 12/121 - loss 7.11881065\n",
      "2019-04-16 16:32:41,677 epoch 54 - iter 24/121 - loss 6.80791094\n",
      "2019-04-16 16:32:47,338 epoch 54 - iter 36/121 - loss 6.70287203\n",
      "2019-04-16 16:32:59,964 epoch 54 - iter 48/121 - loss 7.15856854\n",
      "2019-04-16 16:33:03,881 epoch 54 - iter 60/121 - loss 6.90817716\n",
      "2019-04-16 16:33:10,038 epoch 54 - iter 72/121 - loss 6.92298621\n",
      "2019-04-16 16:33:16,360 epoch 54 - iter 84/121 - loss 6.77731673\n",
      "2019-04-16 16:33:21,998 epoch 54 - iter 96/121 - loss 6.76427495\n",
      "2019-04-16 16:33:31,096 epoch 54 - iter 108/121 - loss 6.89067456\n",
      "2019-04-16 16:33:35,187 epoch 54 - iter 120/121 - loss 6.92239904\n",
      "2019-04-16 16:33:35,206 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:33:35,207 EPOCH 54 done: loss 6.9224 - lr 0.0063 - bad epochs 0\n",
      "2019-04-16 16:33:43,263 DEV  : loss 5.21777248 - f-score 0.6127 - acc 0.4417\n",
      "2019-04-16 16:33:51,679 TEST : loss 5.09784746 - f-score 0.6473 - acc 0.4785\n",
      "2019-04-16 16:33:51,682 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:33:51,961 epoch 55 - iter 0/121 - loss 6.20034218\n",
      "2019-04-16 16:33:55,999 epoch 55 - iter 12/121 - loss 6.32596544\n",
      "2019-04-16 16:33:59,355 epoch 55 - iter 24/121 - loss 6.37886662\n",
      "2019-04-16 16:34:04,067 epoch 55 - iter 36/121 - loss 6.67726572\n",
      "2019-04-16 16:34:10,116 epoch 55 - iter 48/121 - loss 6.80216781\n",
      "2019-04-16 16:34:15,844 epoch 55 - iter 60/121 - loss 6.64010926\n",
      "2019-04-16 16:34:21,264 epoch 55 - iter 72/121 - loss 6.46802889\n",
      "2019-04-16 16:34:37,275 epoch 55 - iter 84/121 - loss 6.68714096\n",
      "2019-04-16 16:34:44,811 epoch 55 - iter 96/121 - loss 6.67970306\n",
      "2019-04-16 16:34:49,412 epoch 55 - iter 108/121 - loss 6.60095806\n",
      "2019-04-16 16:34:57,228 epoch 55 - iter 120/121 - loss 6.67536988\n",
      "2019-04-16 16:34:57,248 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:34:57,248 EPOCH 55 done: loss 6.6754 - lr 0.0063 - bad epochs 1\n",
      "2019-04-16 16:35:05,939 DEV  : loss 5.10830927 - f-score 0.6063 - acc 0.4350\n",
      "2019-04-16 16:35:14,152 TEST : loss 5.02612686 - f-score 0.6324 - acc 0.4624\n",
      "2019-04-16 16:35:25,670 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:35:26,515 epoch 56 - iter 0/121 - loss 8.99927139\n",
      "2019-04-16 16:35:38,099 epoch 56 - iter 12/121 - loss 7.00943903\n",
      "2019-04-16 16:35:50,166 epoch 56 - iter 24/121 - loss 7.76172670\n",
      "2019-04-16 16:35:56,652 epoch 56 - iter 36/121 - loss 7.39707726\n",
      "2019-04-16 16:36:02,880 epoch 56 - iter 48/121 - loss 7.12554635\n",
      "2019-04-16 16:36:07,962 epoch 56 - iter 60/121 - loss 6.96697043\n",
      "2019-04-16 16:36:12,044 epoch 56 - iter 72/121 - loss 6.86797980\n",
      "2019-04-16 16:36:15,915 epoch 56 - iter 84/121 - loss 6.92937650\n",
      "2019-04-16 16:36:19,855 epoch 56 - iter 96/121 - loss 6.91365494\n",
      "2019-04-16 16:36:25,410 epoch 56 - iter 108/121 - loss 6.84463009\n",
      "2019-04-16 16:36:32,865 epoch 56 - iter 120/121 - loss 6.81939980\n",
      "2019-04-16 16:36:32,885 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:36:32,886 EPOCH 56 done: loss 6.8194 - lr 0.0063 - bad epochs 0\n",
      "2019-04-16 16:36:41,081 DEV  : loss 4.87572622 - f-score 0.6378 - acc 0.4682\n",
      "2019-04-16 16:36:47,136 TEST : loss 4.84969997 - f-score 0.6694 - acc 0.5031\n",
      "2019-04-16 16:36:47,139 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:36:47,353 epoch 57 - iter 0/121 - loss 4.56553364\n",
      "2019-04-16 16:36:52,115 epoch 57 - iter 12/121 - loss 6.63521932\n",
      "2019-04-16 16:36:57,797 epoch 57 - iter 24/121 - loss 6.36996700\n",
      "2019-04-16 16:37:03,468 epoch 57 - iter 36/121 - loss 6.25049090\n",
      "2019-04-16 16:37:10,520 epoch 57 - iter 48/121 - loss 6.46063847\n",
      "2019-04-16 16:37:15,782 epoch 57 - iter 60/121 - loss 6.42787637\n",
      "2019-04-16 16:37:21,473 epoch 57 - iter 72/121 - loss 6.45171701\n",
      "2019-04-16 16:37:37,347 epoch 57 - iter 84/121 - loss 6.75391530\n",
      "2019-04-16 16:37:43,339 epoch 57 - iter 96/121 - loss 6.74784430\n",
      "2019-04-16 16:37:50,273 epoch 57 - iter 108/121 - loss 6.73314686\n",
      "2019-04-16 16:37:55,616 epoch 57 - iter 120/121 - loss 6.71805088\n",
      "2019-04-16 16:37:55,637 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:37:55,637 EPOCH 57 done: loss 6.7181 - lr 0.0063 - bad epochs 1\n",
      "2019-04-16 16:38:03,773 DEV  : loss 4.79411697 - f-score 0.6481 - acc 0.4794\n",
      "2019-04-16 16:38:12,390 TEST : loss 4.75450420 - f-score 0.6783 - acc 0.5133\n",
      "2019-04-16 16:38:12,393 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:38:12,714 epoch 58 - iter 0/121 - loss 4.34383869\n",
      "2019-04-16 16:38:16,781 epoch 58 - iter 12/121 - loss 5.86551967\n",
      "2019-04-16 16:38:27,955 epoch 58 - iter 24/121 - loss 6.39891027\n",
      "2019-04-16 16:38:33,638 epoch 58 - iter 36/121 - loss 6.51657199\n",
      "2019-04-16 16:38:38,489 epoch 58 - iter 48/121 - loss 6.62745029\n",
      "2019-04-16 16:38:44,068 epoch 58 - iter 60/121 - loss 6.67357516\n",
      "2019-04-16 16:38:50,842 epoch 58 - iter 72/121 - loss 6.64941418\n",
      "2019-04-16 16:38:59,053 epoch 58 - iter 84/121 - loss 6.66451026\n",
      "2019-04-16 16:39:09,930 epoch 58 - iter 96/121 - loss 6.78916632\n",
      "2019-04-16 16:39:17,291 epoch 58 - iter 108/121 - loss 6.78846609\n",
      "2019-04-16 16:39:22,707 epoch 58 - iter 120/121 - loss 6.74368142\n",
      "2019-04-16 16:39:22,726 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:39:22,727 EPOCH 58 done: loss 6.7437 - lr 0.0063 - bad epochs 2\n",
      "2019-04-16 16:39:31,010 DEV  : loss 4.78065825 - f-score 0.6315 - acc 0.4615\n",
      "2019-04-16 16:39:39,567 TEST : loss 4.60605955 - f-score 0.6514 - acc 0.4830\n",
      "2019-04-16 16:39:39,569 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:39:39,879 epoch 59 - iter 0/121 - loss 6.97467232\n",
      "2019-04-16 16:39:46,068 epoch 59 - iter 12/121 - loss 6.65232739\n",
      "2019-04-16 16:39:49,816 epoch 59 - iter 24/121 - loss 6.60594992\n",
      "2019-04-16 16:39:55,862 epoch 59 - iter 36/121 - loss 6.64887713\n",
      "2019-04-16 16:40:02,980 epoch 59 - iter 48/121 - loss 6.62779123\n",
      "2019-04-16 16:40:09,209 epoch 59 - iter 60/121 - loss 6.62842120\n",
      "2019-04-16 16:40:13,435 epoch 59 - iter 72/121 - loss 6.59986094\n",
      "2019-04-16 16:40:24,567 epoch 59 - iter 84/121 - loss 6.71492690\n",
      "2019-04-16 16:40:29,463 epoch 59 - iter 96/121 - loss 6.58966964\n",
      "2019-04-16 16:40:34,098 epoch 59 - iter 108/121 - loss 6.58232087\n",
      "2019-04-16 16:40:45,210 epoch 59 - iter 120/121 - loss 6.69105555\n",
      "2019-04-16 16:40:45,230 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:40:45,230 EPOCH 59 done: loss 6.6911 - lr 0.0063 - bad epochs 3\n",
      "2019-04-16 16:40:53,906 DEV  : loss 5.15039301 - f-score 0.6183 - acc 0.4475\n",
      "2019-04-16 16:41:02,025 TEST : loss 4.83460283 - f-score 0.6438 - acc 0.4747\n",
      "Epoch    58: reducing learning rate of group 0 to 3.1250e-03.\n",
      "2019-04-16 16:41:02,028 Epoch 58: reducing weight decay factor of group 0 to 3.1250e-03.\n",
      "2019-04-16 16:41:02,028 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:41:02,625 epoch 60 - iter 0/121 - loss 5.20203543\n",
      "2019-04-16 16:41:08,410 epoch 60 - iter 12/121 - loss 6.50524470\n",
      "2019-04-16 16:41:14,996 epoch 60 - iter 24/121 - loss 6.48218752\n",
      "2019-04-16 16:41:23,597 epoch 60 - iter 36/121 - loss 6.73204787\n",
      "2019-04-16 16:41:27,541 epoch 60 - iter 48/121 - loss 6.46207282\n",
      "2019-04-16 16:41:34,637 epoch 60 - iter 60/121 - loss 6.37313065\n",
      "2019-04-16 16:41:40,908 epoch 60 - iter 72/121 - loss 6.27636672\n",
      "2019-04-16 16:41:46,185 epoch 60 - iter 84/121 - loss 6.23383670\n",
      "2019-04-16 16:41:56,999 epoch 60 - iter 96/121 - loss 6.25314122\n",
      "2019-04-16 16:42:03,116 epoch 60 - iter 108/121 - loss 6.23129975\n",
      "2019-04-16 16:42:09,553 epoch 60 - iter 120/121 - loss 6.20660962\n",
      "2019-04-16 16:42:09,577 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:42:09,578 EPOCH 60 done: loss 6.2066 - lr 0.0031 - bad epochs 0\n",
      "2019-04-16 16:42:17,974 DEV  : loss 4.90682650 - f-score 0.6269 - acc 0.4566\n",
      "2019-04-16 16:42:26,438 TEST : loss 4.67839146 - f-score 0.6663 - acc 0.4996\n",
      "2019-04-16 16:42:35,673 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:42:36,154 epoch 61 - iter 0/121 - loss 7.75338364\n",
      "2019-04-16 16:42:42,577 epoch 61 - iter 12/121 - loss 5.93829107\n",
      "2019-04-16 16:42:48,077 epoch 61 - iter 24/121 - loss 6.03768627\n",
      "2019-04-16 16:42:52,789 epoch 61 - iter 36/121 - loss 5.93609783\n",
      "2019-04-16 16:42:59,306 epoch 61 - iter 48/121 - loss 5.83836216\n",
      "2019-04-16 16:43:05,602 epoch 61 - iter 60/121 - loss 5.91032094\n",
      "2019-04-16 16:43:10,335 epoch 61 - iter 72/121 - loss 5.96567592\n",
      "2019-04-16 16:43:21,250 epoch 61 - iter 84/121 - loss 6.01349115\n",
      "2019-04-16 16:43:31,984 epoch 61 - iter 96/121 - loss 6.04513939\n",
      "2019-04-16 16:43:37,093 epoch 61 - iter 108/121 - loss 6.00333949\n",
      "2019-04-16 16:43:45,222 epoch 61 - iter 120/121 - loss 6.04041529\n",
      "2019-04-16 16:43:45,243 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:43:45,244 EPOCH 61 done: loss 6.0404 - lr 0.0031 - bad epochs 0\n",
      "2019-04-16 16:43:53,421 DEV  : loss 4.84965754 - f-score 0.6629 - acc 0.4958\n",
      "2019-04-16 16:44:01,929 TEST : loss 4.56007576 - f-score 0.6790 - acc 0.5139\n",
      "2019-04-16 16:44:11,123 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:44:11,610 epoch 62 - iter 0/121 - loss 6.44874001\n",
      "2019-04-16 16:44:16,448 epoch 62 - iter 12/121 - loss 5.74309828\n",
      "2019-04-16 16:44:20,041 epoch 62 - iter 24/121 - loss 5.65844174\n",
      "2019-04-16 16:44:23,523 epoch 62 - iter 36/121 - loss 5.64509439\n",
      "2019-04-16 16:44:29,521 epoch 62 - iter 48/121 - loss 5.66990291\n",
      "2019-04-16 16:44:37,222 epoch 62 - iter 60/121 - loss 5.78331580\n",
      "2019-04-16 16:44:42,457 epoch 62 - iter 72/121 - loss 5.64712773\n",
      "2019-04-16 16:44:50,916 epoch 62 - iter 84/121 - loss 5.87267328\n",
      "2019-04-16 16:44:55,040 epoch 62 - iter 96/121 - loss 5.86612311\n",
      "2019-04-16 16:45:09,495 epoch 62 - iter 108/121 - loss 5.92618008\n",
      "2019-04-16 16:45:14,872 epoch 62 - iter 120/121 - loss 5.92901532\n",
      "2019-04-16 16:45:14,892 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:45:14,892 EPOCH 62 done: loss 5.9290 - lr 0.0031 - bad epochs 0\n",
      "2019-04-16 16:45:22,942 DEV  : loss 4.84940481 - f-score 0.6516 - acc 0.4832\n",
      "2019-04-16 16:45:31,709 TEST : loss 4.54055309 - f-score 0.6836 - acc 0.5193\n",
      "2019-04-16 16:45:41,106 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:45:41,848 epoch 63 - iter 0/121 - loss 6.01148701\n",
      "2019-04-16 16:45:48,779 epoch 63 - iter 12/121 - loss 6.00680164\n",
      "2019-04-16 16:45:54,038 epoch 63 - iter 24/121 - loss 5.81228175\n",
      "2019-04-16 16:46:00,016 epoch 63 - iter 36/121 - loss 5.89109635\n",
      "2019-04-16 16:46:07,202 epoch 63 - iter 48/121 - loss 6.04671895\n",
      "2019-04-16 16:46:12,835 epoch 63 - iter 60/121 - loss 5.95885199\n",
      "2019-04-16 16:46:22,498 epoch 63 - iter 72/121 - loss 6.07193861\n",
      "2019-04-16 16:46:28,931 epoch 63 - iter 84/121 - loss 6.03433831\n",
      "2019-04-16 16:46:34,992 epoch 63 - iter 96/121 - loss 5.97041601\n",
      "2019-04-16 16:46:39,432 epoch 63 - iter 108/121 - loss 5.95423620\n",
      "2019-04-16 16:46:51,844 epoch 63 - iter 120/121 - loss 6.03475584\n",
      "2019-04-16 16:46:51,864 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:46:51,865 EPOCH 63 done: loss 6.0348 - lr 0.0031 - bad epochs 0\n",
      "2019-04-16 16:47:00,199 DEV  : loss 4.62025023 - f-score 0.6455 - acc 0.4766\n",
      "2019-04-16 16:47:08,771 TEST : loss 4.34151030 - f-score 0.6780 - acc 0.5129\n",
      "2019-04-16 16:47:08,774 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:47:09,243 epoch 64 - iter 0/121 - loss 3.03633165\n",
      "2019-04-16 16:47:19,530 epoch 64 - iter 12/121 - loss 5.75516952\n",
      "2019-04-16 16:47:23,752 epoch 64 - iter 24/121 - loss 5.51232324\n",
      "2019-04-16 16:47:27,270 epoch 64 - iter 36/121 - loss 5.52618471\n",
      "2019-04-16 16:47:31,970 epoch 64 - iter 48/121 - loss 5.63001013\n",
      "2019-04-16 16:47:43,876 epoch 64 - iter 60/121 - loss 5.91950671\n",
      "2019-04-16 16:47:50,012 epoch 64 - iter 72/121 - loss 5.99912557\n",
      "2019-04-16 16:47:56,016 epoch 64 - iter 84/121 - loss 6.02378904\n",
      "2019-04-16 16:48:04,754 epoch 64 - iter 96/121 - loss 6.08774855\n",
      "2019-04-16 16:48:10,314 epoch 64 - iter 108/121 - loss 6.03845250\n",
      "2019-04-16 16:48:14,567 epoch 64 - iter 120/121 - loss 6.00572056\n",
      "2019-04-16 16:48:14,595 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:48:14,596 EPOCH 64 done: loss 6.0057 - lr 0.0031 - bad epochs 1\n",
      "2019-04-16 16:48:23,164 DEV  : loss 4.42303705 - f-score 0.6632 - acc 0.4962\n",
      "2019-04-16 16:48:31,362 TEST : loss 4.20416355 - f-score 0.6988 - acc 0.5371\n",
      "2019-04-16 16:48:31,364 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:48:32,461 epoch 65 - iter 0/121 - loss 7.21769142\n",
      "2019-04-16 16:48:37,785 epoch 65 - iter 12/121 - loss 5.87507765\n",
      "2019-04-16 16:48:43,669 epoch 65 - iter 24/121 - loss 5.62132033\n",
      "2019-04-16 16:48:49,209 epoch 65 - iter 36/121 - loss 5.84580315\n",
      "2019-04-16 16:48:54,976 epoch 65 - iter 48/121 - loss 6.06577088\n",
      "2019-04-16 16:48:58,630 epoch 65 - iter 60/121 - loss 5.90203817\n",
      "2019-04-16 16:49:06,153 epoch 65 - iter 72/121 - loss 5.87549991\n",
      "2019-04-16 16:49:11,576 epoch 65 - iter 84/121 - loss 5.84913134\n",
      "2019-04-16 16:49:18,175 epoch 65 - iter 96/121 - loss 5.85298625\n",
      "2019-04-16 16:49:25,659 epoch 65 - iter 108/121 - loss 5.91138779\n",
      "2019-04-16 16:49:36,531 epoch 65 - iter 120/121 - loss 5.97410311\n",
      "2019-04-16 16:49:36,554 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:49:36,555 EPOCH 65 done: loss 5.9741 - lr 0.0031 - bad epochs 2\n",
      "2019-04-16 16:49:45,849 DEV  : loss 4.44689226 - f-score 0.6787 - acc 0.5136\n",
      "2019-04-16 16:49:52,315 TEST : loss 4.48374176 - f-score 0.6862 - acc 0.5223\n",
      "2019-04-16 16:49:52,317 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:49:52,575 epoch 66 - iter 0/121 - loss 4.67720890\n",
      "2019-04-16 16:49:57,968 epoch 66 - iter 12/121 - loss 5.83105262\n",
      "2019-04-16 16:50:04,484 epoch 66 - iter 24/121 - loss 6.03633475\n",
      "2019-04-16 16:50:11,292 epoch 66 - iter 36/121 - loss 5.83398506\n",
      "2019-04-16 16:50:17,303 epoch 66 - iter 48/121 - loss 5.90153279\n",
      "2019-04-16 16:50:22,262 epoch 66 - iter 60/121 - loss 6.08613993\n",
      "2019-04-16 16:50:28,413 epoch 66 - iter 72/121 - loss 6.00260714\n",
      "2019-04-16 16:50:34,793 epoch 66 - iter 84/121 - loss 5.96594506\n",
      "2019-04-16 16:50:39,659 epoch 66 - iter 96/121 - loss 5.89453844\n",
      "2019-04-16 16:50:55,307 epoch 66 - iter 108/121 - loss 6.04647325\n",
      "2019-04-16 16:51:01,705 epoch 66 - iter 120/121 - loss 6.03634568\n",
      "2019-04-16 16:51:01,726 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:51:01,727 EPOCH 66 done: loss 6.0363 - lr 0.0031 - bad epochs 3\n",
      "2019-04-16 16:51:09,602 DEV  : loss 4.45620584 - f-score 0.6747 - acc 0.5091\n",
      "2019-04-16 16:51:18,332 TEST : loss 4.34266758 - f-score 0.6981 - acc 0.5362\n",
      "Epoch    65: reducing learning rate of group 0 to 1.5625e-03.\n",
      "2019-04-16 16:51:18,335 Epoch 65: reducing weight decay factor of group 0 to 1.5625e-03.\n",
      "2019-04-16 16:51:18,335 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:51:18,718 epoch 67 - iter 0/121 - loss 5.96158409\n",
      "2019-04-16 16:51:23,850 epoch 67 - iter 12/121 - loss 6.01888979\n",
      "2019-04-16 16:51:29,256 epoch 67 - iter 24/121 - loss 5.63993544\n",
      "2019-04-16 16:51:36,935 epoch 67 - iter 36/121 - loss 5.60274216\n",
      "2019-04-16 16:51:42,670 epoch 67 - iter 48/121 - loss 5.64730575\n",
      "2019-04-16 16:51:47,191 epoch 67 - iter 60/121 - loss 5.57067916\n",
      "2019-04-16 16:51:58,383 epoch 67 - iter 72/121 - loss 5.60836938\n",
      "2019-04-16 16:52:04,993 epoch 67 - iter 84/121 - loss 5.57135624\n",
      "2019-04-16 16:52:10,117 epoch 67 - iter 96/121 - loss 5.59186413\n",
      "2019-04-16 16:52:16,720 epoch 67 - iter 108/121 - loss 5.53627727\n",
      "2019-04-16 16:52:27,466 epoch 67 - iter 120/121 - loss 5.60851862\n",
      "2019-04-16 16:52:27,485 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:52:27,486 EPOCH 67 done: loss 5.6085 - lr 0.0016 - bad epochs 0\n",
      "2019-04-16 16:52:35,589 DEV  : loss 4.05949354 - f-score 0.6954 - acc 0.5331\n",
      "2019-04-16 16:52:44,165 TEST : loss 4.01321411 - f-score 0.7049 - acc 0.5443\n",
      "2019-04-16 16:52:53,325 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:52:53,734 epoch 68 - iter 0/121 - loss 7.16528273\n",
      "2019-04-16 16:52:59,183 epoch 68 - iter 12/121 - loss 5.47993700\n",
      "2019-04-16 16:53:04,889 epoch 68 - iter 24/121 - loss 5.39523852\n",
      "2019-04-16 16:53:09,096 epoch 68 - iter 36/121 - loss 5.19592205\n",
      "2019-04-16 16:53:20,021 epoch 68 - iter 48/121 - loss 5.48658004\n",
      "2019-04-16 16:53:26,834 epoch 68 - iter 60/121 - loss 5.50552394\n",
      "2019-04-16 16:53:32,509 epoch 68 - iter 72/121 - loss 5.46524798\n",
      "2019-04-16 16:53:45,079 epoch 68 - iter 84/121 - loss 5.56984213\n",
      "2019-04-16 16:53:51,594 epoch 68 - iter 96/121 - loss 5.57877520\n",
      "2019-04-16 16:53:56,021 epoch 68 - iter 108/121 - loss 5.55344692\n",
      "2019-04-16 16:54:02,637 epoch 68 - iter 120/121 - loss 5.48583394\n",
      "2019-04-16 16:54:02,656 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:54:02,657 EPOCH 68 done: loss 5.4858 - lr 0.0016 - bad epochs 0\n",
      "2019-04-16 16:54:11,308 DEV  : loss 4.26775217 - f-score 0.6850 - acc 0.5209\n",
      "2019-04-16 16:54:19,368 TEST : loss 4.05772591 - f-score 0.7139 - acc 0.5550\n",
      "2019-04-16 16:54:30,046 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:54:30,464 epoch 69 - iter 0/121 - loss 6.39868259\n",
      "2019-04-16 16:54:35,709 epoch 69 - iter 12/121 - loss 5.52772170\n",
      "2019-04-16 16:54:41,244 epoch 69 - iter 24/121 - loss 5.33485436\n",
      "2019-04-16 16:54:48,105 epoch 69 - iter 36/121 - loss 5.39719181\n",
      "2019-04-16 16:54:52,701 epoch 69 - iter 48/121 - loss 5.32342086\n",
      "2019-04-16 16:55:04,154 epoch 69 - iter 60/121 - loss 5.44797995\n",
      "2019-04-16 16:55:10,803 epoch 69 - iter 72/121 - loss 5.52735811\n",
      "2019-04-16 16:55:16,002 epoch 69 - iter 84/121 - loss 5.50723344\n",
      "2019-04-16 16:55:27,353 epoch 69 - iter 96/121 - loss 5.56305061\n",
      "2019-04-16 16:55:33,892 epoch 69 - iter 108/121 - loss 5.53243395\n",
      "2019-04-16 16:55:39,144 epoch 69 - iter 120/121 - loss 5.48355995\n",
      "2019-04-16 16:55:39,164 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:55:39,165 EPOCH 69 done: loss 5.4836 - lr 0.0016 - bad epochs 0\n",
      "2019-04-16 16:55:47,902 DEV  : loss 3.98601198 - f-score 0.6904 - acc 0.5272\n",
      "2019-04-16 16:55:55,592 TEST : loss 3.93380070 - f-score 0.7102 - acc 0.5506\n",
      "2019-04-16 16:56:06,698 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:56:07,095 epoch 70 - iter 0/121 - loss 4.34583235\n",
      "2019-04-16 16:56:12,842 epoch 70 - iter 12/121 - loss 5.52381292\n",
      "2019-04-16 16:56:16,447 epoch 70 - iter 24/121 - loss 5.37203565\n",
      "2019-04-16 16:56:23,672 epoch 70 - iter 36/121 - loss 5.52122801\n",
      "2019-04-16 16:56:30,855 epoch 70 - iter 48/121 - loss 5.56604158\n",
      "2019-04-16 16:56:36,937 epoch 70 - iter 60/121 - loss 5.46041928\n",
      "2019-04-16 16:56:42,734 epoch 70 - iter 72/121 - loss 5.45797371\n",
      "2019-04-16 16:56:47,546 epoch 70 - iter 84/121 - loss 5.42842944\n",
      "2019-04-16 16:56:58,902 epoch 70 - iter 96/121 - loss 5.42770762\n",
      "2019-04-16 16:57:07,629 epoch 70 - iter 108/121 - loss 5.54941813\n",
      "2019-04-16 16:57:13,287 epoch 70 - iter 120/121 - loss 5.52372655\n",
      "2019-04-16 16:57:13,306 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:57:13,307 EPOCH 70 done: loss 5.5237 - lr 0.0016 - bad epochs 0\n",
      "2019-04-16 16:57:22,287 DEV  : loss 4.37809896 - f-score 0.6732 - acc 0.5074\n",
      "2019-04-16 16:57:30,010 TEST : loss 4.13308001 - f-score 0.7059 - acc 0.5455\n",
      "2019-04-16 16:57:30,014 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:57:30,556 epoch 71 - iter 0/121 - loss 4.26743984\n",
      "2019-04-16 16:57:42,353 epoch 71 - iter 12/121 - loss 5.77425738\n",
      "2019-04-16 16:57:47,674 epoch 71 - iter 24/121 - loss 5.25398492\n",
      "2019-04-16 16:57:53,567 epoch 71 - iter 36/121 - loss 5.24584606\n",
      "2019-04-16 16:57:59,805 epoch 71 - iter 48/121 - loss 5.28848471\n",
      "2019-04-16 16:58:05,318 epoch 71 - iter 60/121 - loss 5.32816903\n",
      "2019-04-16 16:58:10,381 epoch 71 - iter 72/121 - loss 5.28312248\n",
      "2019-04-16 16:58:15,664 epoch 71 - iter 84/121 - loss 5.28281488\n",
      "2019-04-16 16:58:21,919 epoch 71 - iter 96/121 - loss 5.33220927\n",
      "2019-04-16 16:58:28,318 epoch 71 - iter 108/121 - loss 5.41491531\n",
      "2019-04-16 16:58:39,096 epoch 71 - iter 120/121 - loss 5.49449863\n",
      "2019-04-16 16:58:39,116 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:58:39,116 EPOCH 71 done: loss 5.4945 - lr 0.0016 - bad epochs 1\n",
      "2019-04-16 16:58:48,136 DEV  : loss 4.26678324 - f-score 0.6766 - acc 0.5112\n",
      "2019-04-16 16:58:55,801 TEST : loss 4.04507494 - f-score 0.7023 - acc 0.5412\n",
      "2019-04-16 16:58:55,804 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 16:58:56,338 epoch 72 - iter 0/121 - loss 6.21787500\n",
      "2019-04-16 16:59:03,048 epoch 72 - iter 12/121 - loss 5.54815505\n",
      "2019-04-16 16:59:08,819 epoch 72 - iter 24/121 - loss 5.47086480\n",
      "2019-04-16 16:59:14,110 epoch 72 - iter 36/121 - loss 5.34730956\n",
      "2019-04-16 16:59:26,849 epoch 72 - iter 48/121 - loss 5.69592292\n",
      "2019-04-16 16:59:32,785 epoch 72 - iter 60/121 - loss 5.63007179\n",
      "2019-04-16 16:59:36,630 epoch 72 - iter 72/121 - loss 5.53633681\n",
      "2019-04-16 16:59:42,138 epoch 72 - iter 84/121 - loss 5.51008745\n",
      "2019-04-16 16:59:54,578 epoch 72 - iter 96/121 - loss 5.57487737\n",
      "2019-04-16 16:59:59,800 epoch 72 - iter 108/121 - loss 5.54396232\n",
      "2019-04-16 17:00:05,719 epoch 72 - iter 120/121 - loss 5.49805320\n",
      "2019-04-16 17:00:05,739 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:00:05,740 EPOCH 72 done: loss 5.4981 - lr 0.0016 - bad epochs 2\n",
      "2019-04-16 17:00:14,694 DEV  : loss 4.35447836 - f-score 0.6810 - acc 0.5164\n",
      "2019-04-16 17:00:22,588 TEST : loss 4.11179972 - f-score 0.7098 - acc 0.5501\n",
      "2019-04-16 17:00:22,591 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:00:24,593 epoch 73 - iter 0/121 - loss 9.61699677\n",
      "2019-04-16 17:00:32,050 epoch 73 - iter 12/121 - loss 5.82543901\n",
      "2019-04-16 17:00:36,436 epoch 73 - iter 24/121 - loss 5.57366967\n",
      "2019-04-16 17:00:45,132 epoch 73 - iter 36/121 - loss 5.90980275\n",
      "2019-04-16 17:00:53,131 epoch 73 - iter 48/121 - loss 5.91321987\n",
      "2019-04-16 17:00:57,287 epoch 73 - iter 60/121 - loss 5.71103758\n",
      "2019-04-16 17:01:02,381 epoch 73 - iter 72/121 - loss 5.62521498\n",
      "2019-04-16 17:01:08,747 epoch 73 - iter 84/121 - loss 5.59638768\n",
      "2019-04-16 17:01:14,799 epoch 73 - iter 96/121 - loss 5.51165490\n",
      "2019-04-16 17:01:19,135 epoch 73 - iter 108/121 - loss 5.48725985\n",
      "2019-04-16 17:01:22,943 epoch 73 - iter 120/121 - loss 5.44993425\n",
      "2019-04-16 17:01:22,962 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:01:22,963 EPOCH 73 done: loss 5.4499 - lr 0.0016 - bad epochs 3\n",
      "2019-04-16 17:01:30,052 DEV  : loss 4.32296419 - f-score 0.6678 - acc 0.5013\n",
      "2019-04-16 17:01:38,754 TEST : loss 3.96940780 - f-score 0.7141 - acc 0.5554\n",
      "2019-04-16 17:01:48,017 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:01:48,236 epoch 74 - iter 0/121 - loss 4.85780478\n",
      "2019-04-16 17:01:51,852 epoch 74 - iter 12/121 - loss 4.64797710\n",
      "2019-04-16 17:01:57,063 epoch 74 - iter 24/121 - loss 4.86950275\n",
      "2019-04-16 17:02:08,582 epoch 74 - iter 36/121 - loss 5.07817384\n",
      "2019-04-16 17:02:14,237 epoch 74 - iter 48/121 - loss 5.16068508\n",
      "2019-04-16 17:02:19,637 epoch 74 - iter 60/121 - loss 5.37160344\n",
      "2019-04-16 17:02:26,566 epoch 74 - iter 72/121 - loss 5.42362667\n",
      "2019-04-16 17:02:31,991 epoch 74 - iter 84/121 - loss 5.37879983\n",
      "2019-04-16 17:02:38,111 epoch 74 - iter 96/121 - loss 5.42167202\n",
      "2019-04-16 17:02:46,391 epoch 74 - iter 108/121 - loss 5.43004388\n",
      "2019-04-16 17:02:51,228 epoch 74 - iter 120/121 - loss 5.42600953\n",
      "2019-04-16 17:02:51,249 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:02:51,250 EPOCH 74 done: loss 5.4260 - lr 0.0016 - bad epochs 0\n",
      "2019-04-16 17:02:57,191 DEV  : loss 4.39156342 - f-score 0.6724 - acc 0.5064\n",
      "2019-04-16 17:03:05,459 TEST : loss 4.12219095 - f-score 0.7105 - acc 0.5511\n",
      "2019-04-16 17:03:14,874 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:03:15,567 epoch 75 - iter 0/121 - loss 5.26282167\n",
      "2019-04-16 17:03:24,041 epoch 75 - iter 12/121 - loss 5.64448428\n",
      "2019-04-16 17:03:29,655 epoch 75 - iter 24/121 - loss 5.31081746\n",
      "2019-04-16 17:03:36,058 epoch 75 - iter 36/121 - loss 5.27284041\n",
      "2019-04-16 17:03:42,088 epoch 75 - iter 48/121 - loss 5.22557388\n",
      "2019-04-16 17:03:48,597 epoch 75 - iter 60/121 - loss 5.27286595\n",
      "2019-04-16 17:03:58,709 epoch 75 - iter 72/121 - loss 5.49042204\n",
      "2019-04-16 17:04:10,420 epoch 75 - iter 84/121 - loss 5.59600634\n",
      "2019-04-16 17:04:17,179 epoch 75 - iter 96/121 - loss 5.59577251\n",
      "2019-04-16 17:04:21,891 epoch 75 - iter 108/121 - loss 5.52640447\n",
      "2019-04-16 17:04:26,040 epoch 75 - iter 120/121 - loss 5.46890173\n",
      "2019-04-16 17:04:26,061 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:04:26,062 EPOCH 75 done: loss 5.4689 - lr 0.0016 - bad epochs 0\n",
      "2019-04-16 17:04:35,013 DEV  : loss 4.34818983 - f-score 0.6710 - acc 0.5049\n",
      "2019-04-16 17:04:42,318 TEST : loss 4.04299784 - f-score 0.7113 - acc 0.5519\n",
      "2019-04-16 17:04:42,323 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:04:42,594 epoch 76 - iter 0/121 - loss 4.96422958\n",
      "2019-04-16 17:04:46,025 epoch 76 - iter 12/121 - loss 5.04315525\n",
      "2019-04-16 17:04:55,615 epoch 76 - iter 24/121 - loss 5.70636868\n",
      "2019-04-16 17:05:01,307 epoch 76 - iter 36/121 - loss 5.52176617\n",
      "2019-04-16 17:05:06,433 epoch 76 - iter 48/121 - loss 5.36720220\n",
      "2019-04-16 17:05:12,011 epoch 76 - iter 60/121 - loss 5.38008890\n",
      "2019-04-16 17:05:15,649 epoch 76 - iter 72/121 - loss 5.34549792\n",
      "2019-04-16 17:05:20,428 epoch 76 - iter 84/121 - loss 5.41189334\n",
      "2019-04-16 17:05:25,692 epoch 76 - iter 96/121 - loss 5.42691192\n",
      "2019-04-16 17:05:36,908 epoch 76 - iter 108/121 - loss 5.50264367\n",
      "2019-04-16 17:05:43,273 epoch 76 - iter 120/121 - loss 5.53277288\n",
      "2019-04-16 17:05:43,294 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:05:43,295 EPOCH 76 done: loss 5.5328 - lr 0.0016 - bad epochs 1\n",
      "2019-04-16 17:05:51,615 DEV  : loss 4.10420990 - f-score 0.6858 - acc 0.5218\n",
      "2019-04-16 17:06:00,172 TEST : loss 3.98779917 - f-score 0.7132 - acc 0.5542\n",
      "2019-04-16 17:06:00,175 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:06:00,530 epoch 77 - iter 0/121 - loss 5.29885960\n",
      "2019-04-16 17:06:07,194 epoch 77 - iter 12/121 - loss 5.62840704\n",
      "2019-04-16 17:06:11,082 epoch 77 - iter 24/121 - loss 5.44878877\n",
      "2019-04-16 17:06:21,182 epoch 77 - iter 36/121 - loss 5.59400098\n",
      "2019-04-16 17:06:27,007 epoch 77 - iter 48/121 - loss 5.40387398\n",
      "2019-04-16 17:06:32,589 epoch 77 - iter 60/121 - loss 5.40396203\n",
      "2019-04-16 17:06:36,833 epoch 77 - iter 72/121 - loss 5.39047130\n",
      "2019-04-16 17:06:43,629 epoch 77 - iter 84/121 - loss 5.39516524\n",
      "2019-04-16 17:06:49,882 epoch 77 - iter 96/121 - loss 5.32364857\n",
      "2019-04-16 17:06:55,787 epoch 77 - iter 108/121 - loss 5.42188565\n",
      "2019-04-16 17:07:05,882 epoch 77 - iter 120/121 - loss 5.48195782\n",
      "2019-04-16 17:07:05,902 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:07:05,903 EPOCH 77 done: loss 5.4820 - lr 0.0016 - bad epochs 2\n",
      "2019-04-16 17:07:14,459 DEV  : loss 4.12863827 - f-score 0.6802 - acc 0.5153\n",
      "2019-04-16 17:07:22,522 TEST : loss 4.03954887 - f-score 0.7175 - acc 0.5594\n",
      "2019-04-16 17:07:22,524 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:07:22,959 epoch 78 - iter 0/121 - loss 4.29871798\n",
      "2019-04-16 17:07:30,868 epoch 78 - iter 12/121 - loss 5.41424362\n",
      "2019-04-16 17:07:36,302 epoch 78 - iter 24/121 - loss 5.30712000\n",
      "2019-04-16 17:07:41,102 epoch 78 - iter 36/121 - loss 5.19704518\n",
      "2019-04-16 17:07:47,309 epoch 78 - iter 48/121 - loss 5.13320751\n",
      "2019-04-16 17:07:59,197 epoch 78 - iter 60/121 - loss 5.52905052\n",
      "2019-04-16 17:08:03,471 epoch 78 - iter 72/121 - loss 5.42940285\n",
      "2019-04-16 17:08:11,221 epoch 78 - iter 84/121 - loss 5.50597832\n",
      "2019-04-16 17:08:17,890 epoch 78 - iter 96/121 - loss 5.37959833\n",
      "2019-04-16 17:08:23,682 epoch 78 - iter 108/121 - loss 5.36284902\n",
      "2019-04-16 17:08:28,741 epoch 78 - iter 120/121 - loss 5.40899958\n",
      "2019-04-16 17:08:28,762 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:08:28,763 EPOCH 78 done: loss 5.4090 - lr 0.0016 - bad epochs 3\n",
      "2019-04-16 17:08:35,155 DEV  : loss 4.06955624 - f-score 0.6842 - acc 0.5200\n",
      "2019-04-16 17:08:41,365 TEST : loss 3.80056572 - f-score 0.7223 - acc 0.5653\n",
      "2019-04-16 17:08:50,895 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:08:51,345 epoch 79 - iter 0/121 - loss 4.32446957\n",
      "2019-04-16 17:08:56,938 epoch 79 - iter 12/121 - loss 5.16404829\n",
      "2019-04-16 17:09:02,029 epoch 79 - iter 24/121 - loss 5.09845489\n",
      "2019-04-16 17:09:07,391 epoch 79 - iter 36/121 - loss 4.90174244\n",
      "2019-04-16 17:09:13,293 epoch 79 - iter 48/121 - loss 5.06369489\n",
      "2019-04-16 17:09:23,133 epoch 79 - iter 60/121 - loss 5.25964887\n",
      "2019-04-16 17:09:30,998 epoch 79 - iter 72/121 - loss 5.36693383\n",
      "2019-04-16 17:09:41,424 epoch 79 - iter 84/121 - loss 5.55663995\n",
      "2019-04-16 17:09:46,471 epoch 79 - iter 96/121 - loss 5.45351100\n",
      "2019-04-16 17:09:54,477 epoch 79 - iter 108/121 - loss 5.49116092\n",
      "2019-04-16 17:09:59,939 epoch 79 - iter 120/121 - loss 5.48084080\n",
      "2019-04-16 17:09:59,966 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:09:59,967 EPOCH 79 done: loss 5.4808 - lr 0.0016 - bad epochs 0\n",
      "2019-04-16 17:10:08,602 DEV  : loss 4.35113478 - f-score 0.6753 - acc 0.5098\n",
      "2019-04-16 17:10:17,494 TEST : loss 3.98355055 - f-score 0.7115 - acc 0.5522\n",
      "2019-04-16 17:10:17,498 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:10:17,834 epoch 80 - iter 0/121 - loss 3.64979672\n",
      "2019-04-16 17:10:22,771 epoch 80 - iter 12/121 - loss 4.98522692\n",
      "2019-04-16 17:10:26,420 epoch 80 - iter 24/121 - loss 5.44021658\n",
      "2019-04-16 17:10:33,880 epoch 80 - iter 36/121 - loss 5.61257768\n",
      "2019-04-16 17:10:41,843 epoch 80 - iter 48/121 - loss 5.72712287\n",
      "2019-04-16 17:10:53,260 epoch 80 - iter 60/121 - loss 5.75879535\n",
      "2019-04-16 17:10:58,022 epoch 80 - iter 72/121 - loss 5.61638740\n",
      "2019-04-16 17:11:02,904 epoch 80 - iter 84/121 - loss 5.61712639\n",
      "2019-04-16 17:11:07,789 epoch 80 - iter 96/121 - loss 5.61571091\n",
      "2019-04-16 17:11:11,974 epoch 80 - iter 108/121 - loss 5.50861408\n",
      "2019-04-16 17:11:17,762 epoch 80 - iter 120/121 - loss 5.48571070\n",
      "2019-04-16 17:11:17,783 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:11:17,784 EPOCH 80 done: loss 5.4857 - lr 0.0016 - bad epochs 1\n",
      "2019-04-16 17:11:26,149 DEV  : loss 4.13130713 - f-score 0.6875 - acc 0.5238\n",
      "2019-04-16 17:11:32,583 TEST : loss 3.92153287 - f-score 0.7158 - acc 0.5574\n",
      "2019-04-16 17:11:32,585 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:11:32,980 epoch 81 - iter 0/121 - loss 3.20974493\n",
      "2019-04-16 17:11:37,819 epoch 81 - iter 12/121 - loss 5.72481456\n",
      "2019-04-16 17:11:43,795 epoch 81 - iter 24/121 - loss 5.49862576\n",
      "2019-04-16 17:11:49,482 epoch 81 - iter 36/121 - loss 5.36339809\n",
      "2019-04-16 17:11:58,603 epoch 81 - iter 48/121 - loss 5.48653218\n",
      "2019-04-16 17:12:05,273 epoch 81 - iter 60/121 - loss 5.42199123\n",
      "2019-04-16 17:12:10,778 epoch 81 - iter 72/121 - loss 5.32212716\n",
      "2019-04-16 17:12:17,921 epoch 81 - iter 84/121 - loss 5.48519415\n",
      "2019-04-16 17:12:26,096 epoch 81 - iter 96/121 - loss 5.50868949\n",
      "2019-04-16 17:12:31,804 epoch 81 - iter 108/121 - loss 5.52993597\n",
      "2019-04-16 17:12:37,881 epoch 81 - iter 120/121 - loss 5.45962143\n",
      "2019-04-16 17:12:37,901 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:12:37,901 EPOCH 81 done: loss 5.4596 - lr 0.0016 - bad epochs 2\n",
      "2019-04-16 17:12:46,601 DEV  : loss 4.28939819 - f-score 0.6868 - acc 0.5230\n",
      "2019-04-16 17:12:53,241 TEST : loss 4.02621937 - f-score 0.7022 - acc 0.5411\n",
      "2019-04-16 17:12:53,245 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:12:53,528 epoch 82 - iter 0/121 - loss 5.72042131\n",
      "2019-04-16 17:12:57,172 epoch 82 - iter 12/121 - loss 5.10461708\n",
      "2019-04-16 17:13:01,873 epoch 82 - iter 24/121 - loss 5.08047577\n",
      "2019-04-16 17:13:13,097 epoch 82 - iter 36/121 - loss 5.28778828\n",
      "2019-04-16 17:13:18,477 epoch 82 - iter 48/121 - loss 5.36681295\n",
      "2019-04-16 17:13:22,914 epoch 82 - iter 60/121 - loss 5.48314299\n",
      "2019-04-16 17:13:29,841 epoch 82 - iter 72/121 - loss 5.58902012\n",
      "2019-04-16 17:13:37,574 epoch 82 - iter 84/121 - loss 5.58063978\n",
      "2019-04-16 17:13:47,077 epoch 82 - iter 96/121 - loss 5.59588544\n",
      "2019-04-16 17:13:51,516 epoch 82 - iter 108/121 - loss 5.55713558\n",
      "2019-04-16 17:13:55,484 epoch 82 - iter 120/121 - loss 5.50733583\n",
      "2019-04-16 17:13:57,534 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:13:57,535 EPOCH 82 done: loss 5.5073 - lr 0.0016 - bad epochs 3\n",
      "2019-04-16 17:14:06,812 DEV  : loss 3.96402359 - f-score 0.6919 - acc 0.5290\n",
      "2019-04-16 17:14:14,554 TEST : loss 3.81055117 - f-score 0.7200 - acc 0.5625\n",
      "Epoch    81: reducing learning rate of group 0 to 7.8125e-04.\n",
      "2019-04-16 17:14:14,557 Epoch 81: reducing weight decay factor of group 0 to 7.8125e-04.\n",
      "2019-04-16 17:14:14,558 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:14:14,969 epoch 83 - iter 0/121 - loss 4.70483398\n",
      "2019-04-16 17:14:22,360 epoch 83 - iter 12/121 - loss 5.21590464\n",
      "2019-04-16 17:14:34,739 epoch 83 - iter 24/121 - loss 5.51616597\n",
      "2019-04-16 17:14:39,862 epoch 83 - iter 36/121 - loss 5.53075097\n",
      "2019-04-16 17:14:45,642 epoch 83 - iter 48/121 - loss 5.39076959\n",
      "2019-04-16 17:14:52,601 epoch 83 - iter 60/121 - loss 5.34562147\n",
      "2019-04-16 17:14:57,492 epoch 83 - iter 72/121 - loss 5.25765474\n",
      "2019-04-16 17:15:05,331 epoch 83 - iter 84/121 - loss 5.35796061\n",
      "2019-04-16 17:15:09,058 epoch 83 - iter 96/121 - loss 5.30039553\n",
      "2019-04-16 17:15:16,349 epoch 83 - iter 108/121 - loss 5.28179428\n",
      "2019-04-16 17:15:21,774 epoch 83 - iter 120/121 - loss 5.18831921\n",
      "2019-04-16 17:15:21,796 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:15:21,797 EPOCH 83 done: loss 5.1883 - lr 0.0008 - bad epochs 0\n",
      "2019-04-16 17:15:29,684 DEV  : loss 3.86989522 - f-score 0.6983 - acc 0.5365\n",
      "2019-04-16 17:15:38,634 TEST : loss 3.65522289 - f-score 0.7312 - acc 0.5763\n",
      "2019-04-16 17:15:47,863 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:15:48,361 epoch 84 - iter 0/121 - loss 3.81253576\n",
      "2019-04-16 17:15:53,462 epoch 84 - iter 12/121 - loss 4.93907373\n",
      "2019-04-16 17:15:58,968 epoch 84 - iter 24/121 - loss 4.60811294\n",
      "2019-04-16 17:16:09,229 epoch 84 - iter 36/121 - loss 4.89079319\n",
      "2019-04-16 17:16:16,662 epoch 84 - iter 48/121 - loss 5.07663632\n",
      "2019-04-16 17:16:23,038 epoch 84 - iter 60/121 - loss 5.17037800\n",
      "2019-04-16 17:16:29,957 epoch 84 - iter 72/121 - loss 5.11049392\n",
      "2019-04-16 17:16:37,503 epoch 84 - iter 84/121 - loss 5.20969593\n",
      "2019-04-16 17:16:41,460 epoch 84 - iter 96/121 - loss 5.07591319\n",
      "2019-04-16 17:16:46,765 epoch 84 - iter 108/121 - loss 4.99252534\n",
      "2019-04-16 17:16:53,320 epoch 84 - iter 120/121 - loss 5.03100413\n",
      "2019-04-16 17:16:53,342 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:16:53,343 EPOCH 84 done: loss 5.0310 - lr 0.0008 - bad epochs 0\n",
      "2019-04-16 17:17:01,352 DEV  : loss 4.06296730 - f-score 0.6884 - acc 0.5248\n",
      "2019-04-16 17:17:10,277 TEST : loss 3.73410821 - f-score 0.7144 - acc 0.5557\n",
      "2019-04-16 17:17:19,858 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:17:20,431 epoch 85 - iter 0/121 - loss 5.49593878\n",
      "2019-04-16 17:17:26,736 epoch 85 - iter 12/121 - loss 4.98587867\n",
      "2019-04-16 17:17:32,842 epoch 85 - iter 24/121 - loss 5.00703794\n",
      "2019-04-16 17:17:37,943 epoch 85 - iter 36/121 - loss 4.94584397\n",
      "2019-04-16 17:17:49,510 epoch 85 - iter 48/121 - loss 5.28655832\n",
      "2019-04-16 17:17:56,410 epoch 85 - iter 60/121 - loss 5.19117078\n",
      "2019-04-16 17:18:01,000 epoch 85 - iter 72/121 - loss 5.14161559\n",
      "2019-04-16 17:18:09,584 epoch 85 - iter 84/121 - loss 5.25637605\n",
      "2019-04-16 17:18:15,075 epoch 85 - iter 96/121 - loss 5.22094786\n",
      "2019-04-16 17:18:22,577 epoch 85 - iter 108/121 - loss 5.19757976\n",
      "2019-04-16 17:18:28,683 epoch 85 - iter 120/121 - loss 5.12797526\n",
      "2019-04-16 17:18:28,704 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:18:28,705 EPOCH 85 done: loss 5.1280 - lr 0.0008 - bad epochs 0\n",
      "2019-04-16 17:18:36,192 DEV  : loss 4.08701277 - f-score 0.6803 - acc 0.5155\n",
      "2019-04-16 17:18:42,153 TEST : loss 3.75527167 - f-score 0.7218 - acc 0.5647\n",
      "2019-04-16 17:18:42,155 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:18:42,617 epoch 86 - iter 0/121 - loss 4.39185810\n",
      "2019-04-16 17:18:54,284 epoch 86 - iter 12/121 - loss 5.56039095\n",
      "2019-04-16 17:19:01,488 epoch 86 - iter 24/121 - loss 5.26171139\n",
      "2019-04-16 17:19:06,051 epoch 86 - iter 36/121 - loss 5.16906500\n",
      "2019-04-16 17:19:10,753 epoch 86 - iter 48/121 - loss 5.23855395\n",
      "2019-04-16 17:19:19,018 epoch 86 - iter 60/121 - loss 5.16283844\n",
      "2019-04-16 17:19:29,100 epoch 86 - iter 72/121 - loss 5.17047260\n",
      "2019-04-16 17:19:34,200 epoch 86 - iter 84/121 - loss 5.14881485\n",
      "2019-04-16 17:19:39,421 epoch 86 - iter 96/121 - loss 5.10160354\n",
      "2019-04-16 17:19:44,860 epoch 86 - iter 108/121 - loss 5.09692932\n",
      "2019-04-16 17:19:50,084 epoch 86 - iter 120/121 - loss 5.06178999\n",
      "2019-04-16 17:19:50,104 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:19:50,105 EPOCH 86 done: loss 5.0618 - lr 0.0008 - bad epochs 1\n",
      "2019-04-16 17:19:58,091 DEV  : loss 4.06984425 - f-score 0.6873 - acc 0.5236\n",
      "2019-04-16 17:20:06,535 TEST : loss 3.82806587 - f-score 0.7216 - acc 0.5644\n",
      "2019-04-16 17:20:06,538 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:20:07,016 epoch 87 - iter 0/121 - loss 5.45657778\n",
      "2019-04-16 17:20:11,671 epoch 87 - iter 12/121 - loss 5.40862329\n",
      "2019-04-16 17:20:22,395 epoch 87 - iter 24/121 - loss 5.42285611\n",
      "2019-04-16 17:20:28,397 epoch 87 - iter 36/121 - loss 5.18296276\n",
      "2019-04-16 17:20:33,110 epoch 87 - iter 48/121 - loss 5.06274392\n",
      "2019-04-16 17:20:36,731 epoch 87 - iter 60/121 - loss 4.99092163\n",
      "2019-04-16 17:20:44,324 epoch 87 - iter 72/121 - loss 5.10559377\n",
      "2019-04-16 17:20:51,447 epoch 87 - iter 84/121 - loss 5.08960308\n",
      "2019-04-16 17:20:58,133 epoch 87 - iter 96/121 - loss 5.14670515\n",
      "2019-04-16 17:21:03,409 epoch 87 - iter 108/121 - loss 5.15296068\n",
      "2019-04-16 17:21:08,865 epoch 87 - iter 120/121 - loss 5.13374035\n",
      "2019-04-16 17:21:08,885 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:21:08,886 EPOCH 87 done: loss 5.1337 - lr 0.0008 - bad epochs 2\n",
      "2019-04-16 17:21:14,751 DEV  : loss 3.94665074 - f-score 0.6951 - acc 0.5327\n",
      "2019-04-16 17:21:22,279 TEST : loss 3.70064688 - f-score 0.7236 - acc 0.5669\n",
      "2019-04-16 17:21:22,281 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:21:23,069 epoch 88 - iter 0/121 - loss 4.86792994\n",
      "2019-04-16 17:21:34,910 epoch 88 - iter 12/121 - loss 5.50120799\n",
      "2019-04-16 17:21:39,057 epoch 88 - iter 24/121 - loss 5.37347434\n",
      "2019-04-16 17:21:48,293 epoch 88 - iter 36/121 - loss 5.59264298\n",
      "2019-04-16 17:21:54,175 epoch 88 - iter 48/121 - loss 5.56512126\n",
      "2019-04-16 17:21:59,472 epoch 88 - iter 60/121 - loss 5.37585461\n",
      "2019-04-16 17:22:06,531 epoch 88 - iter 72/121 - loss 5.31981796\n",
      "2019-04-16 17:22:10,535 epoch 88 - iter 84/121 - loss 5.15476705\n",
      "2019-04-16 17:22:15,733 epoch 88 - iter 96/121 - loss 5.15338340\n",
      "2019-04-16 17:22:21,908 epoch 88 - iter 108/121 - loss 5.12315242\n",
      "2019-04-16 17:22:28,252 epoch 88 - iter 120/121 - loss 5.07914175\n",
      "2019-04-16 17:22:28,274 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:22:28,275 EPOCH 88 done: loss 5.0791 - lr 0.0008 - bad epochs 3\n",
      "2019-04-16 17:22:36,344 DEV  : loss 4.20860910 - f-score 0.6881 - acc 0.5245\n",
      "2019-04-16 17:22:45,050 TEST : loss 3.85905385 - f-score 0.7200 - acc 0.5624\n",
      "Epoch    87: reducing learning rate of group 0 to 3.9063e-04.\n",
      "2019-04-16 17:22:45,054 Epoch 87: reducing weight decay factor of group 0 to 3.9063e-04.\n",
      "2019-04-16 17:22:45,054 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:22:45,438 epoch 89 - iter 0/121 - loss 5.37920809\n",
      "2019-04-16 17:22:50,911 epoch 89 - iter 12/121 - loss 5.02360645\n",
      "2019-04-16 17:23:03,795 epoch 89 - iter 24/121 - loss 5.19477208\n",
      "2019-04-16 17:23:08,886 epoch 89 - iter 36/121 - loss 5.15498799\n",
      "2019-04-16 17:23:20,793 epoch 89 - iter 48/121 - loss 5.22669222\n",
      "2019-04-16 17:23:26,415 epoch 89 - iter 60/121 - loss 5.19192564\n",
      "2019-04-16 17:23:31,114 epoch 89 - iter 72/121 - loss 5.16199795\n",
      "2019-04-16 17:23:35,752 epoch 89 - iter 84/121 - loss 5.12290501\n",
      "2019-04-16 17:23:39,968 epoch 89 - iter 96/121 - loss 5.07432381\n",
      "2019-04-16 17:23:44,727 epoch 89 - iter 108/121 - loss 5.05922435\n",
      "2019-04-16 17:23:51,660 epoch 89 - iter 120/121 - loss 5.01480642\n",
      "2019-04-16 17:23:51,681 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:23:51,682 EPOCH 89 done: loss 5.0148 - lr 0.0004 - bad epochs 0\n",
      "2019-04-16 17:24:00,395 DEV  : loss 3.85397983 - f-score 0.6984 - acc 0.5366\n",
      "2019-04-16 17:24:07,068 TEST : loss 3.66386461 - f-score 0.7245 - acc 0.5681\n",
      "2019-04-16 17:24:19,594 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:24:20,024 epoch 90 - iter 0/121 - loss 3.47474194\n",
      "2019-04-16 17:24:28,039 epoch 90 - iter 12/121 - loss 4.75469963\n",
      "2019-04-16 17:24:33,860 epoch 90 - iter 24/121 - loss 5.05355397\n",
      "2019-04-16 17:24:39,943 epoch 90 - iter 36/121 - loss 5.03044822\n",
      "2019-04-16 17:24:47,458 epoch 90 - iter 48/121 - loss 4.97283785\n",
      "2019-04-16 17:24:53,600 epoch 90 - iter 60/121 - loss 5.00667422\n",
      "2019-04-16 17:24:57,430 epoch 90 - iter 72/121 - loss 4.87482427\n",
      "2019-04-16 17:25:05,419 epoch 90 - iter 84/121 - loss 4.96405459\n",
      "2019-04-16 17:25:10,490 epoch 90 - iter 96/121 - loss 4.92230451\n",
      "2019-04-16 17:25:17,548 epoch 90 - iter 108/121 - loss 4.90896357\n",
      "2019-04-16 17:25:28,696 epoch 90 - iter 120/121 - loss 4.99117934\n",
      "2019-04-16 17:25:28,717 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:25:28,718 EPOCH 90 done: loss 4.9912 - lr 0.0004 - bad epochs 0\n",
      "2019-04-16 17:25:36,804 DEV  : loss 3.76944137 - f-score 0.7055 - acc 0.5450\n",
      "2019-04-16 17:25:45,394 TEST : loss 3.62638497 - f-score 0.7283 - acc 0.5727\n",
      "2019-04-16 17:25:54,719 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:25:55,067 epoch 91 - iter 0/121 - loss 6.99386454\n",
      "2019-04-16 17:25:59,029 epoch 91 - iter 12/121 - loss 5.05567281\n",
      "2019-04-16 17:26:10,299 epoch 91 - iter 24/121 - loss 5.33894277\n",
      "2019-04-16 17:26:16,186 epoch 91 - iter 36/121 - loss 5.15269244\n",
      "2019-04-16 17:26:22,342 epoch 91 - iter 48/121 - loss 5.16183674\n",
      "2019-04-16 17:26:28,132 epoch 91 - iter 60/121 - loss 5.09271901\n",
      "2019-04-16 17:26:35,107 epoch 91 - iter 72/121 - loss 4.98887405\n",
      "2019-04-16 17:26:40,378 epoch 91 - iter 84/121 - loss 4.98006818\n",
      "2019-04-16 17:26:44,546 epoch 91 - iter 96/121 - loss 4.90308095\n",
      "2019-04-16 17:26:50,063 epoch 91 - iter 108/121 - loss 4.86353040\n",
      "2019-04-16 17:26:58,015 epoch 91 - iter 120/121 - loss 4.86532982\n",
      "2019-04-16 17:26:58,035 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:26:58,035 EPOCH 91 done: loss 4.8653 - lr 0.0004 - bad epochs 0\n",
      "2019-04-16 17:27:06,271 DEV  : loss 3.88239026 - f-score 0.7035 - acc 0.5427\n",
      "2019-04-16 17:27:14,265 TEST : loss 3.65928435 - f-score 0.7326 - acc 0.5781\n",
      "2019-04-16 17:27:26,518 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:27:28,018 epoch 92 - iter 0/121 - loss 7.02374125\n",
      "2019-04-16 17:27:39,312 epoch 92 - iter 12/121 - loss 5.93529015\n",
      "2019-04-16 17:27:47,214 epoch 92 - iter 24/121 - loss 5.46750860\n",
      "2019-04-16 17:27:51,604 epoch 92 - iter 36/121 - loss 5.18708096\n",
      "2019-04-16 17:28:02,692 epoch 92 - iter 48/121 - loss 5.29540174\n",
      "2019-04-16 17:28:09,480 epoch 92 - iter 60/121 - loss 5.14555508\n",
      "2019-04-16 17:28:14,010 epoch 92 - iter 72/121 - loss 5.11444782\n",
      "2019-04-16 17:28:18,853 epoch 92 - iter 84/121 - loss 5.02427266\n",
      "2019-04-16 17:28:24,445 epoch 92 - iter 96/121 - loss 4.97777390\n",
      "2019-04-16 17:28:30,544 epoch 92 - iter 108/121 - loss 4.98804079\n",
      "2019-04-16 17:28:35,625 epoch 92 - iter 120/121 - loss 4.96219562\n",
      "2019-04-16 17:28:35,646 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:28:35,647 EPOCH 92 done: loss 4.9622 - lr 0.0004 - bad epochs 0\n",
      "2019-04-16 17:28:41,875 DEV  : loss 3.91833425 - f-score 0.6974 - acc 0.5354\n",
      "2019-04-16 17:28:48,035 TEST : loss 3.65819144 - f-score 0.7353 - acc 0.5815\n",
      "2019-04-16 17:28:48,038 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:28:48,534 epoch 93 - iter 0/121 - loss 6.22687340\n",
      "2019-04-16 17:28:54,175 epoch 93 - iter 12/121 - loss 4.53948725\n",
      "2019-04-16 17:29:09,866 epoch 93 - iter 24/121 - loss 5.30841030\n",
      "2019-04-16 17:29:13,620 epoch 93 - iter 36/121 - loss 5.13335642\n",
      "2019-04-16 17:29:16,884 epoch 93 - iter 48/121 - loss 4.99241724\n",
      "2019-04-16 17:29:22,321 epoch 93 - iter 60/121 - loss 4.95989366\n",
      "2019-04-16 17:29:29,989 epoch 93 - iter 72/121 - loss 5.00486103\n",
      "2019-04-16 17:29:36,520 epoch 93 - iter 84/121 - loss 4.94502443\n",
      "2019-04-16 17:29:42,245 epoch 93 - iter 96/121 - loss 4.95337012\n",
      "2019-04-16 17:29:47,326 epoch 93 - iter 108/121 - loss 4.95051606\n",
      "2019-04-16 17:29:52,566 epoch 93 - iter 120/121 - loss 4.90877540\n",
      "2019-04-16 17:29:52,585 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:29:52,585 EPOCH 93 done: loss 4.9088 - lr 0.0004 - bad epochs 1\n",
      "2019-04-16 17:30:01,188 DEV  : loss 3.91593766 - f-score 0.6998 - acc 0.5382\n",
      "2019-04-16 17:30:09,862 TEST : loss 3.64365721 - f-score 0.7355 - acc 0.5816\n",
      "2019-04-16 17:30:09,866 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:30:10,580 epoch 94 - iter 0/121 - loss 4.43908501\n",
      "2019-04-16 17:30:17,235 epoch 94 - iter 12/121 - loss 4.58518379\n",
      "2019-04-16 17:30:21,161 epoch 94 - iter 24/121 - loss 4.64836136\n",
      "2019-04-16 17:30:28,502 epoch 94 - iter 36/121 - loss 4.70308256\n",
      "2019-04-16 17:30:39,836 epoch 94 - iter 48/121 - loss 4.75706295\n",
      "2019-04-16 17:30:49,081 epoch 94 - iter 60/121 - loss 4.87851893\n",
      "2019-04-16 17:30:54,678 epoch 94 - iter 72/121 - loss 4.84755034\n",
      "2019-04-16 17:31:01,072 epoch 94 - iter 84/121 - loss 4.87590735\n",
      "2019-04-16 17:31:07,201 epoch 94 - iter 96/121 - loss 4.87943525\n",
      "2019-04-16 17:31:12,474 epoch 94 - iter 108/121 - loss 4.87006910\n",
      "2019-04-16 17:31:19,319 epoch 94 - iter 120/121 - loss 4.91362889\n",
      "2019-04-16 17:31:19,339 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:31:19,340 EPOCH 94 done: loss 4.9136 - lr 0.0004 - bad epochs 2\n",
      "2019-04-16 17:31:27,470 DEV  : loss 3.83999085 - f-score 0.7089 - acc 0.5491\n",
      "2019-04-16 17:31:33,898 TEST : loss 3.58083391 - f-score 0.7304 - acc 0.5753\n",
      "2019-04-16 17:31:33,900 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:31:34,247 epoch 95 - iter 0/121 - loss 3.07075167\n",
      "2019-04-16 17:31:38,442 epoch 95 - iter 12/121 - loss 4.41867357\n",
      "2019-04-16 17:31:45,743 epoch 95 - iter 24/121 - loss 4.38919559\n",
      "2019-04-16 17:31:51,770 epoch 95 - iter 36/121 - loss 4.63541667\n",
      "2019-04-16 17:31:56,959 epoch 95 - iter 48/121 - loss 4.59901998\n",
      "2019-04-16 17:32:02,896 epoch 95 - iter 60/121 - loss 4.63899291\n",
      "2019-04-16 17:32:10,934 epoch 95 - iter 72/121 - loss 4.77364015\n",
      "2019-04-16 17:32:17,952 epoch 95 - iter 84/121 - loss 4.77839168\n",
      "2019-04-16 17:32:23,502 epoch 95 - iter 96/121 - loss 4.77390282\n",
      "2019-04-16 17:32:29,667 epoch 95 - iter 108/121 - loss 4.78817758\n",
      "2019-04-16 17:32:40,213 epoch 95 - iter 120/121 - loss 4.84560515\n",
      "2019-04-16 17:32:40,235 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:32:40,235 EPOCH 95 done: loss 4.8456 - lr 0.0004 - bad epochs 3\n",
      "2019-04-16 17:32:48,797 DEV  : loss 4.03909159 - f-score 0.6958 - acc 0.5335\n",
      "2019-04-16 17:32:55,387 TEST : loss 3.70979261 - f-score 0.7323 - acc 0.5776\n",
      "2019-04-16 17:33:09,424 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:33:09,996 epoch 96 - iter 0/121 - loss 3.93392467\n",
      "2019-04-16 17:33:21,438 epoch 96 - iter 12/121 - loss 5.47922861\n",
      "2019-04-16 17:33:27,071 epoch 96 - iter 24/121 - loss 5.01276212\n",
      "2019-04-16 17:33:33,290 epoch 96 - iter 36/121 - loss 4.84095742\n",
      "2019-04-16 17:33:38,563 epoch 96 - iter 48/121 - loss 4.85361729\n",
      "2019-04-16 17:33:45,533 epoch 96 - iter 60/121 - loss 4.93137860\n",
      "2019-04-16 17:33:50,883 epoch 96 - iter 72/121 - loss 4.98657162\n",
      "2019-04-16 17:33:59,317 epoch 96 - iter 84/121 - loss 5.03955199\n",
      "2019-04-16 17:34:04,839 epoch 96 - iter 96/121 - loss 5.02988547\n",
      "2019-04-16 17:34:10,636 epoch 96 - iter 108/121 - loss 4.99521366\n",
      "2019-04-16 17:34:15,764 epoch 96 - iter 120/121 - loss 4.95566258\n",
      "2019-04-16 17:34:15,786 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:34:15,787 EPOCH 96 done: loss 4.9557 - lr 0.0004 - bad epochs 0\n",
      "2019-04-16 17:34:23,807 DEV  : loss 3.96847463 - f-score 0.6951 - acc 0.5327\n",
      "2019-04-16 17:34:32,574 TEST : loss 3.66900587 - f-score 0.7361 - acc 0.5824\n",
      "2019-04-16 17:34:32,576 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:34:32,957 epoch 97 - iter 0/121 - loss 4.45147419\n",
      "2019-04-16 17:34:39,496 epoch 97 - iter 12/121 - loss 5.05703913\n",
      "2019-04-16 17:34:45,219 epoch 97 - iter 24/121 - loss 4.78743564\n",
      "2019-04-16 17:34:51,426 epoch 97 - iter 36/121 - loss 4.76987364\n",
      "2019-04-16 17:34:56,193 epoch 97 - iter 48/121 - loss 4.74714174\n",
      "2019-04-16 17:35:07,496 epoch 97 - iter 60/121 - loss 4.90258004\n",
      "2019-04-16 17:35:13,284 epoch 97 - iter 72/121 - loss 4.83381840\n",
      "2019-04-16 17:35:23,251 epoch 97 - iter 84/121 - loss 4.95136729\n",
      "2019-04-16 17:35:28,995 epoch 97 - iter 96/121 - loss 4.99747834\n",
      "2019-04-16 17:35:33,021 epoch 97 - iter 108/121 - loss 4.94757844\n",
      "2019-04-16 17:35:40,435 epoch 97 - iter 120/121 - loss 4.95903500\n",
      "2019-04-16 17:35:40,455 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:35:40,456 EPOCH 97 done: loss 4.9590 - lr 0.0004 - bad epochs 1\n",
      "2019-04-16 17:35:49,227 DEV  : loss 3.89369297 - f-score 0.6988 - acc 0.5371\n",
      "2019-04-16 17:35:57,226 TEST : loss 3.59379792 - f-score 0.7301 - acc 0.5750\n",
      "2019-04-16 17:35:57,229 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:35:57,628 epoch 98 - iter 0/121 - loss 4.05224752\n",
      "2019-04-16 17:36:04,714 epoch 98 - iter 12/121 - loss 5.01198195\n",
      "2019-04-16 17:36:11,316 epoch 98 - iter 24/121 - loss 4.72551442\n",
      "2019-04-16 17:36:16,058 epoch 98 - iter 36/121 - loss 4.63486095\n",
      "2019-04-16 17:36:27,647 epoch 98 - iter 48/121 - loss 5.02174483\n",
      "2019-04-16 17:36:32,132 epoch 98 - iter 60/121 - loss 4.86876896\n",
      "2019-04-16 17:36:42,733 epoch 98 - iter 72/121 - loss 5.00597019\n",
      "2019-04-16 17:36:48,459 epoch 98 - iter 84/121 - loss 4.88050239\n",
      "2019-04-16 17:36:53,968 epoch 98 - iter 96/121 - loss 4.85231403\n",
      "2019-04-16 17:36:59,570 epoch 98 - iter 108/121 - loss 4.81645663\n",
      "2019-04-16 17:37:05,359 epoch 98 - iter 120/121 - loss 4.79964538\n",
      "2019-04-16 17:37:05,381 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:37:05,382 EPOCH 98 done: loss 4.7996 - lr 0.0004 - bad epochs 2\n",
      "2019-04-16 17:37:13,932 DEV  : loss 4.00433254 - f-score 0.6942 - acc 0.5316\n",
      "2019-04-16 17:37:21,992 TEST : loss 3.72434616 - f-score 0.7261 - acc 0.5699\n",
      "2019-04-16 17:37:31,414 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:37:31,802 epoch 99 - iter 0/121 - loss 3.51741743\n",
      "2019-04-16 17:37:37,146 epoch 99 - iter 12/121 - loss 4.91164794\n",
      "2019-04-16 17:37:44,484 epoch 99 - iter 24/121 - loss 5.03412567\n",
      "2019-04-16 17:37:55,129 epoch 99 - iter 36/121 - loss 5.30876128\n",
      "2019-04-16 17:38:01,284 epoch 99 - iter 48/121 - loss 5.21226506\n",
      "2019-04-16 17:38:06,920 epoch 99 - iter 60/121 - loss 5.00038809\n",
      "2019-04-16 17:38:12,775 epoch 99 - iter 72/121 - loss 4.98889593\n",
      "2019-04-16 17:38:18,284 epoch 99 - iter 84/121 - loss 4.93626644\n",
      "2019-04-16 17:38:23,671 epoch 99 - iter 96/121 - loss 4.84695336\n",
      "2019-04-16 17:38:34,406 epoch 99 - iter 108/121 - loss 4.89711375\n",
      "2019-04-16 17:38:40,573 epoch 99 - iter 120/121 - loss 4.92508850\n",
      "2019-04-16 17:38:40,594 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:38:40,596 EPOCH 99 done: loss 4.9251 - lr 0.0004 - bad epochs 0\n",
      "2019-04-16 17:38:46,899 DEV  : loss 3.96581531 - f-score 0.6945 - acc 0.5320\n",
      "2019-04-16 17:38:53,605 TEST : loss 3.67421818 - f-score 0.7276 - acc 0.5719\n",
      "2019-04-16 17:38:53,608 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:38:54,096 epoch 100 - iter 0/121 - loss 5.17864704\n",
      "2019-04-16 17:38:59,813 epoch 100 - iter 12/121 - loss 4.22613199\n",
      "2019-04-16 17:39:11,304 epoch 100 - iter 24/121 - loss 4.93935256\n",
      "2019-04-16 17:39:19,138 epoch 100 - iter 36/121 - loss 5.26293715\n",
      "2019-04-16 17:39:27,138 epoch 100 - iter 48/121 - loss 5.09325447\n",
      "2019-04-16 17:39:38,132 epoch 100 - iter 60/121 - loss 5.17156557\n",
      "2019-04-16 17:39:43,217 epoch 100 - iter 72/121 - loss 5.01875138\n",
      "2019-04-16 17:39:49,186 epoch 100 - iter 84/121 - loss 4.99859838\n",
      "2019-04-16 17:39:54,964 epoch 100 - iter 96/121 - loss 4.90967884\n",
      "2019-04-16 17:39:59,654 epoch 100 - iter 108/121 - loss 4.90207631\n",
      "2019-04-16 17:40:02,968 epoch 100 - iter 120/121 - loss 4.84155595\n",
      "2019-04-16 17:40:02,987 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:40:02,988 EPOCH 100 done: loss 4.8416 - lr 0.0004 - bad epochs 1\n",
      "2019-04-16 17:40:08,813 DEV  : loss 4.07162619 - f-score 0.6916 - acc 0.5286\n",
      "2019-04-16 17:40:16,311 TEST : loss 3.72344947 - f-score 0.7210 - acc 0.5637\n",
      "2019-04-16 17:40:16,315 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:40:16,822 epoch 101 - iter 0/121 - loss 5.11136246\n",
      "2019-04-16 17:40:23,014 epoch 101 - iter 12/121 - loss 4.67613534\n",
      "2019-04-16 17:40:27,720 epoch 101 - iter 24/121 - loss 4.67039898\n",
      "2019-04-16 17:40:32,304 epoch 101 - iter 36/121 - loss 4.73121846\n",
      "2019-04-16 17:40:43,270 epoch 101 - iter 48/121 - loss 4.76023356\n",
      "2019-04-16 17:40:48,702 epoch 101 - iter 60/121 - loss 4.59453279\n",
      "2019-04-16 17:40:53,641 epoch 101 - iter 72/121 - loss 4.59695422\n",
      "2019-04-16 17:40:59,856 epoch 101 - iter 84/121 - loss 4.71773187\n",
      "2019-04-16 17:41:05,715 epoch 101 - iter 96/121 - loss 4.70209918\n",
      "2019-04-16 17:41:13,405 epoch 101 - iter 108/121 - loss 4.77380534\n",
      "2019-04-16 17:41:24,822 epoch 101 - iter 120/121 - loss 4.91883180\n",
      "2019-04-16 17:41:24,842 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:41:24,843 EPOCH 101 done: loss 4.9188 - lr 0.0004 - bad epochs 2\n",
      "2019-04-16 17:41:33,588 DEV  : loss 3.67680693 - f-score 0.7055 - acc 0.5450\n",
      "2019-04-16 17:41:41,690 TEST : loss 3.51534367 - f-score 0.7357 - acc 0.5819\n",
      "2019-04-16 17:41:41,693 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:41:42,269 epoch 102 - iter 0/121 - loss 5.04849768\n",
      "2019-04-16 17:41:47,826 epoch 102 - iter 12/121 - loss 4.41687552\n",
      "2019-04-16 17:41:54,273 epoch 102 - iter 24/121 - loss 4.63932253\n",
      "2019-04-16 17:42:02,040 epoch 102 - iter 36/121 - loss 4.86629963\n",
      "2019-04-16 17:42:07,746 epoch 102 - iter 48/121 - loss 4.66951445\n",
      "2019-04-16 17:42:18,047 epoch 102 - iter 60/121 - loss 4.86032836\n",
      "2019-04-16 17:42:22,780 epoch 102 - iter 72/121 - loss 4.82051648\n",
      "2019-04-16 17:42:34,538 epoch 102 - iter 84/121 - loss 4.95622433\n",
      "2019-04-16 17:42:39,288 epoch 102 - iter 96/121 - loss 4.93293540\n",
      "2019-04-16 17:42:44,252 epoch 102 - iter 108/121 - loss 4.92785528\n",
      "2019-04-16 17:42:49,911 epoch 102 - iter 120/121 - loss 4.84770851\n",
      "2019-04-16 17:42:49,932 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:42:49,933 EPOCH 102 done: loss 4.8477 - lr 0.0004 - bad epochs 3\n",
      "2019-04-16 17:42:58,552 DEV  : loss 3.99839902 - f-score 0.7015 - acc 0.5403\n",
      "2019-04-16 17:43:06,668 TEST : loss 3.69143033 - f-score 0.7329 - acc 0.5784\n",
      "Epoch   101: reducing learning rate of group 0 to 1.9531e-04.\n",
      "2019-04-16 17:43:06,670 Epoch 101: reducing weight decay factor of group 0 to 1.9531e-04.\n",
      "2019-04-16 17:43:06,671 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:43:07,122 epoch 103 - iter 0/121 - loss 4.77931833\n",
      "2019-04-16 17:43:13,522 epoch 103 - iter 12/121 - loss 4.75032845\n",
      "2019-04-16 17:43:18,106 epoch 103 - iter 24/121 - loss 4.66708783\n",
      "2019-04-16 17:43:23,324 epoch 103 - iter 36/121 - loss 4.75965877\n",
      "2019-04-16 17:43:34,235 epoch 103 - iter 48/121 - loss 4.88911076\n",
      "2019-04-16 17:43:41,012 epoch 103 - iter 60/121 - loss 4.80775830\n",
      "2019-04-16 17:43:46,089 epoch 103 - iter 72/121 - loss 4.76354994\n",
      "2019-04-16 17:43:51,649 epoch 103 - iter 84/121 - loss 4.70314455\n",
      "2019-04-16 17:44:03,272 epoch 103 - iter 96/121 - loss 4.82158753\n",
      "2019-04-16 17:44:10,295 epoch 103 - iter 108/121 - loss 4.79930118\n",
      "2019-04-16 17:44:15,855 epoch 103 - iter 120/121 - loss 4.77692452\n",
      "2019-04-16 17:44:15,877 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:44:15,878 EPOCH 103 done: loss 4.7769 - lr 0.0002 - bad epochs 0\n",
      "2019-04-16 17:44:24,308 DEV  : loss 3.97983885 - f-score 0.6962 - acc 0.5339\n",
      "2019-04-16 17:44:32,724 TEST : loss 3.63019991 - f-score 0.7323 - acc 0.5777\n",
      "2019-04-16 17:44:43,122 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:44:49,549 epoch 104 - iter 0/121 - loss 11.21929741\n",
      "2019-04-16 17:45:01,412 epoch 104 - iter 12/121 - loss 6.02882112\n",
      "2019-04-16 17:45:05,360 epoch 104 - iter 24/121 - loss 5.38534612\n",
      "2019-04-16 17:45:10,768 epoch 104 - iter 36/121 - loss 4.89291281\n",
      "2019-04-16 17:45:18,583 epoch 104 - iter 48/121 - loss 4.86142024\n",
      "2019-04-16 17:45:23,917 epoch 104 - iter 60/121 - loss 4.79499662\n",
      "2019-04-16 17:45:29,011 epoch 104 - iter 72/121 - loss 4.89077712\n",
      "2019-04-16 17:45:35,757 epoch 104 - iter 84/121 - loss 4.87398046\n",
      "2019-04-16 17:45:41,930 epoch 104 - iter 96/121 - loss 4.82163448\n",
      "2019-04-16 17:45:48,071 epoch 104 - iter 108/121 - loss 4.82741571\n",
      "2019-04-16 17:45:52,073 epoch 104 - iter 120/121 - loss 4.83613579\n",
      "2019-04-16 17:45:52,092 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:45:52,093 EPOCH 104 done: loss 4.8361 - lr 0.0002 - bad epochs 0\n",
      "2019-04-16 17:46:00,999 DEV  : loss 3.71505094 - f-score 0.7064 - acc 0.5461\n",
      "2019-04-16 17:46:08,756 TEST : loss 3.52162242 - f-score 0.7344 - acc 0.5803\n",
      "2019-04-16 17:46:08,758 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:46:09,170 epoch 105 - iter 0/121 - loss 5.52467346\n",
      "2019-04-16 17:46:14,528 epoch 105 - iter 12/121 - loss 4.30084799\n",
      "2019-04-16 17:46:19,892 epoch 105 - iter 24/121 - loss 4.21203377\n",
      "2019-04-16 17:46:25,365 epoch 105 - iter 36/121 - loss 4.19015169\n",
      "2019-04-16 17:46:31,602 epoch 105 - iter 48/121 - loss 4.37898687\n",
      "2019-04-16 17:46:37,082 epoch 105 - iter 60/121 - loss 4.36786838\n",
      "2019-04-16 17:46:46,611 epoch 105 - iter 72/121 - loss 4.50014608\n",
      "2019-04-16 17:46:51,301 epoch 105 - iter 84/121 - loss 4.52712594\n",
      "2019-04-16 17:47:02,820 epoch 105 - iter 96/121 - loss 4.70904951\n",
      "2019-04-16 17:47:13,648 epoch 105 - iter 108/121 - loss 4.80111544\n",
      "2019-04-16 17:47:17,889 epoch 105 - iter 120/121 - loss 4.80267266\n",
      "2019-04-16 17:47:17,910 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:47:17,911 EPOCH 105 done: loss 4.8027 - lr 0.0002 - bad epochs 1\n",
      "2019-04-16 17:47:26,879 DEV  : loss 3.82157946 - f-score 0.6948 - acc 0.5323\n",
      "2019-04-16 17:47:34,548 TEST : loss 3.54681826 - f-score 0.7272 - acc 0.5713\n",
      "2019-04-16 17:47:34,551 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:47:34,955 epoch 106 - iter 0/121 - loss 4.83428431\n",
      "2019-04-16 17:47:41,590 epoch 106 - iter 12/121 - loss 4.88334683\n",
      "2019-04-16 17:47:47,800 epoch 106 - iter 24/121 - loss 4.64531376\n",
      "2019-04-16 17:47:52,877 epoch 106 - iter 36/121 - loss 4.75564397\n",
      "2019-04-16 17:48:03,692 epoch 106 - iter 48/121 - loss 4.84157185\n",
      "2019-04-16 17:48:11,185 epoch 106 - iter 60/121 - loss 4.78032065\n",
      "2019-04-16 17:48:20,977 epoch 106 - iter 72/121 - loss 4.90529295\n",
      "2019-04-16 17:48:26,641 epoch 106 - iter 84/121 - loss 4.90945732\n",
      "2019-04-16 17:48:33,171 epoch 106 - iter 96/121 - loss 4.92386651\n",
      "2019-04-16 17:48:39,028 epoch 106 - iter 108/121 - loss 4.84984738\n",
      "2019-04-16 17:48:43,403 epoch 106 - iter 120/121 - loss 4.86589205\n",
      "2019-04-16 17:48:43,424 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:48:43,425 EPOCH 106 done: loss 4.8659 - lr 0.0002 - bad epochs 2\n",
      "2019-04-16 17:48:52,526 DEV  : loss 3.92548203 - f-score 0.6956 - acc 0.5332\n",
      "2019-04-16 17:49:00,123 TEST : loss 3.60590625 - f-score 0.7331 - acc 0.5786\n",
      "2019-04-16 17:49:00,125 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:49:00,694 epoch 107 - iter 0/121 - loss 6.75908756\n",
      "2019-04-16 17:49:13,553 epoch 107 - iter 12/121 - loss 5.54324860\n",
      "2019-04-16 17:49:17,761 epoch 107 - iter 24/121 - loss 4.90552209\n",
      "2019-04-16 17:49:22,183 epoch 107 - iter 36/121 - loss 4.81129335\n",
      "2019-04-16 17:49:26,077 epoch 107 - iter 48/121 - loss 4.76510242\n",
      "2019-04-16 17:49:33,106 epoch 107 - iter 60/121 - loss 4.90497485\n",
      "2019-04-16 17:49:46,112 epoch 107 - iter 72/121 - loss 5.11020278\n",
      "2019-04-16 17:49:51,248 epoch 107 - iter 84/121 - loss 5.00574690\n",
      "2019-04-16 17:49:55,882 epoch 107 - iter 96/121 - loss 4.96547314\n",
      "2019-04-16 17:50:01,341 epoch 107 - iter 108/121 - loss 4.87463200\n",
      "2019-04-16 17:50:06,877 epoch 107 - iter 120/121 - loss 4.83944900\n",
      "2019-04-16 17:50:06,904 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:50:06,905 EPOCH 107 done: loss 4.8394 - lr 0.0002 - bad epochs 3\n",
      "2019-04-16 17:50:15,424 DEV  : loss 3.98100543 - f-score 0.6888 - acc 0.5253\n",
      "2019-04-16 17:50:24,219 TEST : loss 3.63939953 - f-score 0.7380 - acc 0.5848\n",
      "Epoch   106: reducing learning rate of group 0 to 9.7656e-05.\n",
      "2019-04-16 17:50:24,223 Epoch 106: reducing weight decay factor of group 0 to 9.7656e-05.\n",
      "2019-04-16 17:50:24,224 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:50:24,735 epoch 108 - iter 0/121 - loss 4.15105057\n",
      "2019-04-16 17:50:29,794 epoch 108 - iter 12/121 - loss 4.05620014\n",
      "2019-04-16 17:50:35,456 epoch 108 - iter 24/121 - loss 4.37031351\n",
      "2019-04-16 17:50:41,249 epoch 108 - iter 36/121 - loss 4.38390908\n",
      "2019-04-16 17:50:47,392 epoch 108 - iter 48/121 - loss 4.48279319\n",
      "2019-04-16 17:50:53,178 epoch 108 - iter 60/121 - loss 4.45981242\n",
      "2019-04-16 17:51:04,696 epoch 108 - iter 72/121 - loss 4.64539321\n",
      "2019-04-16 17:51:14,890 epoch 108 - iter 84/121 - loss 4.72662657\n",
      "2019-04-16 17:51:19,755 epoch 108 - iter 96/121 - loss 4.68238543\n",
      "2019-04-16 17:51:25,733 epoch 108 - iter 108/121 - loss 4.73072951\n",
      "2019-04-16 17:51:33,161 epoch 108 - iter 120/121 - loss 4.82300059\n",
      "2019-04-16 17:51:33,182 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:51:33,183 EPOCH 108 done: loss 4.8230 - lr 0.0001 - bad epochs 0\n",
      "2019-04-16 17:51:40,622 DEV  : loss 3.91055608 - f-score 0.6979 - acc 0.5360\n",
      "2019-04-16 17:51:46,691 TEST : loss 3.58283567 - f-score 0.7362 - acc 0.5826\n",
      "2019-04-16 17:51:46,694 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:51:46,989 epoch 109 - iter 0/121 - loss 3.45180154\n",
      "2019-04-16 17:51:52,051 epoch 109 - iter 12/121 - loss 4.67394946\n",
      "2019-04-16 17:52:05,746 epoch 109 - iter 24/121 - loss 5.04254457\n",
      "2019-04-16 17:52:11,501 epoch 109 - iter 36/121 - loss 4.92598235\n",
      "2019-04-16 17:52:17,628 epoch 109 - iter 48/121 - loss 4.93629675\n",
      "2019-04-16 17:52:23,859 epoch 109 - iter 60/121 - loss 4.84761672\n",
      "2019-04-16 17:52:28,882 epoch 109 - iter 72/121 - loss 4.79067702\n",
      "2019-04-16 17:52:33,259 epoch 109 - iter 84/121 - loss 4.73466873\n",
      "2019-04-16 17:52:45,440 epoch 109 - iter 96/121 - loss 4.78967194\n",
      "2019-04-16 17:52:51,456 epoch 109 - iter 108/121 - loss 4.76787354\n",
      "2019-04-16 17:52:56,171 epoch 109 - iter 120/121 - loss 4.74148782\n",
      "2019-04-16 17:52:56,192 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:52:56,193 EPOCH 109 done: loss 4.7415 - lr 0.0001 - bad epochs 1\n",
      "2019-04-16 17:53:04,258 DEV  : loss 3.83999968 - f-score 0.6960 - acc 0.5337\n",
      "2019-04-16 17:53:12,597 TEST : loss 3.56278348 - f-score 0.7348 - acc 0.5808\n",
      "2019-04-16 17:53:21,950 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:53:22,238 epoch 110 - iter 0/121 - loss 3.87360144\n",
      "2019-04-16 17:53:26,301 epoch 110 - iter 12/121 - loss 4.32937552\n",
      "2019-04-16 17:53:42,112 epoch 110 - iter 24/121 - loss 5.24713683\n",
      "2019-04-16 17:53:47,654 epoch 110 - iter 36/121 - loss 4.97585524\n",
      "2019-04-16 17:53:51,811 epoch 110 - iter 48/121 - loss 4.82400900\n",
      "2019-04-16 17:53:56,350 epoch 110 - iter 60/121 - loss 4.81197841\n",
      "2019-04-16 17:54:01,741 epoch 110 - iter 72/121 - loss 4.79667220\n",
      "2019-04-16 17:54:07,369 epoch 110 - iter 84/121 - loss 4.75493004\n",
      "2019-04-16 17:54:14,890 epoch 110 - iter 96/121 - loss 4.77195481\n",
      "2019-04-16 17:54:20,909 epoch 110 - iter 108/121 - loss 4.78194577\n",
      "2019-04-16 17:54:24,717 epoch 110 - iter 120/121 - loss 4.73306408\n",
      "2019-04-16 17:54:24,739 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:54:24,740 EPOCH 110 done: loss 4.7331 - lr 0.0001 - bad epochs 0\n",
      "2019-04-16 17:54:30,728 DEV  : loss 3.92686248 - f-score 0.6938 - acc 0.5312\n",
      "2019-04-16 17:54:39,473 TEST : loss 3.59214973 - f-score 0.7393 - acc 0.5863\n",
      "2019-04-16 17:54:48,843 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:54:49,240 epoch 111 - iter 0/121 - loss 5.24686432\n",
      "2019-04-16 17:54:52,389 epoch 111 - iter 12/121 - loss 4.59004008\n",
      "2019-04-16 17:55:03,277 epoch 111 - iter 24/121 - loss 5.10877859\n",
      "2019-04-16 17:55:14,064 epoch 111 - iter 36/121 - loss 5.06851426\n",
      "2019-04-16 17:55:18,622 epoch 111 - iter 48/121 - loss 4.92805365\n",
      "2019-04-16 17:55:22,362 epoch 111 - iter 60/121 - loss 4.77274972\n",
      "2019-04-16 17:55:27,547 epoch 111 - iter 72/121 - loss 4.87860652\n",
      "2019-04-16 17:55:31,287 epoch 111 - iter 84/121 - loss 4.77318128\n",
      "2019-04-16 17:55:38,492 epoch 111 - iter 96/121 - loss 4.76565968\n",
      "2019-04-16 17:55:46,490 epoch 111 - iter 108/121 - loss 4.83480633\n",
      "2019-04-16 17:55:50,352 epoch 111 - iter 120/121 - loss 4.77499800\n",
      "2019-04-16 17:55:50,372 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:55:50,372 EPOCH 111 done: loss 4.7750 - lr 0.0001 - bad epochs 0\n",
      "2019-04-16 17:55:58,792 DEV  : loss 3.86467361 - f-score 0.6993 - acc 0.5377\n",
      "2019-04-16 17:56:07,008 TEST : loss 3.54222798 - f-score 0.7321 - acc 0.5774\n",
      "2019-04-16 17:56:07,010 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:56:07,327 epoch 112 - iter 0/121 - loss 5.47644424\n",
      "2019-04-16 17:56:19,160 epoch 112 - iter 12/121 - loss 5.51116375\n",
      "2019-04-16 17:56:30,604 epoch 112 - iter 24/121 - loss 5.16130636\n",
      "2019-04-16 17:56:34,514 epoch 112 - iter 36/121 - loss 4.83900141\n",
      "2019-04-16 17:56:38,879 epoch 112 - iter 48/121 - loss 4.80768182\n",
      "2019-04-16 17:56:43,577 epoch 112 - iter 60/121 - loss 4.77296468\n",
      "2019-04-16 17:56:50,022 epoch 112 - iter 72/121 - loss 4.75508439\n",
      "2019-04-16 17:56:56,086 epoch 112 - iter 84/121 - loss 4.72263352\n",
      "2019-04-16 17:57:01,482 epoch 112 - iter 96/121 - loss 4.69948206\n",
      "2019-04-16 17:57:06,457 epoch 112 - iter 108/121 - loss 4.68857615\n",
      "2019-04-16 17:57:14,111 epoch 112 - iter 120/121 - loss 4.69571259\n",
      "2019-04-16 17:57:14,137 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:57:14,139 EPOCH 112 done: loss 4.6957 - lr 0.0001 - bad epochs 1\n",
      "2019-04-16 17:57:22,575 DEV  : loss 3.88215351 - f-score 0.6947 - acc 0.5322\n",
      "2019-04-16 17:57:30,964 TEST : loss 3.55625272 - f-score 0.7350 - acc 0.5810\n",
      "2019-04-16 17:57:41,476 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:57:41,985 epoch 113 - iter 0/121 - loss 5.74824047\n",
      "2019-04-16 17:57:49,430 epoch 113 - iter 12/121 - loss 4.91625646\n",
      "2019-04-16 17:57:55,551 epoch 113 - iter 24/121 - loss 4.58792691\n",
      "2019-04-16 17:58:00,288 epoch 113 - iter 36/121 - loss 4.59203471\n",
      "2019-04-16 17:58:04,451 epoch 113 - iter 48/121 - loss 4.61173404\n",
      "2019-04-16 17:58:08,081 epoch 113 - iter 60/121 - loss 4.59712539\n",
      "2019-04-16 17:58:12,813 epoch 113 - iter 72/121 - loss 4.58117836\n",
      "2019-04-16 17:58:19,107 epoch 113 - iter 84/121 - loss 4.51360657\n",
      "2019-04-16 17:58:32,291 epoch 113 - iter 96/121 - loss 4.67822557\n",
      "2019-04-16 17:58:37,122 epoch 113 - iter 108/121 - loss 4.70682587\n",
      "2019-04-16 17:58:44,753 epoch 113 - iter 120/121 - loss 4.80196792\n",
      "2019-04-16 17:58:44,774 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:58:44,775 EPOCH 113 done: loss 4.8020 - lr 0.0001 - bad epochs 0\n",
      "2019-04-16 17:58:52,997 DEV  : loss 3.74591446 - f-score 0.7029 - acc 0.5419\n",
      "2019-04-16 17:59:01,334 TEST : loss 3.49579620 - f-score 0.7381 - acc 0.5849\n",
      "2019-04-16 17:59:01,337 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 17:59:01,640 epoch 114 - iter 0/121 - loss 3.06251407\n",
      "2019-04-16 17:59:09,509 epoch 114 - iter 12/121 - loss 5.18971282\n",
      "2019-04-16 17:59:14,019 epoch 114 - iter 24/121 - loss 5.24315095\n",
      "2019-04-16 17:59:19,662 epoch 114 - iter 36/121 - loss 4.89943198\n",
      "2019-04-16 17:59:30,764 epoch 114 - iter 48/121 - loss 5.01794956\n",
      "2019-04-16 17:59:36,277 epoch 114 - iter 60/121 - loss 4.93316916\n",
      "2019-04-16 17:59:42,050 epoch 114 - iter 72/121 - loss 4.88160742\n",
      "2019-04-16 17:59:47,465 epoch 114 - iter 84/121 - loss 4.79040900\n",
      "2019-04-16 17:59:54,766 epoch 114 - iter 96/121 - loss 4.81646504\n",
      "2019-04-16 18:00:01,331 epoch 114 - iter 108/121 - loss 4.81406355\n",
      "2019-04-16 18:00:07,663 epoch 114 - iter 120/121 - loss 4.79272867\n",
      "2019-04-16 18:00:07,683 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:00:07,684 EPOCH 114 done: loss 4.7927 - lr 0.0001 - bad epochs 1\n",
      "2019-04-16 18:00:15,911 DEV  : loss 3.92798781 - f-score 0.6916 - acc 0.5286\n",
      "2019-04-16 18:00:24,485 TEST : loss 3.58131766 - f-score 0.7369 - acc 0.5835\n",
      "2019-04-16 18:00:24,489 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:00:25,006 epoch 115 - iter 0/121 - loss 3.47173548\n",
      "2019-04-16 18:00:32,095 epoch 115 - iter 12/121 - loss 4.43390685\n",
      "2019-04-16 18:00:36,219 epoch 115 - iter 24/121 - loss 4.35345291\n",
      "2019-04-16 18:00:42,329 epoch 115 - iter 36/121 - loss 4.45595638\n",
      "2019-04-16 18:00:49,119 epoch 115 - iter 48/121 - loss 4.48475537\n",
      "2019-04-16 18:00:54,825 epoch 115 - iter 60/121 - loss 4.50453776\n",
      "2019-04-16 18:00:59,306 epoch 115 - iter 72/121 - loss 4.52588656\n",
      "2019-04-16 18:01:06,839 epoch 115 - iter 84/121 - loss 4.66171847\n",
      "2019-04-16 18:01:16,007 epoch 115 - iter 96/121 - loss 4.70168072\n",
      "2019-04-16 18:01:21,682 epoch 115 - iter 108/121 - loss 4.66523116\n",
      "2019-04-16 18:01:29,675 epoch 115 - iter 120/121 - loss 4.72237393\n",
      "2019-04-16 18:01:29,698 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:01:29,699 EPOCH 115 done: loss 4.7224 - lr 0.0001 - bad epochs 2\n",
      "2019-04-16 18:01:36,195 DEV  : loss 3.94016910 - f-score 0.6994 - acc 0.5377\n",
      "2019-04-16 18:01:42,990 TEST : loss 3.60968447 - f-score 0.7340 - acc 0.5798\n",
      "2019-04-16 18:01:42,993 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:01:43,368 epoch 116 - iter 0/121 - loss 5.00413132\n",
      "2019-04-16 18:01:48,630 epoch 116 - iter 12/121 - loss 4.20566557\n",
      "2019-04-16 18:02:01,184 epoch 116 - iter 24/121 - loss 4.91913397\n",
      "2019-04-16 18:02:08,777 epoch 116 - iter 36/121 - loss 5.00363245\n",
      "2019-04-16 18:02:16,090 epoch 116 - iter 48/121 - loss 4.91030183\n",
      "2019-04-16 18:02:21,424 epoch 116 - iter 60/121 - loss 4.81294975\n",
      "2019-04-16 18:02:25,832 epoch 116 - iter 72/121 - loss 4.67349582\n",
      "2019-04-16 18:02:30,849 epoch 116 - iter 84/121 - loss 4.61211094\n",
      "2019-04-16 18:02:43,001 epoch 116 - iter 96/121 - loss 4.73303880\n",
      "2019-04-16 18:02:47,657 epoch 116 - iter 108/121 - loss 4.75483977\n",
      "2019-04-16 18:02:52,563 epoch 116 - iter 120/121 - loss 4.66498246\n",
      "2019-04-16 18:02:52,583 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:02:52,584 EPOCH 116 done: loss 4.6650 - lr 0.0001 - bad epochs 3\n",
      "2019-04-16 18:03:01,443 DEV  : loss 3.90161228 - f-score 0.7037 - acc 0.5429\n",
      "2019-04-16 18:03:08,539 TEST : loss 3.57657409 - f-score 0.7377 - acc 0.5844\n",
      "2019-04-16 18:03:22,884 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:03:23,372 epoch 117 - iter 0/121 - loss 5.57213736\n",
      "2019-04-16 18:03:31,915 epoch 117 - iter 12/121 - loss 4.77121047\n",
      "2019-04-16 18:03:37,348 epoch 117 - iter 24/121 - loss 4.75056555\n",
      "2019-04-16 18:03:41,564 epoch 117 - iter 36/121 - loss 4.75878987\n",
      "2019-04-16 18:03:47,581 epoch 117 - iter 48/121 - loss 4.77865678\n",
      "2019-04-16 18:04:00,057 epoch 117 - iter 60/121 - loss 4.98295887\n",
      "2019-04-16 18:04:09,840 epoch 117 - iter 72/121 - loss 4.92250811\n",
      "2019-04-16 18:04:15,921 epoch 117 - iter 84/121 - loss 4.88656074\n",
      "2019-04-16 18:04:21,216 epoch 117 - iter 96/121 - loss 4.76348257\n",
      "2019-04-16 18:04:26,148 epoch 117 - iter 108/121 - loss 4.81308252\n",
      "2019-04-16 18:04:31,934 epoch 117 - iter 120/121 - loss 4.80000396\n",
      "2019-04-16 18:04:31,956 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:04:31,957 EPOCH 117 done: loss 4.8000 - lr 0.0001 - bad epochs 0\n",
      "2019-04-16 18:04:40,937 DEV  : loss 3.86479115 - f-score 0.7015 - acc 0.5402\n",
      "2019-04-16 18:04:48,725 TEST : loss 3.56583738 - f-score 0.7353 - acc 0.5815\n",
      "2019-04-16 18:04:48,728 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:04:49,192 epoch 118 - iter 0/121 - loss 4.74198914\n",
      "2019-04-16 18:04:55,594 epoch 118 - iter 12/121 - loss 4.36269122\n",
      "2019-04-16 18:05:01,934 epoch 118 - iter 24/121 - loss 4.57336444\n",
      "2019-04-16 18:05:07,729 epoch 118 - iter 36/121 - loss 4.76125334\n",
      "2019-04-16 18:05:11,343 epoch 118 - iter 48/121 - loss 4.75350069\n",
      "2019-04-16 18:05:15,049 epoch 118 - iter 60/121 - loss 4.63504577\n",
      "2019-04-16 18:05:19,423 epoch 118 - iter 72/121 - loss 4.65232369\n",
      "2019-04-16 18:05:25,274 epoch 118 - iter 84/121 - loss 4.61136728\n",
      "2019-04-16 18:05:33,095 epoch 118 - iter 96/121 - loss 4.66778842\n",
      "2019-04-16 18:05:41,921 epoch 118 - iter 108/121 - loss 4.73429420\n",
      "2019-04-16 18:05:49,592 epoch 118 - iter 120/121 - loss 4.82069584\n",
      "2019-04-16 18:05:49,612 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:05:49,612 EPOCH 118 done: loss 4.8207 - lr 0.0001 - bad epochs 1\n",
      "2019-04-16 18:05:57,998 DEV  : loss 3.89647508 - f-score 0.6986 - acc 0.5369\n",
      "2019-04-16 18:06:06,043 TEST : loss 3.57720542 - f-score 0.7338 - acc 0.5796\n",
      "2019-04-16 18:06:06,046 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:06:06,454 epoch 119 - iter 0/121 - loss 4.26530933\n",
      "2019-04-16 18:06:10,531 epoch 119 - iter 12/121 - loss 4.02854619\n",
      "2019-04-16 18:06:18,282 epoch 119 - iter 24/121 - loss 4.86049758\n",
      "2019-04-16 18:06:24,471 epoch 119 - iter 36/121 - loss 4.74763294\n",
      "2019-04-16 18:06:30,383 epoch 119 - iter 48/121 - loss 4.70377655\n",
      "2019-04-16 18:06:34,836 epoch 119 - iter 60/121 - loss 4.68362653\n",
      "2019-04-16 18:06:39,658 epoch 119 - iter 72/121 - loss 4.65480521\n",
      "2019-04-16 18:06:46,965 epoch 119 - iter 84/121 - loss 4.71639661\n",
      "2019-04-16 18:06:54,133 epoch 119 - iter 96/121 - loss 4.68458492\n",
      "2019-04-16 18:07:01,422 epoch 119 - iter 108/121 - loss 4.73012910\n",
      "2019-04-16 18:07:07,971 epoch 119 - iter 120/121 - loss 4.74457483\n",
      "2019-04-16 18:07:07,994 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:07:07,994 EPOCH 119 done: loss 4.7446 - lr 0.0001 - bad epochs 2\n",
      "2019-04-16 18:07:16,493 DEV  : loss 3.86913991 - f-score 0.7035 - acc 0.5427\n",
      "2019-04-16 18:07:24,731 TEST : loss 3.55797458 - f-score 0.7353 - acc 0.5815\n",
      "2019-04-16 18:07:24,733 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:07:25,082 epoch 120 - iter 0/121 - loss 4.87849283\n",
      "2019-04-16 18:07:28,819 epoch 120 - iter 12/121 - loss 4.46992484\n",
      "2019-04-16 18:07:37,381 epoch 120 - iter 24/121 - loss 4.84977415\n",
      "2019-04-16 18:07:42,743 epoch 120 - iter 36/121 - loss 4.68578982\n",
      "2019-04-16 18:07:53,481 epoch 120 - iter 48/121 - loss 4.80723446\n",
      "2019-04-16 18:07:59,054 epoch 120 - iter 60/121 - loss 4.82385824\n",
      "2019-04-16 18:08:06,861 epoch 120 - iter 72/121 - loss 4.83327952\n",
      "2019-04-16 18:08:13,046 epoch 120 - iter 84/121 - loss 4.80925182\n",
      "2019-04-16 18:08:19,123 epoch 120 - iter 96/121 - loss 4.74389520\n",
      "2019-04-16 18:08:24,380 epoch 120 - iter 108/121 - loss 4.68748213\n",
      "2019-04-16 18:08:30,441 epoch 120 - iter 120/121 - loss 4.68617601\n",
      "2019-04-16 18:08:30,461 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:08:30,462 EPOCH 120 done: loss 4.6862 - lr 0.0001 - bad epochs 3\n",
      "2019-04-16 18:08:38,503 DEV  : loss 3.82694411 - f-score 0.6945 - acc 0.5320\n",
      "2019-04-16 18:08:44,611 TEST : loss 3.53120446 - f-score 0.7365 - acc 0.5830\n",
      "Epoch   119: reducing learning rate of group 0 to 4.8828e-05.\n",
      "2019-04-16 18:08:44,615 Epoch 119: reducing weight decay factor of group 0 to 4.8828e-05.\n",
      "2019-04-16 18:08:44,616 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:08:44,905 epoch 121 - iter 0/121 - loss 4.58109808\n",
      "2019-04-16 18:08:49,317 epoch 121 - iter 12/121 - loss 4.58343482\n",
      "2019-04-16 18:08:57,718 epoch 121 - iter 24/121 - loss 4.60996043\n",
      "2019-04-16 18:09:02,996 epoch 121 - iter 36/121 - loss 4.61032734\n",
      "2019-04-16 18:09:08,982 epoch 121 - iter 48/121 - loss 4.56307216\n",
      "2019-04-16 18:09:14,485 epoch 121 - iter 60/121 - loss 4.56123572\n",
      "2019-04-16 18:09:20,542 epoch 121 - iter 72/121 - loss 4.56716834\n",
      "2019-04-16 18:09:31,809 epoch 121 - iter 84/121 - loss 4.68333996\n",
      "2019-04-16 18:09:36,121 epoch 121 - iter 96/121 - loss 4.69760583\n",
      "2019-04-16 18:09:47,855 epoch 121 - iter 108/121 - loss 4.70783203\n",
      "2019-04-16 18:09:52,958 epoch 121 - iter 120/121 - loss 4.69484465\n",
      "2019-04-16 18:09:52,978 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:09:52,979 EPOCH 121 done: loss 4.6948 - lr 0.0000 - bad epochs 0\n",
      "2019-04-16 18:10:01,964 DEV  : loss 3.95234227 - f-score 0.6965 - acc 0.5343\n",
      "2019-04-16 18:10:10,277 TEST : loss 3.58091235 - f-score 0.7356 - acc 0.5818\n",
      "2019-04-16 18:10:10,281 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:10:10,862 epoch 122 - iter 0/121 - loss 4.80689049\n",
      "2019-04-16 18:10:16,093 epoch 122 - iter 12/121 - loss 4.43920257\n",
      "2019-04-16 18:10:23,272 epoch 122 - iter 24/121 - loss 4.35219627\n",
      "2019-04-16 18:10:30,350 epoch 122 - iter 36/121 - loss 4.42104043\n",
      "2019-04-16 18:10:34,755 epoch 122 - iter 48/121 - loss 4.44921198\n",
      "2019-04-16 18:10:38,722 epoch 122 - iter 60/121 - loss 4.42129479\n",
      "2019-04-16 18:10:46,127 epoch 122 - iter 72/121 - loss 4.56232019\n",
      "2019-04-16 18:10:53,779 epoch 122 - iter 84/121 - loss 4.65573802\n",
      "2019-04-16 18:11:05,016 epoch 122 - iter 96/121 - loss 4.67668266\n",
      "2019-04-16 18:11:09,711 epoch 122 - iter 108/121 - loss 4.63115644\n",
      "2019-04-16 18:11:13,659 epoch 122 - iter 120/121 - loss 4.62901599\n",
      "2019-04-16 18:11:13,682 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:11:13,683 EPOCH 122 done: loss 4.6290 - lr 0.0000 - bad epochs 1\n",
      "2019-04-16 18:11:19,935 DEV  : loss 3.80628204 - f-score 0.7017 - acc 0.5405\n",
      "2019-04-16 18:11:28,887 TEST : loss 3.50852180 - f-score 0.7375 - acc 0.5841\n",
      "2019-04-16 18:11:38,230 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:11:38,635 epoch 123 - iter 0/121 - loss 5.08415318\n",
      "2019-04-16 18:11:42,999 epoch 123 - iter 12/121 - loss 4.33487325\n",
      "2019-04-16 18:11:47,815 epoch 123 - iter 24/121 - loss 4.20978564\n",
      "2019-04-16 18:11:55,295 epoch 123 - iter 36/121 - loss 4.28666461\n",
      "2019-04-16 18:12:00,887 epoch 123 - iter 48/121 - loss 4.44537983\n",
      "2019-04-16 18:12:04,855 epoch 123 - iter 60/121 - loss 4.51285503\n",
      "2019-04-16 18:12:15,739 epoch 123 - iter 72/121 - loss 4.59541245\n",
      "2019-04-16 18:12:21,284 epoch 123 - iter 84/121 - loss 4.59828954\n",
      "2019-04-16 18:12:28,408 epoch 123 - iter 96/121 - loss 4.61579755\n",
      "2019-04-16 18:12:32,414 epoch 123 - iter 108/121 - loss 4.61050851\n",
      "2019-04-16 18:12:40,163 epoch 123 - iter 120/121 - loss 4.69245438\n",
      "2019-04-16 18:12:41,370 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:12:41,372 EPOCH 123 done: loss 4.6925 - lr 0.0000 - bad epochs 0\n",
      "2019-04-16 18:12:50,289 DEV  : loss 3.85863805 - f-score 0.6997 - acc 0.5381\n",
      "2019-04-16 18:12:57,994 TEST : loss 3.52960491 - f-score 0.7363 - acc 0.5827\n",
      "2019-04-16 18:12:57,997 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:12:58,313 epoch 124 - iter 0/121 - loss 3.22465253\n",
      "2019-04-16 18:13:09,969 epoch 124 - iter 12/121 - loss 4.84278672\n",
      "2019-04-16 18:13:15,646 epoch 124 - iter 24/121 - loss 4.51589242\n",
      "2019-04-16 18:13:21,625 epoch 124 - iter 36/121 - loss 4.64366122\n",
      "2019-04-16 18:13:28,959 epoch 124 - iter 48/121 - loss 4.80372697\n",
      "2019-04-16 18:13:36,103 epoch 124 - iter 60/121 - loss 4.67814561\n",
      "2019-04-16 18:13:41,034 epoch 124 - iter 72/121 - loss 4.62982088\n",
      "2019-04-16 18:13:45,291 epoch 124 - iter 84/121 - loss 4.65728675\n",
      "2019-04-16 18:13:50,919 epoch 124 - iter 96/121 - loss 4.59793886\n",
      "2019-04-16 18:13:56,287 epoch 124 - iter 108/121 - loss 4.56127953\n",
      "2019-04-16 18:14:06,599 epoch 124 - iter 120/121 - loss 4.69486872\n",
      "2019-04-16 18:14:06,620 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:14:06,621 EPOCH 124 done: loss 4.6949 - lr 0.0000 - bad epochs 1\n",
      "2019-04-16 18:14:12,453 DEV  : loss 3.92097569 - f-score 0.6976 - acc 0.5356\n",
      "2019-04-16 18:14:19,682 TEST : loss 3.56423283 - f-score 0.7359 - acc 0.5821\n",
      "2019-04-16 18:14:19,685 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:14:20,171 epoch 125 - iter 0/121 - loss 4.48560429\n",
      "2019-04-16 18:14:31,714 epoch 125 - iter 12/121 - loss 5.34959635\n",
      "2019-04-16 18:14:35,758 epoch 125 - iter 24/121 - loss 4.77023744\n",
      "2019-04-16 18:14:40,470 epoch 125 - iter 36/121 - loss 4.54230068\n",
      "2019-04-16 18:14:43,995 epoch 125 - iter 48/121 - loss 4.52880643\n",
      "2019-04-16 18:14:48,554 epoch 125 - iter 60/121 - loss 4.66328665\n",
      "2019-04-16 18:14:54,080 epoch 125 - iter 72/121 - loss 4.66654831\n",
      "2019-04-16 18:15:00,254 epoch 125 - iter 84/121 - loss 4.67552507\n",
      "2019-04-16 18:15:05,148 epoch 125 - iter 96/121 - loss 4.64849881\n",
      "2019-04-16 18:15:11,659 epoch 125 - iter 108/121 - loss 4.65811965\n",
      "2019-04-16 18:15:24,544 epoch 125 - iter 120/121 - loss 4.71840789\n",
      "2019-04-16 18:15:24,565 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:15:24,566 EPOCH 125 done: loss 4.7184 - lr 0.0000 - bad epochs 2\n",
      "2019-04-16 18:15:31,806 DEV  : loss 3.86803484 - f-score 0.6966 - acc 0.5344\n",
      "2019-04-16 18:15:37,880 TEST : loss 3.53950977 - f-score 0.7329 - acc 0.5785\n",
      "2019-04-16 18:15:37,882 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:15:38,177 epoch 126 - iter 0/121 - loss 4.60092449\n",
      "2019-04-16 18:15:43,174 epoch 126 - iter 12/121 - loss 4.68704464\n",
      "2019-04-16 18:15:48,891 epoch 126 - iter 24/121 - loss 4.64863858\n",
      "2019-04-16 18:15:59,910 epoch 126 - iter 36/121 - loss 4.76125346\n",
      "2019-04-16 18:16:05,896 epoch 126 - iter 48/121 - loss 4.79722850\n",
      "2019-04-16 18:16:13,634 epoch 126 - iter 60/121 - loss 4.89300261\n",
      "2019-04-16 18:16:19,152 epoch 126 - iter 72/121 - loss 4.87955881\n",
      "2019-04-16 18:16:25,805 epoch 126 - iter 84/121 - loss 4.76709847\n",
      "2019-04-16 18:16:31,386 epoch 126 - iter 96/121 - loss 4.69351681\n",
      "2019-04-16 18:16:36,556 epoch 126 - iter 108/121 - loss 4.72357496\n",
      "2019-04-16 18:16:42,884 epoch 126 - iter 120/121 - loss 4.68754469\n",
      "2019-04-16 18:16:42,905 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:16:42,906 EPOCH 126 done: loss 4.6875 - lr 0.0000 - bad epochs 3\n",
      "2019-04-16 18:16:51,802 DEV  : loss 3.92115402 - f-score 0.6987 - acc 0.5370\n",
      "2019-04-16 18:16:59,624 TEST : loss 3.55766082 - f-score 0.7350 - acc 0.5809\n",
      "Epoch   125: reducing learning rate of group 0 to 2.4414e-05.\n",
      "2019-04-16 18:16:59,629 Epoch 125: reducing weight decay factor of group 0 to 2.4414e-05.\n",
      "2019-04-16 18:16:59,630 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:17:00,198 epoch 127 - iter 0/121 - loss 5.11553240\n",
      "2019-04-16 18:17:11,646 epoch 127 - iter 12/121 - loss 5.45486668\n",
      "2019-04-16 18:17:16,638 epoch 127 - iter 24/121 - loss 4.95437425\n",
      "2019-04-16 18:17:22,608 epoch 127 - iter 36/121 - loss 4.91393733\n",
      "2019-04-16 18:17:32,101 epoch 127 - iter 48/121 - loss 5.05379601\n",
      "2019-04-16 18:17:37,937 epoch 127 - iter 60/121 - loss 4.87205829\n",
      "2019-04-16 18:17:43,588 epoch 127 - iter 72/121 - loss 4.79685523\n",
      "2019-04-16 18:17:49,320 epoch 127 - iter 84/121 - loss 4.70692996\n",
      "2019-04-16 18:17:54,335 epoch 127 - iter 96/121 - loss 4.67214325\n",
      "2019-04-16 18:18:00,527 epoch 127 - iter 108/121 - loss 4.63005755\n",
      "2019-04-16 18:18:06,709 epoch 127 - iter 120/121 - loss 4.64360866\n",
      "2019-04-16 18:18:06,731 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:18:06,732 EPOCH 127 done: loss 4.6436 - lr 0.0000 - bad epochs 0\n",
      "2019-04-16 18:18:13,593 DEV  : loss 3.89329195 - f-score 0.6991 - acc 0.5374\n",
      "2019-04-16 18:18:21,565 TEST : loss 3.54731560 - f-score 0.7361 - acc 0.5824\n",
      "2019-04-16 18:18:21,568 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:18:22,130 epoch 128 - iter 0/121 - loss 4.80650043\n",
      "2019-04-16 18:18:28,323 epoch 128 - iter 12/121 - loss 4.40934746\n",
      "2019-04-16 18:18:34,160 epoch 128 - iter 24/121 - loss 4.40164161\n",
      "2019-04-16 18:18:39,039 epoch 128 - iter 36/121 - loss 4.47468546\n",
      "2019-04-16 18:18:51,024 epoch 128 - iter 48/121 - loss 4.60781273\n",
      "2019-04-16 18:18:57,101 epoch 128 - iter 60/121 - loss 4.65616247\n",
      "2019-04-16 18:19:01,768 epoch 128 - iter 72/121 - loss 4.63753544\n",
      "2019-04-16 18:19:06,508 epoch 128 - iter 84/121 - loss 4.64293793\n",
      "2019-04-16 18:19:19,094 epoch 128 - iter 96/121 - loss 4.76714433\n",
      "2019-04-16 18:19:24,312 epoch 128 - iter 108/121 - loss 4.68959407\n",
      "2019-04-16 18:19:29,973 epoch 128 - iter 120/121 - loss 4.68280897\n",
      "2019-04-16 18:19:29,994 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:19:29,994 EPOCH 128 done: loss 4.6828 - lr 0.0000 - bad epochs 1\n",
      "2019-04-16 18:19:39,044 DEV  : loss 3.91447115 - f-score 0.7008 - acc 0.5394\n",
      "2019-04-16 18:19:46,875 TEST : loss 3.56603098 - f-score 0.7384 - acc 0.5853\n",
      "2019-04-16 18:19:46,878 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:19:47,348 epoch 129 - iter 0/121 - loss 4.42431450\n",
      "2019-04-16 18:19:53,251 epoch 129 - iter 12/121 - loss 4.70097298\n",
      "2019-04-16 18:20:04,172 epoch 129 - iter 24/121 - loss 4.75596011\n",
      "2019-04-16 18:20:09,218 epoch 129 - iter 36/121 - loss 4.56675715\n",
      "2019-04-16 18:20:12,840 epoch 129 - iter 48/121 - loss 4.56529824\n",
      "2019-04-16 18:20:17,049 epoch 129 - iter 60/121 - loss 4.61128746\n",
      "2019-04-16 18:20:22,375 epoch 129 - iter 72/121 - loss 4.62217046\n",
      "2019-04-16 18:20:27,777 epoch 129 - iter 84/121 - loss 4.60896873\n",
      "2019-04-16 18:20:35,994 epoch 129 - iter 96/121 - loss 4.63998700\n",
      "2019-04-16 18:20:40,961 epoch 129 - iter 108/121 - loss 4.62583859\n",
      "2019-04-16 18:20:49,223 epoch 129 - iter 120/121 - loss 4.68445373\n",
      "2019-04-16 18:20:49,243 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:20:49,244 EPOCH 129 done: loss 4.6845 - lr 0.0000 - bad epochs 2\n",
      "2019-04-16 18:20:57,704 DEV  : loss 3.93711209 - f-score 0.7013 - acc 0.5400\n",
      "2019-04-16 18:21:05,912 TEST : loss 3.57189059 - f-score 0.7359 - acc 0.5822\n",
      "2019-04-16 18:21:05,915 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:21:07,552 epoch 130 - iter 0/121 - loss 10.11261749\n",
      "2019-04-16 18:21:14,832 epoch 130 - iter 12/121 - loss 5.05397219\n",
      "2019-04-16 18:21:20,100 epoch 130 - iter 24/121 - loss 4.76939413\n",
      "2019-04-16 18:21:25,666 epoch 130 - iter 36/121 - loss 4.60531548\n",
      "2019-04-16 18:21:31,599 epoch 130 - iter 48/121 - loss 4.66414120\n",
      "2019-04-16 18:21:36,283 epoch 130 - iter 60/121 - loss 4.69206373\n",
      "2019-04-16 18:21:40,471 epoch 130 - iter 72/121 - loss 4.66849924\n",
      "2019-04-16 18:21:51,955 epoch 130 - iter 84/121 - loss 4.71733563\n",
      "2019-04-16 18:21:58,445 epoch 130 - iter 96/121 - loss 4.69892519\n",
      "2019-04-16 18:22:04,948 epoch 130 - iter 108/121 - loss 4.71138150\n",
      "2019-04-16 18:22:11,423 epoch 130 - iter 120/121 - loss 4.71711562\n",
      "2019-04-16 18:22:11,445 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:22:11,445 EPOCH 130 done: loss 4.7171 - lr 0.0000 - bad epochs 3\n",
      "2019-04-16 18:22:19,852 DEV  : loss 3.93351912 - f-score 0.7002 - acc 0.5387\n",
      "2019-04-16 18:22:26,198 TEST : loss 3.57704139 - f-score 0.7375 - acc 0.5842\n",
      "Epoch   129: reducing learning rate of group 0 to 1.2207e-05.\n",
      "2019-04-16 18:22:26,201 Epoch 129: reducing weight decay factor of group 0 to 1.2207e-05.\n",
      "2019-04-16 18:22:26,202 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:22:26,415 epoch 131 - iter 0/121 - loss 3.88677597\n",
      "2019-04-16 18:22:30,379 epoch 131 - iter 12/121 - loss 4.51843566\n",
      "2019-04-16 18:22:43,134 epoch 131 - iter 24/121 - loss 4.92718815\n",
      "2019-04-16 18:22:49,337 epoch 131 - iter 36/121 - loss 4.83853578\n",
      "2019-04-16 18:22:53,800 epoch 131 - iter 48/121 - loss 4.86540615\n",
      "2019-04-16 18:22:57,343 epoch 131 - iter 60/121 - loss 4.75265752\n",
      "2019-04-16 18:23:01,340 epoch 131 - iter 72/121 - loss 4.67690166\n",
      "2019-04-16 18:23:11,141 epoch 131 - iter 84/121 - loss 4.76873759\n",
      "2019-04-16 18:23:16,648 epoch 131 - iter 96/121 - loss 4.76386479\n",
      "2019-04-16 18:23:22,689 epoch 131 - iter 108/121 - loss 4.71045938\n",
      "2019-04-16 18:23:29,081 epoch 131 - iter 120/121 - loss 4.71201704\n",
      "2019-04-16 18:23:29,100 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:23:29,101 EPOCH 131 done: loss 4.7120 - lr 0.0000 - bad epochs 0\n",
      "2019-04-16 18:23:38,099 DEV  : loss 3.95596528 - f-score 0.7002 - acc 0.5387\n",
      "2019-04-16 18:23:45,691 TEST : loss 3.58987713 - f-score 0.7348 - acc 0.5807\n",
      "2019-04-16 18:23:45,695 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:23:46,215 epoch 132 - iter 0/121 - loss 5.51164865\n",
      "2019-04-16 18:23:52,312 epoch 132 - iter 12/121 - loss 4.67582666\n",
      "2019-04-16 18:24:03,387 epoch 132 - iter 24/121 - loss 5.05940998\n",
      "2019-04-16 18:24:10,704 epoch 132 - iter 36/121 - loss 4.85211785\n",
      "2019-04-16 18:24:17,246 epoch 132 - iter 48/121 - loss 4.76900813\n",
      "2019-04-16 18:24:27,723 epoch 132 - iter 60/121 - loss 4.93520252\n",
      "2019-04-16 18:24:32,354 epoch 132 - iter 72/121 - loss 4.74547781\n",
      "2019-04-16 18:24:39,494 epoch 132 - iter 84/121 - loss 4.75572051\n",
      "2019-04-16 18:24:45,711 epoch 132 - iter 96/121 - loss 4.77628429\n",
      "2019-04-16 18:24:49,931 epoch 132 - iter 108/121 - loss 4.68785490\n",
      "2019-04-16 18:24:53,770 epoch 132 - iter 120/121 - loss 4.65454474\n",
      "2019-04-16 18:24:53,791 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:24:53,792 EPOCH 132 done: loss 4.6545 - lr 0.0000 - bad epochs 1\n",
      "2019-04-16 18:24:59,608 DEV  : loss 3.90861607 - f-score 0.6975 - acc 0.5356\n",
      "2019-04-16 18:25:09,174 TEST : loss 3.56266379 - f-score 0.7394 - acc 0.5865\n",
      "2019-04-16 18:25:09,177 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:25:09,619 epoch 133 - iter 0/121 - loss 3.08321500\n",
      "2019-04-16 18:25:19,874 epoch 133 - iter 12/121 - loss 5.27039618\n",
      "2019-04-16 18:25:24,254 epoch 133 - iter 24/121 - loss 4.78643379\n",
      "2019-04-16 18:25:28,802 epoch 133 - iter 36/121 - loss 4.81864439\n",
      "2019-04-16 18:25:33,574 epoch 133 - iter 48/121 - loss 4.76030483\n",
      "2019-04-16 18:25:38,734 epoch 133 - iter 60/121 - loss 4.62831025\n",
      "2019-04-16 18:25:49,203 epoch 133 - iter 72/121 - loss 4.70325060\n",
      "2019-04-16 18:25:53,822 epoch 133 - iter 84/121 - loss 4.62861579\n",
      "2019-04-16 18:25:59,706 epoch 133 - iter 96/121 - loss 4.60292398\n",
      "2019-04-16 18:26:08,458 epoch 133 - iter 108/121 - loss 4.63785359\n",
      "2019-04-16 18:26:13,653 epoch 133 - iter 120/121 - loss 4.66750877\n",
      "2019-04-16 18:26:13,674 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:26:13,675 EPOCH 133 done: loss 4.6675 - lr 0.0000 - bad epochs 2\n",
      "2019-04-16 18:26:19,944 DEV  : loss 3.87964201 - f-score 0.7012 - acc 0.5399\n",
      "2019-04-16 18:26:27,618 TEST : loss 3.54834342 - f-score 0.7379 - acc 0.5847\n",
      "2019-04-16 18:26:27,620 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:26:28,056 epoch 134 - iter 0/121 - loss 4.91003990\n",
      "2019-04-16 18:26:33,592 epoch 134 - iter 12/121 - loss 4.40372401\n",
      "2019-04-16 18:26:38,643 epoch 134 - iter 24/121 - loss 4.43619736\n",
      "2019-04-16 18:26:45,173 epoch 134 - iter 36/121 - loss 4.56909446\n",
      "2019-04-16 18:26:51,717 epoch 134 - iter 48/121 - loss 4.51974471\n",
      "2019-04-16 18:26:58,758 epoch 134 - iter 60/121 - loss 4.60360290\n",
      "2019-04-16 18:27:12,949 epoch 134 - iter 72/121 - loss 4.87474427\n",
      "2019-04-16 18:27:16,386 epoch 134 - iter 84/121 - loss 4.72141082\n",
      "2019-04-16 18:27:20,392 epoch 134 - iter 96/121 - loss 4.61609407\n",
      "2019-04-16 18:27:29,589 epoch 134 - iter 108/121 - loss 4.61681076\n",
      "2019-04-16 18:27:35,502 epoch 134 - iter 120/121 - loss 4.68125000\n",
      "2019-04-16 18:27:35,524 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:27:35,525 EPOCH 134 done: loss 4.6813 - lr 0.0000 - bad epochs 3\n",
      "2019-04-16 18:27:43,353 DEV  : loss 3.88189626 - f-score 0.7038 - acc 0.5430\n",
      "2019-04-16 18:27:52,357 TEST : loss 3.54620266 - f-score 0.7356 - acc 0.5818\n",
      "Epoch   133: reducing learning rate of group 0 to 6.1035e-06.\n",
      "2019-04-16 18:27:52,359 Epoch 133: reducing weight decay factor of group 0 to 6.1035e-06.\n",
      "2019-04-16 18:27:52,360 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:27:53,141 epoch 135 - iter 0/121 - loss 5.48994970\n",
      "2019-04-16 18:27:58,882 epoch 135 - iter 12/121 - loss 4.16734661\n",
      "2019-04-16 18:28:04,341 epoch 135 - iter 24/121 - loss 4.49708580\n",
      "2019-04-16 18:28:11,473 epoch 135 - iter 36/121 - loss 4.65334241\n",
      "2019-04-16 18:28:19,384 epoch 135 - iter 48/121 - loss 4.73241769\n",
      "2019-04-16 18:28:24,029 epoch 135 - iter 60/121 - loss 4.60894697\n",
      "2019-04-16 18:28:29,390 epoch 135 - iter 72/121 - loss 4.64726112\n",
      "2019-04-16 18:28:44,954 epoch 135 - iter 84/121 - loss 4.76355502\n",
      "2019-04-16 18:28:51,162 epoch 135 - iter 96/121 - loss 4.73180638\n",
      "2019-04-16 18:28:57,174 epoch 135 - iter 108/121 - loss 4.68872532\n",
      "2019-04-16 18:29:02,450 epoch 135 - iter 120/121 - loss 4.64025802\n",
      "2019-04-16 18:29:02,470 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:29:02,471 EPOCH 135 done: loss 4.6403 - lr 0.0000 - bad epochs 0\n",
      "2019-04-16 18:29:09,827 DEV  : loss 3.88893938 - f-score 0.7026 - acc 0.5416\n",
      "2019-04-16 18:29:16,060 TEST : loss 3.55044508 - f-score 0.7362 - acc 0.5826\n",
      "2019-04-16 18:29:16,062 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:29:16,422 epoch 136 - iter 0/121 - loss 3.24571037\n",
      "2019-04-16 18:29:21,887 epoch 136 - iter 12/121 - loss 4.35593368\n",
      "2019-04-16 18:29:34,194 epoch 136 - iter 24/121 - loss 4.86063342\n",
      "2019-04-16 18:29:38,562 epoch 136 - iter 36/121 - loss 4.77290802\n",
      "2019-04-16 18:29:43,366 epoch 136 - iter 48/121 - loss 4.69498137\n",
      "2019-04-16 18:29:48,228 epoch 136 - iter 60/121 - loss 4.79456993\n",
      "2019-04-16 18:29:53,544 epoch 136 - iter 72/121 - loss 4.59599176\n",
      "2019-04-16 18:29:58,833 epoch 136 - iter 84/121 - loss 4.53917583\n",
      "2019-04-16 18:30:08,263 epoch 136 - iter 96/121 - loss 4.60787437\n",
      "2019-04-16 18:30:15,151 epoch 136 - iter 108/121 - loss 4.60930989\n",
      "2019-04-16 18:30:20,929 epoch 136 - iter 120/121 - loss 4.59780305\n",
      "2019-04-16 18:30:20,951 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:30:20,952 EPOCH 136 done: loss 4.5978 - lr 0.0000 - bad epochs 1\n",
      "2019-04-16 18:30:29,106 DEV  : loss 3.90171599 - f-score 0.7019 - acc 0.5407\n",
      "2019-04-16 18:30:37,747 TEST : loss 3.55951500 - f-score 0.7342 - acc 0.5801\n",
      "2019-04-16 18:30:48,905 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:30:49,384 epoch 137 - iter 0/121 - loss 5.16290712\n",
      "2019-04-16 18:30:56,107 epoch 137 - iter 12/121 - loss 4.06737962\n",
      "2019-04-16 18:31:01,952 epoch 137 - iter 24/121 - loss 4.33607606\n",
      "2019-04-16 18:31:10,805 epoch 137 - iter 36/121 - loss 4.46251341\n",
      "2019-04-16 18:31:14,376 epoch 137 - iter 48/121 - loss 4.36329267\n",
      "2019-04-16 18:31:18,202 epoch 137 - iter 60/121 - loss 4.36829373\n",
      "2019-04-16 18:31:26,594 epoch 137 - iter 72/121 - loss 4.45961085\n",
      "2019-04-16 18:31:32,350 epoch 137 - iter 84/121 - loss 4.52436366\n",
      "2019-04-16 18:31:43,173 epoch 137 - iter 96/121 - loss 4.63441121\n",
      "2019-04-16 18:31:47,166 epoch 137 - iter 108/121 - loss 4.67054798\n",
      "2019-04-16 18:31:52,362 epoch 137 - iter 120/121 - loss 4.66022684\n",
      "2019-04-16 18:31:53,253 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:31:53,254 EPOCH 137 done: loss 4.6602 - lr 0.0000 - bad epochs 0\n",
      "2019-04-16 18:32:02,471 DEV  : loss 3.89877748 - f-score 0.7030 - acc 0.5420\n",
      "2019-04-16 18:32:10,254 TEST : loss 3.55793190 - f-score 0.7370 - acc 0.5835\n",
      "2019-04-16 18:32:10,257 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:32:10,583 epoch 138 - iter 0/121 - loss 5.59917927\n",
      "2019-04-16 18:32:16,697 epoch 138 - iter 12/121 - loss 4.73804186\n",
      "2019-04-16 18:32:22,197 epoch 138 - iter 24/121 - loss 4.45928188\n",
      "2019-04-16 18:32:32,432 epoch 138 - iter 36/121 - loss 4.63061556\n",
      "2019-04-16 18:32:38,213 epoch 138 - iter 48/121 - loss 4.59393982\n",
      "2019-04-16 18:32:50,433 epoch 138 - iter 60/121 - loss 4.62914361\n",
      "2019-04-16 18:32:54,877 epoch 138 - iter 72/121 - loss 4.54394503\n",
      "2019-04-16 18:33:00,792 epoch 138 - iter 84/121 - loss 4.51130494\n",
      "2019-04-16 18:33:06,374 epoch 138 - iter 96/121 - loss 4.59975571\n",
      "2019-04-16 18:33:12,935 epoch 138 - iter 108/121 - loss 4.63973158\n",
      "2019-04-16 18:33:19,937 epoch 138 - iter 120/121 - loss 4.62125189\n",
      "2019-04-16 18:33:19,958 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:33:19,959 EPOCH 138 done: loss 4.6213 - lr 0.0000 - bad epochs 1\n",
      "2019-04-16 18:33:29,117 DEV  : loss 3.89752078 - f-score 0.7010 - acc 0.5396\n",
      "2019-04-16 18:33:36,345 TEST : loss 3.55442858 - f-score 0.7342 - acc 0.5800\n",
      "2019-04-16 18:33:36,347 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:33:36,818 epoch 139 - iter 0/121 - loss 5.57819796\n",
      "2019-04-16 18:33:41,242 epoch 139 - iter 12/121 - loss 4.81017619\n",
      "2019-04-16 18:33:51,144 epoch 139 - iter 24/121 - loss 4.76324106\n",
      "2019-04-16 18:33:57,013 epoch 139 - iter 36/121 - loss 4.73469037\n",
      "2019-04-16 18:34:02,243 epoch 139 - iter 48/121 - loss 4.66971916\n",
      "2019-04-16 18:34:06,562 epoch 139 - iter 60/121 - loss 4.71356100\n",
      "2019-04-16 18:34:14,831 epoch 139 - iter 72/121 - loss 4.75045351\n",
      "2019-04-16 18:34:18,233 epoch 139 - iter 84/121 - loss 4.69054990\n",
      "2019-04-16 18:34:24,043 epoch 139 - iter 96/121 - loss 4.62414933\n",
      "2019-04-16 18:34:29,246 epoch 139 - iter 108/121 - loss 4.59501313\n",
      "2019-04-16 18:34:36,635 epoch 139 - iter 120/121 - loss 4.66497883\n",
      "2019-04-16 18:34:36,657 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:34:36,658 EPOCH 139 done: loss 4.6650 - lr 0.0000 - bad epochs 2\n",
      "2019-04-16 18:34:43,038 DEV  : loss 3.90575242 - f-score 0.7025 - acc 0.5414\n",
      "2019-04-16 18:34:49,544 TEST : loss 3.55990076 - f-score 0.7361 - acc 0.5824\n",
      "2019-04-16 18:34:49,546 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:34:50,363 epoch 140 - iter 0/121 - loss 5.57003975\n",
      "2019-04-16 18:34:57,180 epoch 140 - iter 12/121 - loss 4.80864752\n",
      "2019-04-16 18:34:59,971 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-16 18:34:59,972 Exiting from training early.\n",
      "2019-04-16 18:34:59,973 Saving model ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/code/flair/flair/trainers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, base_path, evaluation_metric, learning_rate, mini_batch_size, eval_mini_batch_size, max_epochs, anneal_factor, patience, anneal_against_train_loss, train_with_dev, monitor_train, embeddings_in_memory, checkpoint, save_final_model, anneal_with_restarts, test_mode, param_selection_mode, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-8d2841cf4b48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m               \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m               \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m               checkpoint=False)\n\u001b[0m",
      "\u001b[0;32m~/code/flair/flair/trainers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, base_path, evaluation_metric, learning_rate, mini_batch_size, eval_mini_batch_size, max_epochs, anneal_factor, patience, anneal_against_train_loss, train_with_dev, monitor_train, embeddings_in_memory, checkpoint, save_final_model, anneal_with_restarts, test_mode, param_selection_mode, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparam_selection_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saving model ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'final-model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/flair/flair/models/sequence_tagger_model.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, model_file)\u001b[0m\n\u001b[1;32m    182\u001b[0m         }\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_torch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     def save_checkpoint(self, model_file: Union[str, Path], optimizer_state: dict, scheduler_state: dict, epoch: int,\n",
      "\u001b[0;32m~/code/flair/flair/models/sequence_tagger_model.py\u001b[0m in \u001b[0;36msave_torch_model\u001b[0;34m(model_state, model_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \"\"\"\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \"\"\"\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0mserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_storages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_id\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# https://docs.python.org/2/library/pickle.html#pickling-and-unpickling-external-objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# https://github.com/python/cpython/blob/master/Lib/pickle.py#L527-L537\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserialized_container_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 7. start training\n",
    "result_folder = '/home/jupyter/result_ANER_64_2_l01_w01_d02_adamw/'\n",
    "trainer.train(result_folder,\n",
    "              EvaluationMetric.MICRO_F1_SCORE,\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              weight_decay = 0.1,\n",
    "              max_epochs=200,\n",
    "              checkpoint=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.visual.training_curves import Plotter\n",
    "plotter = Plotter()\n",
    "plotter.plot_training_curves(os.path.join(result_folder, 'loss.tsv'))\n",
    "plotter.plot_weights(os.path.join(result_folder, 'weights.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
